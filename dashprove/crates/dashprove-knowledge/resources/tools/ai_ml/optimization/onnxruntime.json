{
  "id": "onnxruntime",
  "name": "ONNX Runtime",
  "category": "ai_ml_optimization",
  "subcategory": "inference",

  "description": "High-performance ML inference engine",
  "long_description": "ONNX Runtime is Microsoft's high-performance inference engine for ONNX models. It provides optimized inference across CPU, GPU, and specialized accelerators, with support for quantization, graph optimizations, and execution providers for different hardware.",

  "capabilities": [
    "model_inference",
    "graph_optimization",
    "quantization",
    "hardware_acceleration",
    "dynamic_shapes",
    "multi_threading"
  ],
  "property_types": ["performance", "accuracy"],
  "input_languages": ["onnx", "pytorch", "tensorflow"],
  "output_formats": ["tensor", "numpy"],

  "installation": {
    "methods": [
      {"type": "pip", "command": "pip install onnxruntime"},
      {"type": "pip", "command": "pip install onnxruntime-gpu"}
    ],
    "dependencies": [],
    "platforms": ["linux", "macos", "windows"]
  },

  "documentation": {
    "official": "https://onnxruntime.ai/",
    "tutorial": "https://onnxruntime.ai/docs/get-started/",
    "api_reference": "https://onnxruntime.ai/docs/api/",
    "examples": "https://github.com/microsoft/onnxruntime/tree/main/samples"
  },

  "tactics": [
    {
      "name": "optimize",
      "description": "Apply graph optimizations",
      "syntax": "sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_ALL",
      "when_to_use": "To maximize inference speed",
      "examples": ["sess_options.graph_optimization_level = ort.GraphOptimizationLevel.ORT_ENABLE_EXTENDED"]
    },
    {
      "name": "quantize",
      "description": "Reduce precision",
      "syntax": "quantize_dynamic(model_path, quantized_path)",
      "when_to_use": "To reduce model size and improve speed",
      "examples": ["from onnxruntime.quantization import quantize_dynamic"]
    },
    {
      "name": "execution_provider",
      "description": "Select hardware backend",
      "syntax": "InferenceSession(model, providers=['CUDAExecutionProvider'])",
      "when_to_use": "To use GPU or accelerators",
      "examples": ["session = ort.InferenceSession(model, providers=['TensorrtExecutionProvider'])"]
    }
  ],

  "error_patterns": [
    {
      "pattern": "Invalid model",
      "meaning": "ONNX model has errors",
      "common_causes": ["Conversion error", "Unsupported operator"],
      "fixes": ["Re-export model", "Use opset upgrade"]
    },
    {
      "pattern": "Out of memory",
      "meaning": "Insufficient GPU/CPU memory",
      "common_causes": ["Large model", "Large batch size"],
      "fixes": ["Reduce batch size", "Use quantization"]
    }
  ],

  "integration": {
    "dashprove_backend": true,
    "usl_property_types": ["performance", "accuracy"],
    "cli_command": "dashprove verify --backend onnxruntime"
  },

  "performance": {
    "typical_runtime": "milliseconds per inference",
    "scalability": "Production-ready",
    "memory_usage": "Depends on model"
  },

  "comparisons": {
    "similar_tools": ["tensorrt", "openvino", "tvm"],
    "advantages": [
      "Wide hardware support",
      "Production quality",
      "Good Python API",
      "ONNX standard format"
    ],
    "disadvantages": [
      "Not always fastest",
      "Some ops not supported"
    ]
  },

  "metadata": {
    "version": "1.16.0",
    "last_updated": "2025-12-20",
    "maintainer": "Microsoft",
    "license": "MIT"
  }
}
