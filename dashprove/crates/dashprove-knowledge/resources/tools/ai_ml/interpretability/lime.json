{
  "id": "lime",
  "name": "LIME",
  "category": "ai_ml_interpretability",
  "subcategory": "local_explanation",

  "description": "Local Interpretable Model-agnostic Explanations",
  "long_description": "LIME (Local Interpretable Model-agnostic Explanations) explains individual predictions of any classifier by approximating it locally with an interpretable model. It works by perturbing the input and observing how predictions change, then fitting a simple model to explain the local behavior.",

  "capabilities": [
    "local_explanations",
    "model_agnostic",
    "tabular_data",
    "text_data",
    "image_data",
    "feature_importance",
    "prediction_probabilities",
    "interpretable_surrogate"
  ],
  "property_types": ["interpretability", "explainability"],
  "input_languages": ["python", "scikit-learn", "any_classifier"],
  "output_formats": ["explanation", "html", "feature_weights"],

  "installation": {
    "methods": [
      {"type": "pip", "command": "pip install lime"},
      {"type": "source", "url": "https://github.com/marcotcr/lime"}
    ],
    "dependencies": ["python", "numpy", "scikit-learn"],
    "platforms": ["linux", "macos", "windows"]
  },

  "documentation": {
    "official": "https://github.com/marcotcr/lime",
    "tutorial": "https://marcotcr.github.io/lime/tutorials/Lime%20-%20basic%20usage%2C%20two%20class%20case.html",
    "api_reference": "https://lime-ml.readthedocs.io/en/latest/",
    "examples": "https://github.com/marcotcr/lime/tree/master/doc/notebooks"
  },

  "tactics": [
    {
      "name": "tabular",
      "description": "Explain tabular prediction",
      "syntax": "LimeTabularExplainer(data).explain_instance(x, predict_fn)",
      "when_to_use": "Explaining structured data predictions",
      "examples": ["explainer = LimeTabularExplainer(X_train, feature_names=names)\nexp = explainer.explain_instance(x, model.predict_proba)"]
    },
    {
      "name": "text",
      "description": "Explain text classification",
      "syntax": "LimeTextExplainer().explain_instance(text, predict_fn)",
      "when_to_use": "Explaining NLP model predictions",
      "examples": ["explainer = LimeTextExplainer()\nexp = explainer.explain_instance(text, classifier.predict_proba)"]
    },
    {
      "name": "image",
      "description": "Explain image classification",
      "syntax": "LimeImageExplainer().explain_instance(image, predict_fn)",
      "when_to_use": "Explaining computer vision predictions",
      "examples": ["explainer = LimeImageExplainer()\nexp = explainer.explain_instance(image, model.predict)"]
    },
    {
      "name": "show_in_notebook",
      "description": "Display explanation in Jupyter",
      "syntax": "exp.show_in_notebook()",
      "when_to_use": "Interactive exploration",
      "examples": ["exp.show_in_notebook(show_all=False)"]
    }
  ],

  "error_patterns": [
    {
      "pattern": "Explanation varies significantly",
      "meaning": "Local approximation unstable",
      "common_causes": ["High-dimensional data", "Complex decision boundary"],
      "fixes": ["Increase num_samples", "Use different kernel width"]
    },
    {
      "pattern": "Features with zero weight",
      "meaning": "Feature not locally important",
      "common_causes": ["Feature irrelevant for this prediction"],
      "fixes": ["Check if expected, may be correct"]
    }
  ],

  "integration": {
    "dashprove_backend": true,
    "usl_property_types": ["interpretability"],
    "cli_command": "dashprove verify --backend lime"
  },

  "performance": {
    "typical_runtime": "Seconds per explanation",
    "scalability": "Per-instance, parallelizable",
    "memory_usage": "Low"
  },

  "comparisons": {
    "similar_tools": ["shap", "anchors", "eli5"],
    "advantages": ["Model agnostic", "Intuitive explanations", "Multiple data types", "Simple API"],
    "disadvantages": ["Local only", "Sampling instability", "Can be slow for images"]
  },

  "metadata": {
    "version": "0.2.0.1",
    "last_updated": "2025-12-20",
    "maintainer": "Marco Ribeiro",
    "license": "BSD-2-Clause"
  }
}
