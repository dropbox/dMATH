{
  "id": "alibi",
  "name": "Alibi",
  "category": "ai_ml",
  "subcategory": "interpretability",

  "description": "Machine learning model inspection and interpretation library",
  "long_description": "Alibi is an open-source Python library for machine learning model inspection and interpretation. It provides a collection of state-of-the-art explanation methods for classification and regression models. Supports both black-box and white-box explanations with counterfactual generation.",

  "capabilities": [
    "counterfactual_explanations",
    "anchor_explanations",
    "contrastive_explanations",
    "integrated_gradients",
    "kernel_shap",
    "tree_shap",
    "ale_plots",
    "prototype_explanations"
  ],
  "property_types": ["explanation", "counterfactual", "attribution"],
  "input_languages": ["python"],
  "output_formats": ["explanation", "visualization", "json"],

  "installation": {
    "methods": [
      {"type": "pip", "command": "pip install alibi"},
      {"type": "source", "url": "https://github.com/SeldonIO/alibi"}
    ],
    "dependencies": ["numpy", "scikit-learn", "tensorflow", "pytorch"],
    "platforms": ["linux", "macos", "windows"]
  },

  "documentation": {
    "official": "https://docs.seldon.io/projects/alibi/",
    "tutorial": "https://docs.seldon.io/projects/alibi/en/stable/overview/getting_started.html",
    "api_reference": "https://docs.seldon.io/projects/alibi/en/stable/api/",
    "examples": "https://github.com/SeldonIO/alibi/tree/master/doc/source/examples"
  },

  "tactics": [
    {
      "name": "counterfactual",
      "description": "Generate counterfactual explanation",
      "syntax": "Counterfactual(model).explain(instance)",
      "when_to_use": "Understanding what would change prediction",
      "examples": ["cf = Counterfactual(predict_fn); exp = cf.explain(x)"]
    },
    {
      "name": "anchor",
      "description": "Find anchor explanation",
      "syntax": "AnchorTabular(predictor, features).explain(instance)",
      "when_to_use": "Finding sufficient conditions",
      "examples": ["anchor = AnchorTabular(pred, names); exp = anchor.explain(x)"]
    },
    {
      "name": "integrated_gradients",
      "description": "Compute attributions via IG",
      "syntax": "IntegratedGradients(model).explain(instance)",
      "when_to_use": "Deep learning attributions",
      "examples": ["ig = IntegratedGradients(model); exp = ig.explain(x)"]
    },
    {
      "name": "ale",
      "description": "Compute ALE plots",
      "syntax": "ALE(predictor).explain(X)",
      "when_to_use": "Understanding feature effects",
      "examples": ["ale = ALE(pred); exp = ale.explain(X_train)"]
    }
  ],

  "error_patterns": [
    {
      "pattern": "No counterfactual found",
      "meaning": "Could not find valid counterfactual",
      "common_causes": ["Constrained search space", "Model structure"],
      "fixes": ["Relax constraints", "Try different method"]
    },
    {
      "pattern": "Anchor timeout",
      "meaning": "Anchor search took too long",
      "common_causes": ["Complex feature space", "Strict threshold"],
      "fixes": ["Reduce precision", "Limit beam size"]
    }
  ],

  "integration": {
    "dashprove_backend": true,
    "usl_property_types": ["interpretability"],
    "cli_command": "dashprove verify --backend alibi"
  },

  "performance": {
    "typical_runtime": "Seconds to minutes per explanation",
    "scalability": "Depends on method",
    "memory_usage": "Moderate"
  },

  "comparisons": {
    "similar_tools": ["shap", "lime", "captum", "interpret"],
    "advantages": [
      "Counterfactual explanations",
      "Anchor explanations",
      "Good documentation",
      "Production-ready (Seldon)"
    ],
    "disadvantages": [
      "Some methods slow",
      "Complex configuration"
    ]
  },

  "metadata": {
    "version": "0.9.4",
    "last_updated": "2025-12-20",
    "maintainer": "Seldon",
    "license": "Apache-2.0"
  }
}
