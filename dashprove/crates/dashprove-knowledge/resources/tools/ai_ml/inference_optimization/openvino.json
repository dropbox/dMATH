{
  "id": "openvino",
  "name": "OpenVINO",
  "category": "ai_ml",
  "subcategory": "inference_optimization",

  "description": "Intel toolkit for optimizing and deploying AI inference",
  "long_description": "OpenVINO (Open Visual Inference and Neural Network Optimization) is Intel's toolkit for optimizing and deploying AI inference. It supports multiple Intel hardware targets including CPUs, integrated GPUs, VPUs, and FPGAs. Features include model optimization, quantization, and heterogeneous execution.",

  "capabilities": [
    "model_optimization",
    "int8_quantization",
    "cpu_inference",
    "gpu_inference",
    "vpu_inference",
    "heterogeneous_execution",
    "async_inference",
    "model_caching"
  ],
  "property_types": ["performance", "accuracy"],
  "input_languages": ["onnx", "tensorflow", "pytorch", "paddlepaddle"],
  "output_formats": ["ir_model", "inference_result", "benchmark"],

  "installation": {
    "methods": [
      {"type": "pip", "command": "pip install openvino"},
      {"type": "download", "url": "https://www.intel.com/content/www/us/en/developer/tools/openvino-toolkit/download.html"}
    ],
    "dependencies": [],
    "platforms": ["linux", "macos", "windows"]
  },

  "documentation": {
    "official": "https://docs.openvino.ai/",
    "tutorial": "https://docs.openvino.ai/latest/get_started.html",
    "api_reference": "https://docs.openvino.ai/latest/api_reference.html",
    "examples": "https://github.com/openvinotoolkit/openvino_notebooks"
  },

  "tactics": [
    {
      "name": "convert",
      "description": "Convert model to OpenVINO IR",
      "syntax": "ovc model.onnx",
      "when_to_use": "Preparing model for inference",
      "examples": ["ovc resnet50.onnx", "ovc model.pb --input_shape [1,224,224,3]"]
    },
    {
      "name": "benchmark",
      "description": "Benchmark inference performance",
      "syntax": "benchmark_app -m model.xml",
      "when_to_use": "Measuring latency and throughput",
      "examples": ["benchmark_app -m model.xml -d CPU -niter 100"]
    },
    {
      "name": "quantize",
      "description": "Apply INT8 quantization",
      "syntax": "pot -q default -m model.xml --engine simplified",
      "when_to_use": "Improving inference speed",
      "examples": ["pot -q default -m model.xml -w model.bin --ac-config config.json"]
    },
    {
      "name": "infer",
      "description": "Run inference",
      "syntax": "core.compile_model(model, device).infer(inputs)",
      "when_to_use": "Running model in application",
      "examples": ["compiled = core.compile_model('model.xml', 'CPU'); result = compiled.infer(data)"]
    }
  ],

  "error_patterns": [
    {
      "pattern": "Unsupported layer type",
      "meaning": "Operation not supported",
      "common_causes": ["Custom op", "New op version"],
      "fixes": ["Update OpenVINO", "Use extension", "Modify model"]
    },
    {
      "pattern": "Shape inference failed",
      "meaning": "Cannot determine output shapes",
      "common_causes": ["Dynamic shapes", "Missing input shape"],
      "fixes": ["Specify input shapes", "Use static shapes"]
    }
  ],

  "integration": {
    "dashprove_backend": true,
    "usl_property_types": ["model_optimization"],
    "cli_command": "dashprove verify --backend openvino"
  },

  "performance": {
    "typical_runtime": "Seconds for conversion, milliseconds for inference",
    "scalability": "Good across Intel hardware",
    "memory_usage": "Efficient"
  },

  "comparisons": {
    "similar_tools": ["tensorrt", "onnxruntime", "tvm"],
    "advantages": [
      "Best performance on Intel hardware",
      "Wide hardware support",
      "Active open source development",
      "Cross-platform"
    ],
    "disadvantages": [
      "Optimized for Intel",
      "Some advanced ops need work"
    ]
  },

  "metadata": {
    "version": "2024.0",
    "last_updated": "2025-12-20",
    "maintainer": "Intel",
    "license": "Apache-2.0"
  }
}
