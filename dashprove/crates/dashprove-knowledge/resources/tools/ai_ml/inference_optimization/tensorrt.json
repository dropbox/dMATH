{
  "id": "tensorrt",
  "name": "NVIDIA TensorRT",
  "category": "ai_ml",
  "subcategory": "inference_optimization",

  "description": "NVIDIA SDK for high-performance deep learning inference",
  "long_description": "TensorRT is NVIDIA's SDK for high-performance deep learning inference. It includes a deep learning inference optimizer and runtime that delivers low latency and high throughput for inference applications. TensorRT optimizes neural networks through layer fusion, kernel auto-tuning, precision calibration, and more.",

  "capabilities": [
    "inference_optimization",
    "layer_fusion",
    "kernel_auto_tuning",
    "fp16_quantization",
    "int8_quantization",
    "dynamic_shapes",
    "plugin_support",
    "multi_gpu_inference"
  ],
  "property_types": ["performance", "accuracy"],
  "input_languages": ["onnx", "tensorflow", "pytorch", "caffe"],
  "output_formats": ["trt_engine", "inference_result", "benchmark"],

  "installation": {
    "methods": [
      {"type": "pip", "command": "pip install tensorrt"},
      {"type": "download", "url": "https://developer.nvidia.com/tensorrt"}
    ],
    "dependencies": ["cuda", "cudnn", "nvidia_gpu"],
    "platforms": ["linux", "windows"]
  },

  "documentation": {
    "official": "https://developer.nvidia.com/tensorrt",
    "tutorial": "https://docs.nvidia.com/deeplearning/tensorrt/quick-start-guide/",
    "api_reference": "https://docs.nvidia.com/deeplearning/tensorrt/api/",
    "examples": "https://github.com/NVIDIA/TensorRT/tree/main/samples"
  },

  "tactics": [
    {
      "name": "build_engine",
      "description": "Build optimized TensorRT engine",
      "syntax": "trtexec --onnx=model.onnx --saveEngine=model.trt",
      "when_to_use": "Converting model for inference",
      "examples": ["trtexec --onnx=resnet50.onnx --saveEngine=resnet50.trt --fp16"]
    },
    {
      "name": "benchmark",
      "description": "Benchmark inference performance",
      "syntax": "trtexec --loadEngine=model.trt --benchmark",
      "when_to_use": "Measuring latency and throughput",
      "examples": ["trtexec --loadEngine=model.trt --iterations=1000 --warmUp=100"]
    },
    {
      "name": "int8_calibration",
      "description": "Calibrate for INT8 precision",
      "syntax": "trtexec --onnx=model.onnx --int8 --calib=calibration_data",
      "when_to_use": "Maximum performance with quantization",
      "examples": ["trtexec --onnx=model.onnx --int8 --calib=calib_images/"]
    },
    {
      "name": "dynamic_shapes",
      "description": "Build with dynamic input shapes",
      "syntax": "trtexec --onnx=model.onnx --minShapes=... --optShapes=... --maxShapes=...",
      "when_to_use": "Variable batch size or input dimensions",
      "examples": ["trtexec --onnx=model.onnx --minShapes=input:1x3x224x224 --maxShapes=input:32x3x224x224"]
    }
  ],

  "error_patterns": [
    {
      "pattern": "Unsupported layer",
      "meaning": "Operation not supported by TensorRT",
      "common_causes": ["Custom op", "Unsupported attribute"],
      "fixes": ["Write TensorRT plugin", "Simplify model", "Use different export"]
    },
    {
      "pattern": "Out of memory",
      "meaning": "GPU memory exhausted",
      "common_causes": ["Large model", "Large batch size"],
      "fixes": ["Reduce batch size", "Use smaller precision", "Enable memory pooling"]
    },
    {
      "pattern": "Accuracy drop",
      "meaning": "Quantization degraded accuracy",
      "common_causes": ["Poor calibration", "Sensitive layers"],
      "fixes": ["Better calibration data", "Keep sensitive layers in FP32"]
    }
  ],

  "integration": {
    "dashprove_backend": true,
    "usl_property_types": ["model_optimization"],
    "cli_command": "dashprove verify --backend tensorrt"
  },

  "performance": {
    "typical_runtime": "Minutes for build, milliseconds for inference",
    "scalability": "Excellent on NVIDIA GPUs",
    "memory_usage": "Varies by model and precision"
  },

  "comparisons": {
    "similar_tools": ["onnxruntime", "openvino", "tvm"],
    "advantages": [
      "Best performance on NVIDIA GPUs",
      "Extensive optimization techniques",
      "Production-ready",
      "Multi-GPU support"
    ],
    "disadvantages": [
      "NVIDIA GPUs only",
      "Complex optimization process",
      "Some ops need plugins"
    ]
  },

  "metadata": {
    "version": "10.0",
    "last_updated": "2025-12-20",
    "maintainer": "NVIDIA",
    "license": "NVIDIA Proprietary"
  }
}
