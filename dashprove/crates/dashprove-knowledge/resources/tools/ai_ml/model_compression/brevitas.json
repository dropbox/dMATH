{
  "id": "brevitas",
  "name": "Brevitas",
  "category": "ai_ml",
  "subcategory": "model_compression",

  "description": "PyTorch library for quantization-aware training with flexible bit-widths",
  "long_description": "Brevitas is a PyTorch library for training neural networks with reduced precision. It supports flexible quantization with various bit-widths and quantization schemes, making it ideal for research and FPGA deployment. Features include learned quantization parameters and export to various formats.",

  "capabilities": [
    "quantization_aware_training",
    "flexible_bitwidth",
    "learned_quantization",
    "binary_networks",
    "ternary_networks",
    "mixed_precision",
    "finn_export",
    "onnx_export"
  ],
  "property_types": ["accuracy", "bitwidth", "compression_ratio"],
  "input_languages": ["pytorch"],
  "output_formats": ["quantized_model", "onnx", "finn"],

  "installation": {
    "methods": [
      {"type": "pip", "command": "pip install brevitas"},
      {"type": "source", "url": "https://github.com/Xilinx/brevitas"}
    ],
    "dependencies": ["pytorch"],
    "platforms": ["linux", "macos", "windows"]
  },

  "documentation": {
    "official": "https://xilinx.github.io/brevitas/",
    "tutorial": "https://xilinx.github.io/brevitas/tutorials/",
    "api_reference": "https://xilinx.github.io/brevitas/api_reference/",
    "examples": "https://github.com/Xilinx/brevitas/tree/master/src/brevitas_examples"
  },

  "tactics": [
    {
      "name": "quant_module",
      "description": "Replace layer with quantized version",
      "syntax": "QuantLinear(in_features, out_features, bit_width)",
      "when_to_use": "Building quantized network",
      "examples": ["layer = QuantLinear(128, 64, bit_width=8, weight_quant=Int8WeightPerTensorFloat)"]
    },
    {
      "name": "quant_identity",
      "description": "Quantize activations",
      "syntax": "QuantIdentity(bit_width, act_quant)",
      "when_to_use": "Adding activation quantization",
      "examples": ["act = QuantIdentity(bit_width=8, act_quant=Int8ActPerTensorFloat)"]
    },
    {
      "name": "export_onnx",
      "description": "Export to ONNX format",
      "syntax": "export_onnx_qcdq(model, args, path)",
      "when_to_use": "Deploying quantized model",
      "examples": ["export_onnx_qcdq(model, (input,), 'model_quant.onnx')"]
    },
    {
      "name": "export_finn",
      "description": "Export for FINN FPGA",
      "syntax": "export_finn_onnx(model, args, path)",
      "when_to_use": "FPGA deployment",
      "examples": ["export_finn_onnx(model, (input,), 'model_finn.onnx')"]
    }
  ],

  "error_patterns": [
    {
      "pattern": "Quantizer mismatch",
      "meaning": "Incompatible quantization configs",
      "common_causes": ["Wrong quantizer type", "Bit-width mismatch"],
      "fixes": ["Match input/output quantizers", "Check configs"]
    },
    {
      "pattern": "Training instability",
      "meaning": "Quantization caused divergence",
      "common_causes": ["Too aggressive quantization", "Wrong initialization"],
      "fixes": ["Start with higher precision", "Use STE properly"]
    }
  ],

  "integration": {
    "dashprove_backend": true,
    "usl_property_types": ["model_compression"],
    "cli_command": "dashprove verify --backend brevitas"
  },

  "performance": {
    "typical_runtime": "Training time",
    "scalability": "Good for various models",
    "memory_usage": "Reduced after quantization"
  },

  "comparisons": {
    "similar_tools": ["nncf", "pytorch_quantization", "qat"],
    "advantages": [
      "Flexible bit-width support",
      "Research-friendly",
      "FPGA export (FINN)",
      "Learned quantization"
    ],
    "disadvantages": [
      "More manual setup",
      "Smaller community",
      "Xilinx-focused export"
    ]
  },

  "metadata": {
    "version": "0.10.2",
    "last_updated": "2025-12-20",
    "maintainer": "Xilinx (AMD)",
    "license": "BSD-3-Clause"
  }
}
