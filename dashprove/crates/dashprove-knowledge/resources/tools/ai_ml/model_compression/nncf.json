{
  "id": "nncf",
  "name": "NNCF",
  "category": "ai_ml",
  "subcategory": "model_compression",

  "description": "Neural Network Compression Framework for training-time compression",
  "long_description": "NNCF (Neural Network Compression Framework) is Intel's framework for applying quantization, pruning, and other compression methods during training. It integrates tightly with PyTorch and enables training compressed models from scratch or fine-tuning pre-trained models with compression.",

  "capabilities": [
    "quantization_aware_training",
    "structured_pruning",
    "filter_pruning",
    "knowledge_distillation",
    "sparsity_training",
    "mixed_precision_training",
    "openvino_export"
  ],
  "property_types": ["accuracy", "compression_ratio", "sparsity"],
  "input_languages": ["pytorch", "tensorflow"],
  "output_formats": ["compressed_model", "onnx", "openvino_ir"],

  "installation": {
    "methods": [
      {"type": "pip", "command": "pip install nncf"},
      {"type": "source", "url": "https://github.com/openvinotoolkit/nncf"}
    ],
    "dependencies": ["pytorch", "openvino"],
    "platforms": ["linux", "macos", "windows"]
  },

  "documentation": {
    "official": "https://github.com/openvinotoolkit/nncf",
    "tutorial": "https://github.com/openvinotoolkit/nncf/blob/develop/docs/Usage.md",
    "api_reference": "https://openvinotoolkit.github.io/nncf/",
    "examples": "https://github.com/openvinotoolkit/nncf/tree/develop/examples"
  },

  "tactics": [
    {
      "name": "create_compressed_model",
      "description": "Create compression-ready model",
      "syntax": "nncf.create_compressed_model(model, config)",
      "when_to_use": "Setting up model for compression training",
      "examples": ["compressed_model, compression_ctrl = nncf.create_compressed_model(model, nncf_config)"]
    },
    {
      "name": "quantize",
      "description": "Apply quantization-aware training",
      "syntax": "nncf.quantization.quantize(model, calibration_dataset)",
      "when_to_use": "INT8 quantization during training",
      "examples": ["q_model = nncf.quantize(model, calibration_data)"]
    },
    {
      "name": "prune",
      "description": "Apply filter pruning",
      "syntax": "nncf.prune(model, pruning_config)",
      "when_to_use": "Reducing number of filters",
      "examples": ["pruned = nncf.prune(model, target_sparsity=0.5)"]
    },
    {
      "name": "export",
      "description": "Export to OpenVINO format",
      "syntax": "compression_ctrl.export_model(path)",
      "when_to_use": "Deploying compressed model",
      "examples": ["compression_ctrl.export_model('model_int8.onnx')"]
    }
  ],

  "error_patterns": [
    {
      "pattern": "Incompatible layer",
      "meaning": "Layer cannot be compressed",
      "common_causes": ["Custom layer", "Unsupported op"],
      "fixes": ["Add to ignored list", "Register custom handler"]
    },
    {
      "pattern": "Training diverged",
      "meaning": "Compression caused training instability",
      "common_causes": ["Aggressive compression", "Wrong learning rate"],
      "fixes": ["Gradual compression", "Adjust learning rate"]
    }
  ],

  "integration": {
    "dashprove_backend": true,
    "usl_property_types": ["model_compression"],
    "cli_command": "dashprove verify --backend nncf"
  },

  "performance": {
    "typical_runtime": "Training time + compression overhead",
    "scalability": "Good for various architectures",
    "memory_usage": "Similar to training"
  },

  "comparisons": {
    "similar_tools": ["neuralcompressor", "aimet", "pytorch_quantization"],
    "advantages": [
      "Training-time compression",
      "OpenVINO integration",
      "Structured pruning support",
      "Good accuracy preservation"
    ],
    "disadvantages": [
      "Requires retraining",
      "More complex than PTQ"
    ]
  },

  "metadata": {
    "version": "2.8.0",
    "last_updated": "2025-12-20",
    "maintainer": "Intel (OpenVINO)",
    "license": "Apache-2.0"
  }
}
