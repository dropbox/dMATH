{
  "id": "deepeval",
  "name": "DeepEval",
  "category": "llm",
  "subcategory": "evaluation",

  "description": "Unit testing framework for LLM applications",
  "long_description": "DeepEval is an open-source LLM evaluation framework that provides unit testing capabilities for LLM outputs. It includes metrics for hallucination detection, answer relevancy, contextual relevancy, and custom metrics. Integrates with pytest for CI/CD workflows.",

  "capabilities": [
    "hallucination_detection",
    "answer_relevancy",
    "contextual_precision",
    "faithfulness",
    "toxicity_detection",
    "bias_detection",
    "custom_metrics",
    "pytest_integration"
  ],
  "property_types": ["llm_evaluation", "hallucination", "relevancy"],
  "input_languages": ["python"],
  "output_formats": ["test_results", "metrics", "dashboard"],

  "installation": {
    "methods": [
      {"type": "pip", "command": "pip install deepeval"},
      {"type": "source", "url": "https://github.com/confident-ai/deepeval"}
    ],
    "dependencies": ["openai", "pytest"],
    "platforms": ["linux", "macos", "windows"]
  },

  "documentation": {
    "official": "https://docs.confident-ai.com/",
    "tutorial": "https://docs.confident-ai.com/docs/getting-started",
    "api_reference": "https://docs.confident-ai.com/docs/metrics-introduction",
    "examples": "https://github.com/confident-ai/deepeval/tree/main/examples"
  },

  "tactics": [
    {
      "name": "test_case",
      "description": "Create LLM test case",
      "syntax": "LLMTestCase(input, actual_output, expected_output)",
      "when_to_use": "Defining test scenarios",
      "examples": ["test_case = LLMTestCase(input='What is 2+2?', actual_output='4', expected_output='4')"]
    },
    {
      "name": "evaluate",
      "description": "Run evaluation metrics",
      "syntax": "evaluate([test_case], metrics=[HallucinationMetric()])",
      "when_to_use": "Testing LLM outputs",
      "examples": ["evaluate([test_case], [AnswerRelevancyMetric(), HallucinationMetric()])"]
    },
    {
      "name": "assert_test",
      "description": "Assert in pytest",
      "syntax": "assert_test(test_case, metrics)",
      "when_to_use": "CI/CD integration",
      "examples": ["assert_test(test_case, [FaithfulnessMetric(threshold=0.7)])"]
    },
    {
      "name": "custom_metric",
      "description": "Define custom metric",
      "syntax": "class CustomMetric(BaseMetric): ...",
      "when_to_use": "Domain-specific evaluation",
      "examples": ["class AccuracyMetric(BaseMetric): def measure(self, test_case): ..."]
    }
  ],

  "error_patterns": [
    {
      "pattern": "Metric failed",
      "meaning": "Output did not meet threshold",
      "common_causes": ["Poor model output", "Wrong metric"],
      "fixes": ["Improve prompt", "Adjust threshold"]
    },
    {
      "pattern": "API error",
      "meaning": "Evaluation API call failed",
      "common_causes": ["Rate limit", "Invalid key"],
      "fixes": ["Add retry logic", "Check API key"]
    }
  ],

  "integration": {
    "dashprove_backend": true,
    "usl_property_types": ["llm_evaluation", "hallucination_detection"],
    "cli_command": "dashprove verify --backend deepeval"
  },

  "performance": {
    "typical_runtime": "Seconds per test case",
    "scalability": "Good batch evaluation",
    "memory_usage": "Moderate"
  },

  "comparisons": {
    "similar_tools": ["trulens", "ragas", "promptfoo"],
    "advantages": [
      "pytest integration",
      "Many built-in metrics",
      "Good for CI/CD",
      "Active development"
    ],
    "disadvantages": [
      "Some metrics need API calls",
      "Newer project"
    ]
  },

  "metadata": {
    "version": "0.21.0",
    "last_updated": "2025-12-20",
    "maintainer": "Confident AI",
    "license": "Apache-2.0"
  }
}
