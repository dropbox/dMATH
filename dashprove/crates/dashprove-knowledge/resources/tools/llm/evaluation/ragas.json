{
  "id": "ragas",
  "name": "RAGAS",
  "category": "llm_evaluation",
  "subcategory": "rag_evaluation",

  "description": "Evaluation framework for Retrieval Augmented Generation",
  "long_description": "RAGAS (Retrieval Augmented Generation Assessment) is a framework for evaluating RAG pipelines. It provides metrics like faithfulness, answer relevancy, context precision, and context recall. RAGAS can evaluate without ground truth labels using LLM-based evaluation.",

  "capabilities": [
    "faithfulness_evaluation",
    "answer_relevancy",
    "context_precision",
    "context_recall",
    "context_relevancy",
    "harmfulness_detection",
    "reference_free_evaluation",
    "batch_evaluation"
  ],
  "property_types": ["rag_quality", "llm_quality"],
  "input_languages": ["python", "langchain", "llamaindex"],
  "output_formats": ["scores", "metrics_report"],

  "installation": {
    "methods": [
      {"type": "pip", "command": "pip install ragas"},
      {"type": "source", "url": "https://github.com/explodinggradients/ragas"}
    ],
    "dependencies": ["python", "openai/anthropic", "datasets"],
    "platforms": ["linux", "macos", "windows"]
  },

  "documentation": {
    "official": "https://docs.ragas.io/",
    "tutorial": "https://docs.ragas.io/en/latest/getstarted/index.html",
    "api_reference": "https://docs.ragas.io/en/latest/references/",
    "examples": "https://github.com/explodinggradients/ragas/tree/main/docs/howtos"
  },

  "tactics": [
    {
      "name": "evaluate",
      "description": "Run evaluation metrics",
      "syntax": "evaluate(dataset, metrics=[...], llm=...)",
      "when_to_use": "Evaluating RAG pipeline quality",
      "examples": ["from ragas import evaluate\nresult = evaluate(dataset, metrics=[faithfulness, answer_relevancy])"]
    },
    {
      "name": "faithfulness",
      "description": "Check if answer is faithful to context",
      "syntax": "faithfulness metric",
      "when_to_use": "Detecting hallucinations",
      "examples": ["from ragas.metrics import faithfulness\nresult = evaluate(dataset, metrics=[faithfulness])"]
    },
    {
      "name": "context_precision",
      "description": "Measure context relevance",
      "syntax": "context_precision metric",
      "when_to_use": "Evaluating retrieval quality",
      "examples": ["from ragas.metrics import context_precision\nresult = evaluate(dataset, metrics=[context_precision])"]
    },
    {
      "name": "synthetic_data",
      "description": "Generate test data",
      "syntax": "TestsetGenerator.from_documents(documents)",
      "when_to_use": "Creating evaluation dataset",
      "examples": ["generator = TestsetGenerator.from_documents(docs)\ntestset = generator.generate(test_size=100)"]
    }
  ],

  "error_patterns": [
    {
      "pattern": "Low faithfulness",
      "meaning": "Answer contains hallucinated information",
      "common_causes": ["Insufficient context", "Model confabulation"],
      "fixes": ["Improve retrieval", "Constrain generation"]
    },
    {
      "pattern": "Low context recall",
      "meaning": "Missing relevant context",
      "common_causes": ["Poor retrieval", "Wrong chunk size"],
      "fixes": ["Tune retrieval parameters", "Adjust chunking"]
    },
    {
      "pattern": "Low answer relevancy",
      "meaning": "Answer doesn't address question",
      "common_causes": ["Poor prompt", "Context confusion"],
      "fixes": ["Improve prompt template"]
    }
  ],

  "integration": {
    "dashprove_backend": true,
    "usl_property_types": ["rag_quality"],
    "cli_command": "dashprove verify --backend ragas"
  },

  "performance": {
    "typical_runtime": "Depends on dataset size and LLM",
    "scalability": "Batch processing supported",
    "memory_usage": "Moderate"
  },

  "comparisons": {
    "similar_tools": ["trulens", "langsmith", "deepeval"],
    "advantages": ["RAG-focused metrics", "Reference-free evaluation", "Synthetic data generation", "Active development"],
    "disadvantages": ["LLM cost for evaluation", "Some metrics require ground truth"]
  },

  "metadata": {
    "version": "0.1.0",
    "last_updated": "2025-12-20",
    "maintainer": "Exploding Gradients",
    "license": "Apache-2.0"
  }
}
