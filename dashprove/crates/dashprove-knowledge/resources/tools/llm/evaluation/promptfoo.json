{
  "id": "promptfoo",
  "name": "Promptfoo",
  "category": "llm_evaluation",
  "subcategory": "testing",

  "description": "Test and evaluate LLM prompts and models",
  "long_description": "Promptfoo is a tool for testing LLM applications. It enables systematic evaluation of prompts across multiple models, measuring quality with various assertions and metrics. Used for prompt engineering, regression testing, and model comparison.",

  "capabilities": [
    "prompt_testing",
    "model_comparison",
    "assertion_evaluation",
    "regression_testing",
    "red_teaming",
    "caching"
  ],
  "property_types": ["assertion", "metric", "eval"],
  "input_languages": ["yaml", "javascript", "python"],
  "output_formats": ["json", "html", "csv"],

  "installation": {
    "methods": [
      {"type": "npm", "command": "npm install -g promptfoo"},
      {"type": "pip", "command": "pip install promptfoo"}
    ],
    "dependencies": [],
    "platforms": ["linux", "macos", "windows"]
  },

  "documentation": {
    "official": "https://promptfoo.dev/",
    "tutorial": "https://promptfoo.dev/docs/getting-started/",
    "api_reference": "https://promptfoo.dev/docs/configuration/",
    "examples": "https://promptfoo.dev/docs/guides/"
  },

  "tactics": [
    {
      "name": "eval",
      "description": "Run evaluation",
      "syntax": "promptfoo eval",
      "when_to_use": "To test prompts",
      "examples": ["promptfoo eval -c config.yaml"]
    },
    {
      "name": "assert",
      "description": "Add assertion",
      "syntax": "assert: [{type: 'contains', value: 'expected'}]",
      "when_to_use": "To validate outputs",
      "examples": ["assert:\n  - type: icontains\n    value: 'success'"]
    },
    {
      "name": "compare",
      "description": "Compare models",
      "syntax": "providers: [openai:gpt-4, anthropic:claude-3]",
      "when_to_use": "To benchmark models",
      "examples": ["providers:\n  - openai:gpt-4\n  - openai:gpt-3.5-turbo"]
    },
    {
      "name": "redteam",
      "description": "Red team testing",
      "syntax": "promptfoo redteam generate",
      "when_to_use": "To find vulnerabilities",
      "examples": ["promptfoo redteam generate -c config.yaml"]
    }
  ],

  "error_patterns": [
    {
      "pattern": "Assertion failed",
      "meaning": "Output didn't meet criteria",
      "common_causes": ["Prompt issue", "Model limitation"],
      "fixes": ["Improve prompt", "Adjust assertion"]
    },
    {
      "pattern": "Rate limit",
      "meaning": "API rate limit hit",
      "common_causes": ["Too many requests"],
      "fixes": ["Add delay", "Use caching"]
    }
  ],

  "integration": {
    "dashprove_backend": true,
    "usl_property_types": ["assertion", "metric"],
    "cli_command": "dashprove verify --backend promptfoo"
  },

  "performance": {
    "typical_runtime": "depends on API calls",
    "scalability": "Good with caching",
    "memory_usage": "Low"
  },

  "comparisons": {
    "similar_tools": ["trulens", "langsmith", "ragas"],
    "advantages": [
      "Easy to use",
      "Good CLI",
      "Model-agnostic",
      "CI/CD integration"
    ],
    "disadvantages": [
      "Less sophisticated metrics",
      "Manual test case creation"
    ]
  },

  "metadata": {
    "version": "0.70.0",
    "last_updated": "2025-12-20",
    "maintainer": "Promptfoo",
    "license": "MIT"
  }
}
