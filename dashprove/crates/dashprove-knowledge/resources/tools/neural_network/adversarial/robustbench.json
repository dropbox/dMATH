{
  "id": "robustbench",
  "name": "RobustBench",
  "category": "neural_network",
  "subcategory": "adversarial",

  "description": "Standardized benchmark for adversarial robustness evaluation",
  "long_description": "RobustBench is a standardized benchmark for evaluating adversarial robustness of image classifiers. It provides a model zoo of pre-trained robust models, a leaderboard for comparing defenses, and standardized evaluation protocols using AutoAttack for reliable robustness assessment.",

  "capabilities": [
    "robustness_benchmarking",
    "model_zoo",
    "standardized_evaluation",
    "autoattack_integration",
    "leaderboard_submission",
    "pretrained_models"
  ],
  "property_types": ["adversarial_robustness", "clean_accuracy"],
  "input_languages": ["pytorch"],
  "output_formats": ["robustness_score", "evaluation_log"],

  "installation": {
    "methods": [
      {"type": "pip", "command": "pip install robustbench"},
      {"type": "source", "url": "https://github.com/RobustBench/robustbench"}
    ],
    "dependencies": ["pytorch", "torchvision", "autoattack"],
    "platforms": ["linux", "macos", "windows"]
  },

  "documentation": {
    "official": "https://robustbench.github.io/",
    "tutorial": "https://github.com/RobustBench/robustbench/blob/master/README.md",
    "api_reference": "https://robustbench.readthedocs.io/",
    "examples": "https://github.com/RobustBench/robustbench/tree/master/notebooks"
  },

  "tactics": [
    {
      "name": "load_model",
      "description": "Load pre-trained robust model",
      "syntax": "load_model(model_name, dataset, threat_model)",
      "when_to_use": "Getting benchmark robust models",
      "examples": ["model = load_model('Carmon2019Unlabeled', dataset='cifar10', threat_model='Linf')"]
    },
    {
      "name": "benchmark",
      "description": "Evaluate model robustness",
      "syntax": "benchmark(model, dataset, threat_model)",
      "when_to_use": "Standardized robustness evaluation",
      "examples": ["clean_acc, robust_acc = benchmark(model, x_test, y_test, threat_model='Linf')"]
    },
    {
      "name": "list_models",
      "description": "List available models",
      "syntax": "list_models(dataset, threat_model)",
      "when_to_use": "Finding available benchmark models",
      "examples": ["models = list_models(dataset='cifar10', threat_model='Linf')"]
    },
    {
      "name": "clean_accuracy",
      "description": "Evaluate clean accuracy",
      "syntax": "clean_accuracy(model, x, y)",
      "when_to_use": "Checking standard accuracy",
      "examples": ["acc = clean_accuracy(model, x_test, y_test)"]
    }
  ],

  "error_patterns": [
    {
      "pattern": "Model not found",
      "meaning": "Requested model not in zoo",
      "common_causes": ["Typo in model name", "Model not uploaded"],
      "fixes": ["Check model list", "Use correct identifier"]
    },
    {
      "pattern": "Evaluation timeout",
      "meaning": "AutoAttack took too long",
      "common_causes": ["Large dataset", "Strong defense"],
      "fixes": ["Reduce test samples", "Increase timeout"]
    }
  ],

  "integration": {
    "dashprove_backend": true,
    "usl_property_types": ["adversarial_robustness"],
    "cli_command": "dashprove verify --backend robustbench"
  },

  "performance": {
    "typical_runtime": "Minutes to hours for full evaluation",
    "scalability": "Standard benchmark sizes (10K images)",
    "memory_usage": "Moderate to high"
  },

  "comparisons": {
    "similar_tools": ["autoattack", "foolbox", "art"],
    "advantages": [
      "Standardized evaluation protocol",
      "Large model zoo",
      "Community leaderboard",
      "Reliable robustness numbers"
    ],
    "disadvantages": [
      "Image classification focus",
      "Limited threat models"
    ]
  },

  "metadata": {
    "version": "1.1",
    "last_updated": "2025-12-20",
    "maintainer": "RobustBench Team",
    "license": "MIT"
  }
}
