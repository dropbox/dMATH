{
  "id": "art",
  "name": "Adversarial Robustness Toolbox (ART)",
  "category": "adversarial_robustness",
  "subcategory": "attack_defense",

  "description": "IBM library for ML security - attacks, defenses, and evaluation",
  "long_description": "The Adversarial Robustness Toolbox (ART) is a Python library for machine learning security from IBM Research. It provides tools to evaluate and improve the robustness of ML models against adversarial attacks. ART supports evasion attacks, poisoning attacks, extraction attacks, and various defense mechanisms.",

  "capabilities": [
    "evasion_attacks",
    "poisoning_attacks",
    "extraction_attacks",
    "inference_attacks",
    "certified_defenses",
    "preprocessing_defenses",
    "postprocessing_defenses",
    "detector_training",
    "framework_agnostic"
  ],
  "property_types": ["adversarial_robustness", "security"],
  "input_languages": ["pytorch", "tensorflow", "keras", "scikit-learn"],
  "output_formats": ["adversarial_examples", "robustness_metrics", "defense_evaluation"],

  "installation": {
    "methods": [
      {"type": "pip", "command": "pip install adversarial-robustness-toolbox"},
      {"type": "source", "url": "https://github.com/Trusted-AI/adversarial-robustness-toolbox"}
    ],
    "dependencies": ["python", "numpy", "pytorch/tensorflow"],
    "platforms": ["linux", "macos", "windows"]
  },

  "documentation": {
    "official": "https://adversarial-robustness-toolbox.readthedocs.io/",
    "tutorial": "https://adversarial-robustness-toolbox.readthedocs.io/en/latest/guide/",
    "api_reference": "https://adversarial-robustness-toolbox.readthedocs.io/en/latest/modules/",
    "examples": "https://github.com/Trusted-AI/adversarial-robustness-toolbox/tree/main/notebooks"
  },

  "tactics": [
    {
      "name": "fgsm",
      "description": "Fast Gradient Sign Method attack",
      "syntax": "FastGradientMethod(estimator, eps=0.3)",
      "when_to_use": "Quick robustness evaluation",
      "examples": ["attack = FastGradientMethod(classifier, eps=0.3)\nadv_x = attack.generate(x)"]
    },
    {
      "name": "pgd",
      "description": "Projected Gradient Descent attack",
      "syntax": "ProjectedGradientDescent(estimator, eps=0.3, max_iter=100)",
      "when_to_use": "Strong iterative attack",
      "examples": ["attack = ProjectedGradientDescent(classifier, eps=0.3, max_iter=100)"]
    },
    {
      "name": "certified_defense",
      "description": "Apply certified defense",
      "syntax": "PixelDefend(estimator)",
      "when_to_use": "Adding provable robustness",
      "examples": ["defense = PixelDefend(classifier)\ndefended_x = defense(x)"]
    },
    {
      "name": "adversarial_training",
      "description": "Train with adversarial examples",
      "syntax": "AdversarialTrainer(classifier, attack)",
      "when_to_use": "Improving model robustness",
      "examples": ["trainer = AdversarialTrainer(classifier, attack)\ntrainer.fit(x, y)"]
    }
  ],

  "error_patterns": [
    {
      "pattern": "Attack success rate",
      "meaning": "Percentage of adversarial examples that fool model",
      "common_causes": ["Model vulnerability to attack type"],
      "fixes": ["Apply appropriate defense", "Adversarial training"]
    },
    {
      "pattern": "Certified accuracy",
      "meaning": "Accuracy guaranteed under perturbations",
      "common_causes": ["Robustness metric"],
      "fixes": ["Target higher certified accuracy via training"]
    }
  ],

  "integration": {
    "dashprove_backend": true,
    "usl_property_types": ["adversarial_robustness"],
    "cli_command": "dashprove verify --backend art"
  },

  "performance": {
    "typical_runtime": "Varies by attack complexity",
    "scalability": "Handles large models",
    "memory_usage": "Moderate to high"
  },

  "comparisons": {
    "similar_tools": ["foolbox", "cleverhans", "robustbench"],
    "advantages": ["Comprehensive attack library", "Defense implementations", "Active IBM support", "Framework agnostic"],
    "disadvantages": ["Large dependency", "Some attacks slow", "Complex API"]
  },

  "metadata": {
    "version": "1.15.0",
    "last_updated": "2025-12-20",
    "maintainer": "IBM Research",
    "license": "MIT"
  }
}
