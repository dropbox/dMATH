{
  "id": "foolbox",
  "name": "Foolbox",
  "category": "adversarial_robustness",
  "subcategory": "attack",

  "description": "Python toolbox for creating adversarial examples",
  "long_description": "Foolbox is a Python toolbox to create adversarial examples that fool neural networks. It supports many popular deep learning frameworks (PyTorch, TensorFlow, JAX) and provides both gradient-based and decision-based attacks. Foolbox emphasizes a clean API and reliable implementations.",

  "capabilities": [
    "gradient_attacks",
    "decision_based_attacks",
    "score_based_attacks",
    "lp_norm_attacks",
    "targeted_attacks",
    "multi_framework",
    "batch_attacks",
    "minimal_perturbation"
  ],
  "property_types": ["adversarial_robustness"],
  "input_languages": ["pytorch", "tensorflow", "jax"],
  "output_formats": ["adversarial_examples", "perturbation_bounds"],

  "installation": {
    "methods": [
      {"type": "pip", "command": "pip install foolbox"},
      {"type": "source", "url": "https://github.com/bethgelab/foolbox"}
    ],
    "dependencies": ["python", "numpy", "eagerpy"],
    "platforms": ["linux", "macos", "windows"]
  },

  "documentation": {
    "official": "https://foolbox.readthedocs.io/",
    "tutorial": "https://foolbox.readthedocs.io/en/stable/guide/getting-started.html",
    "api_reference": "https://foolbox.readthedocs.io/en/stable/modules/attacks.html",
    "examples": "https://github.com/bethgelab/foolbox/tree/master/examples"
  },

  "tactics": [
    {
      "name": "fgsm",
      "description": "Fast Gradient Sign Method",
      "syntax": "fb.attacks.FGSM()",
      "when_to_use": "Quick single-step attack",
      "examples": ["attack = fb.attacks.FGSM()\nadv, _, success = attack(fmodel, images, labels, epsilons=[0.03])"]
    },
    {
      "name": "pgd",
      "description": "Projected Gradient Descent",
      "syntax": "fb.attacks.LinfPGD()",
      "when_to_use": "Strong iterative gradient attack",
      "examples": ["attack = fb.attacks.LinfPGD()\nadv, _, _ = attack(fmodel, images, labels, epsilons=[0.03])"]
    },
    {
      "name": "boundary",
      "description": "Boundary Attack",
      "syntax": "fb.attacks.BoundaryAttack()",
      "when_to_use": "Decision-based attack (no gradients)",
      "examples": ["attack = fb.attacks.BoundaryAttack()\nadv = attack(fmodel, images, labels)"]
    },
    {
      "name": "deepfool",
      "description": "DeepFool attack",
      "syntax": "fb.attacks.LinfDeepFoolAttack()",
      "when_to_use": "Finding minimal perturbations",
      "examples": ["attack = fb.attacks.LinfDeepFoolAttack()\nadv, _, _ = attack(fmodel, images, labels)"]
    }
  ],

  "error_patterns": [
    {
      "pattern": "success rate",
      "meaning": "Proportion of successful adversarial examples",
      "common_causes": ["Model robustness level"],
      "fixes": ["Try stronger attack", "Increase epsilon"]
    },
    {
      "pattern": "perturbation size",
      "meaning": "Size of adversarial perturbation",
      "common_causes": ["Attack parameters"],
      "fixes": ["Adjust epsilon", "Try different attack"]
    }
  ],

  "integration": {
    "dashprove_backend": true,
    "usl_property_types": ["adversarial_robustness"],
    "cli_command": "dashprove verify --backend foolbox"
  },

  "performance": {
    "typical_runtime": "Milliseconds to seconds per sample",
    "scalability": "Batch processing supported",
    "memory_usage": "Moderate"
  },

  "comparisons": {
    "similar_tools": ["art", "cleverhans", "advertorch"],
    "advantages": ["Clean API", "Multi-framework", "Decision-based attacks", "Well documented"],
    "disadvantages": ["Fewer defenses", "Focused on attacks only"]
  },

  "metadata": {
    "version": "3.3.1",
    "last_updated": "2025-12-20",
    "maintainer": "Bethge Lab",
    "license": "MIT"
  }
}
