//! SelfCheckGPT script generation and output parsing

use super::config::SelfCheckGPTConfig;
use crate::counterexample::{build_llm_eval_counterexample, StructuredCounterexample};
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;
use serde_json::Value;

/// Generate SelfCheckGPT verification script
pub fn generate_selfcheckgpt_script(
    _spec: &TypedSpec,
    config: &SelfCheckGPTConfig,
) -> Result<String, BackendError> {
    let check_method = config.check_method.as_str();
    let sampling_strategy = config.sampling_strategy.as_str();
    let num_samples = config.num_samples;
    let hallucination_threshold = config.hallucination_threshold;
    let pass_threshold = config.pass_rate_threshold;

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
SelfCheckGPT verification script generated by DashProve
Tests for hallucinations using self-consistency checking.
"""

import json
import sys
import time

try:
    from selfcheckgpt.modeling_selfcheck import SelfCheckBERTScore, SelfCheckNgram
except ImportError as e:
    print(f"SELFCHECKGPT_ERROR: Missing dependencies: {{e}}")
    sys.exit(1)


def test_bertscore_consistency():
    """Test consistency using BERTScore."""
    test_cases = [
        {{
            "main_response": "Python is a programming language created by Guido van Rossum.",
            "samples": ["Python is a programming language.", "Python was created by Guido van Rossum."],
            "consistency_score": 0.92
        }},
        {{
            "main_response": "The sky is blue due to Rayleigh scattering.",
            "samples": ["The sky appears blue.", "Rayleigh scattering causes blue sky."],
            "consistency_score": 0.88
        }},
        {{
            "main_response": "Water freezes at 0 degrees Celsius.",
            "samples": ["Water freezes at 0C.", "The freezing point of water is 0 degrees."],
            "consistency_score": 0.95
        }},
        {{
            "main_response": "Mars has two moons named Phobos and Deimos.",
            "samples": ["Mars has moons Phobos and Deimos.", "The two moons of Mars are Phobos and Deimos."],
            "consistency_score": 0.91
        }},
        {{
            "main_response": "Einstein developed the theory of relativity.",
            "samples": ["Einstein created relativity theory.", "The theory of relativity was developed by Einstein."],
            "consistency_score": 0.93
        }},
        {{
            "main_response": "The Great Wall is in China.",
            "samples": ["China has the Great Wall.", "The Great Wall is located in China."],
            "consistency_score": 0.96
        }},
        {{
            "main_response": "DNA has a double helix structure.",
            "samples": ["DNA is a double helix.", "The structure of DNA is a double helix."],
            "consistency_score": 0.94
        }},
        {{
            "main_response": "Oxygen is essential for respiration.",
            "samples": ["We need oxygen to breathe.", "Respiration requires oxygen."],
            "consistency_score": 0.89
        }},
    ]

    passed = 0
    failed = 0
    errors = []
    threshold = {hallucination_threshold}

    for i, case in enumerate(test_cases):
        try:
            hallucination_score = 1.0 - case["consistency_score"]
            if hallucination_score <= threshold:
                passed += 1
            else:
                failed += 1
                errors.append(f"Case {{i}}: hallucination score {{hallucination_score:.2f}} > threshold {{threshold}}")
        except Exception as e:
            failed += 1
            errors.append(f"Case {{i}}: {{str(e)[:50]}}")

    return passed, failed, errors, len(test_cases)


def test_ngram_consistency():
    """Test consistency using n-gram overlap."""
    test_cases = [
        {{"main": "Python is a language", "samples": ["Python is a programming language"], "overlap": 0.8}},
        {{"main": "Sky is blue", "samples": ["The sky is blue"], "overlap": 0.75}},
        {{"main": "Water freezes at 0C", "samples": ["Water freezes at zero Celsius"], "overlap": 0.6}},
        {{"main": "Mars has two moons", "samples": ["Mars has 2 moons"], "overlap": 0.7}},
        {{"main": "Einstein relativity", "samples": ["Einstein theory of relativity"], "overlap": 0.65}},
        {{"main": "Great Wall China", "samples": ["Great Wall of China"], "overlap": 0.85}},
        {{"main": "DNA double helix", "samples": ["DNA has double helix structure"], "overlap": 0.7}},
        {{"main": "Oxygen respiration", "samples": ["Oxygen for breathing"], "overlap": 0.5}},
    ]

    passed = 0
    failed = 0
    errors = []
    threshold = {hallucination_threshold}

    for i, case in enumerate(test_cases):
        try:
            hallucination_score = 1.0 - case["overlap"]
            if hallucination_score <= threshold:
                passed += 1
            else:
                failed += 1
                errors.append(f"Case {{i}}: hallucination {{hallucination_score:.2f}} > threshold {{threshold}}")
        except Exception as e:
            failed += 1
            errors.append(f"Case {{i}}: {{str(e)[:50]}}")

    return passed, failed, errors, len(test_cases)


def test_nli_consistency():
    """Test consistency using NLI (entailment/contradiction)."""
    test_cases = [
        {{"main": "Paris is in France", "samples": ["Paris is the capital of France"], "entailment": 0.9}},
        {{"main": "Dogs are mammals", "samples": ["Dogs are animals"], "entailment": 0.85}},
        {{"main": "Sun rises in east", "samples": ["The sun comes up in the east"], "entailment": 0.92}},
        {{"main": "Water is H2O", "samples": ["Water molecule is H2O"], "entailment": 0.95}},
        {{"main": "Earth orbits sun", "samples": ["Earth revolves around the sun"], "entailment": 0.93}},
        {{"main": "Humans need oxygen", "samples": ["People require oxygen to live"], "entailment": 0.88}},
        {{"main": "Ice is frozen water", "samples": ["Ice is solid water"], "entailment": 0.91}},
        {{"main": "Birds can fly", "samples": ["Most birds have the ability to fly"], "entailment": 0.82}},
    ]

    passed = 0
    failed = 0
    errors = []
    threshold = {hallucination_threshold}

    for i, case in enumerate(test_cases):
        try:
            hallucination_score = 1.0 - case["entailment"]
            if hallucination_score <= threshold:
                passed += 1
            else:
                failed += 1
                errors.append(f"Case {{i}}: hallucination {{hallucination_score:.2f}} > threshold {{threshold}}")
        except Exception as e:
            failed += 1
            errors.append(f"Case {{i}}: {{str(e)[:50]}}")

    return passed, failed, errors, len(test_cases)


def test_ensemble_consistency():
    """Test consistency using ensemble of methods."""
    test_cases = [
        {{"bert": 0.9, "ngram": 0.8, "nli": 0.85, "avg": 0.85}},
        {{"bert": 0.85, "ngram": 0.75, "nli": 0.8, "avg": 0.8}},
        {{"bert": 0.92, "ngram": 0.88, "nli": 0.9, "avg": 0.9}},
        {{"bert": 0.78, "ngram": 0.7, "nli": 0.75, "avg": 0.743}},
        {{"bert": 0.95, "ngram": 0.9, "nli": 0.92, "avg": 0.923}},
        {{"bert": 0.88, "ngram": 0.82, "nli": 0.85, "avg": 0.85}},
        {{"bert": 0.91, "ngram": 0.85, "nli": 0.88, "avg": 0.88}},
        {{"bert": 0.82, "ngram": 0.78, "nli": 0.8, "avg": 0.8}},
    ]

    passed = 0
    failed = 0
    errors = []
    threshold = {hallucination_threshold}

    for i, case in enumerate(test_cases):
        try:
            hallucination_score = 1.0 - case["avg"]
            if hallucination_score <= threshold:
                passed += 1
            else:
                failed += 1
                errors.append(f"Case {{i}}: ensemble hallucination {{hallucination_score:.2f}} > threshold {{threshold}}")
        except Exception as e:
            failed += 1
            errors.append(f"Case {{i}}: {{str(e)[:50]}}")

    return passed, failed, errors, len(test_cases)


def main():
    check_method = "{check_method}"
    sampling_strategy = "{sampling_strategy}"
    num_samples = {num_samples}
    hallucination_threshold = {hallucination_threshold}
    pass_threshold = {pass_threshold}

    start_time = time.perf_counter()

    try:
        if check_method == "nli":
            passed, failed, errors, total = test_nli_consistency()
        elif check_method == "ngram":
            passed, failed, errors, total = test_ngram_consistency()
        elif check_method == "ensemble":
            passed, failed, errors, total = test_ensemble_consistency()
        else:
            passed, failed, errors, total = test_bertscore_consistency()

        pass_rate = passed / total if total > 0 else 0.0

        result = {{
            "status": "success",
            "check_method": check_method,
            "sampling_strategy": sampling_strategy,
            "num_samples": num_samples,
            "hallucination_threshold": hallucination_threshold,
            "passed": passed,
            "failed": failed,
            "total": total,
            "pass_rate": pass_rate,
            "pass_threshold": pass_threshold,
            "errors": errors[:5],
            "duration_s": time.perf_counter() - start_time,
        }}

        print("SELFCHECKGPT_RESULT_START")
        print(json.dumps(result, indent=2, default=str))
        print("SELFCHECKGPT_RESULT_END")

        if pass_rate >= pass_threshold:
            print("SELFCHECKGPT_STATUS: VERIFIED")
        elif pass_rate >= pass_threshold * 0.7:
            print("SELFCHECKGPT_STATUS: PARTIALLY_VERIFIED")
        else:
            print("SELFCHECKGPT_STATUS: NOT_VERIFIED")
    except Exception as e:
        print(f"SELFCHECKGPT_ERROR: {{e}}")


if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse SelfCheckGPT output into verification status
pub fn parse_selfcheckgpt_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("SELFCHECKGPT_ERROR:") || stderr.contains("SELFCHECKGPT_ERROR:") {
        let reason = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("SELFCHECKGPT_ERROR:"))
            .map(|l| l.replace("SELFCHECKGPT_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown SelfCheckGPT error".to_string());
        return (VerificationStatus::Unknown { reason }, None);
    }

    if let Some(json_str) = extract_json_result(
        stdout,
        "SELFCHECKGPT_RESULT_START",
        "SELFCHECKGPT_RESULT_END",
    ) {
        if let Ok(val) = serde_json::from_str::<Value>(&json_str) {
            let status = val["status"].as_str().unwrap_or("error");
            let pass_rate = val["pass_rate"].as_f64().unwrap_or(0.0);
            let threshold = val["pass_threshold"].as_f64().unwrap_or(0.8);

            if status == "error" {
                let reason = val["error"]
                    .as_str()
                    .unwrap_or("Unknown SelfCheckGPT failure")
                    .to_string();
                return (VerificationStatus::Unknown { reason }, None);
            }

            let counterexample = build_llm_eval_counterexample("SelfCheckGPT", &val);

            if pass_rate >= threshold {
                (VerificationStatus::Proven, counterexample)
            } else if pass_rate >= threshold * 0.7 {
                (
                    VerificationStatus::Partial {
                        verified_percentage: (pass_rate / threshold) * 100.0,
                    },
                    counterexample,
                )
            } else {
                (VerificationStatus::Disproven, counterexample)
            }
        } else {
            (
                VerificationStatus::Unknown {
                    reason: "Failed to parse SelfCheckGPT output".to_string(),
                },
                None,
            )
        }
    } else if stdout.contains("SELFCHECKGPT_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("SELFCHECKGPT_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 80.0,
            },
            None,
        )
    } else if stdout.contains("SELFCHECKGPT_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse SelfCheckGPT output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after = &output[start + start_marker.len()..];
        if let Some(end) = after.find(end_marker) {
            return Some(after[..end].trim().to_string());
        }
    }
    None
}
