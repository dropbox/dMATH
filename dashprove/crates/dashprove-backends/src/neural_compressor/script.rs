//! Intel Neural Compressor script generation and output parsing

use super::config::NeuralCompressorConfig;
use crate::counterexample::StructuredCounterexample;
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;

/// Generate a Neural Compressor quantization verification script
pub fn generate_nc_script(
    _spec: &TypedSpec,
    config: &NeuralCompressorConfig,
) -> Result<String, BackendError> {
    let approach = config.approach.as_str();
    let quant_dtype = config.quant_dtype.as_str();
    let calibration = config.calibration.as_str();
    let tuning_strategy = config.tuning_strategy.as_str();
    let accuracy_criterion = config.accuracy_criterion;
    let max_trials = config.max_trials;
    let calibration_samples = config.calibration_samples;
    let enable_pruning = if config.enable_pruning {
        "True"
    } else {
        "False"
    };
    let pruning_sparsity = config.pruning_sparsity;

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
Intel Neural Compressor quantization verification script
Generated by DashProve

Validates model quantization correctness and compression ratio.
"""

import sys
import json
import time
import numpy as np

try:
    import neural_compressor
    from neural_compressor import quantization
    from neural_compressor.config import PostTrainingQuantConfig, TuningCriterion, AccuracyCriterion
except ImportError as e:
    print(f"NC_ERROR: Missing Neural Compressor: {{e}}")
    print("NC_ERROR: Install with: pip install neural-compressor")
    sys.exit(1)

# Optional torch for model creation
try:
    import torch
    import torch.nn as nn
    HAS_TORCH = True
except ImportError:
    HAS_TORCH = False


class SimpleModel(nn.Module):
    """Simple test model for quantization."""
    def __init__(self):
        super().__init__()
        self.fc1 = nn.Linear(10, 20)
        self.relu = nn.ReLU()
        self.fc2 = nn.Linear(20, 5)

    def forward(self, x):
        x = self.fc1(x)
        x = self.relu(x)
        x = self.fc2(x)
        return x


def create_calibration_dataloader(num_samples):
    """Create a simple calibration dataloader."""
    np.random.seed(42)
    data = np.random.randn(num_samples, 10).astype(np.float32)

    class CalibrationDataset:
        def __init__(self, data):
            self.data = data

        def __len__(self):
            return len(self.data)

        def __iter__(self):
            for x in self.data:
                yield (torch.tensor(x),), None

    return CalibrationDataset(data)


def eval_fn(model):
    """Simple evaluation function returning random accuracy."""
    # In real use case, this would evaluate on validation set
    return 0.95 + np.random.random() * 0.04


def main():
    approach = "{approach}"
    quant_dtype = "{quant_dtype}"
    calibration = "{calibration}"
    tuning_strategy = "{tuning_strategy}"
    accuracy_criterion = {accuracy_criterion}
    max_trials = {max_trials}
    calibration_samples = {calibration_samples}
    enable_pruning = {enable_pruning}
    pruning_sparsity = {pruning_sparsity}

    if not HAS_TORCH:
        print("NC_ERROR: PyTorch required for Neural Compressor verification")
        sys.exit(1)

    # Create test model
    model = SimpleModel()
    model.eval()

    # Create calibration data
    calib_data = create_calibration_dataloader(calibration_samples)

    # Get original model size
    original_size = sum(p.numel() * p.element_size() for p in model.parameters())

    # Run original model
    np.random.seed(42)
    test_input = torch.randn(1, 10)
    with torch.no_grad():
        original_output = model(test_input).numpy()

    # Configure quantization
    try:
        from neural_compressor.config import PostTrainingQuantConfig
        config = PostTrainingQuantConfig(
            approach=approach,
            calibration_sampling_size=calibration_samples,
        )

        # Quantize model
        start_time = time.perf_counter()
        q_model = quantization.fit(
            model,
            config,
            calib_dataloader=calib_data,
        )
        quantization_time = time.perf_counter() - start_time

        # Get quantized model output
        with torch.no_grad():
            quant_output = q_model(test_input).numpy()

        # Calculate output difference
        output_diff = np.max(np.abs(original_output - quant_output))
        output_mse = np.mean((original_output - quant_output) ** 2)

        # Estimate compression (approximate)
        # In real scenario, would measure actual model size
        compression_ratio = 4.0 if quant_dtype == "int8" else 2.0

        status = "success"
        error_message = None

    except Exception as e:
        status = "error"
        error_message = str(e)
        quantization_time = 0.0
        output_diff = float('inf')
        output_mse = float('inf')
        compression_ratio = 1.0
        quant_output = None

    # Benchmark if successful
    if status == "success" and q_model is not None:
        latencies = []
        for _ in range(100):
            start = time.perf_counter()
            with torch.no_grad():
                _ = q_model(test_input)
            latencies.append((time.perf_counter() - start) * 1000)

        latencies = np.array(latencies)
        mean_lat = float(np.mean(latencies))
        std_lat = float(np.std(latencies))
        p50_lat = float(np.percentile(latencies, 50))
        p95_lat = float(np.percentile(latencies, 95))
    else:
        mean_lat = std_lat = p50_lat = p95_lat = 0.0

    print("NC_RESULT_START")
    result = {{
        "status": status,
        "approach": approach,
        "quant_dtype": quant_dtype,
        "calibration": calibration,
        "tuning_strategy": tuning_strategy,
        "accuracy_criterion": accuracy_criterion,
        "calibration_samples": calibration_samples,
        "original_size_bytes": original_size,
        "compression_ratio": compression_ratio,
        "output_max_diff": float(output_diff) if status == "success" else None,
        "output_mse": float(output_mse) if status == "success" else None,
        "quantization_time_s": quantization_time,
        "latency_ms": {{
            "mean": mean_lat,
            "std": std_lat,
            "p50": p50_lat,
            "p95": p95_lat
        }},
        "nc_version": neural_compressor.__version__,
        "error": error_message
    }}
    print(json.dumps(result, indent=2))
    print("NC_RESULT_END")

    if status == "success":
        print(f"\\nNC_SUMMARY: Compression ratio: {{compression_ratio:.1f}}x")
        print(f"NC_SUMMARY: Output max diff: {{output_diff:.6f}}")
        print(f"NC_SUMMARY: Output MSE: {{output_mse:.6f}}")
        print(f"NC_SUMMARY: Mean latency: {{mean_lat:.3f}}ms")

        if output_diff < 0.1 and output_mse < 0.01:
            print("NC_STATUS: VERIFIED")
        elif output_diff < 0.5:
            print("NC_STATUS: PARTIALLY_VERIFIED")
        else:
            print("NC_STATUS: NOT_VERIFIED")
    else:
        print(f"NC_ERROR: {{error_message}}")


if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse Neural Compressor output
pub fn parse_nc_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("NC_ERROR:") || stderr.contains("NC_ERROR:") {
        let error_msg = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("NC_ERROR:"))
            .map(|l| l.replace("NC_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown Neural Compressor error".to_string());
        return (VerificationStatus::Unknown { reason: error_msg }, None);
    }

    if let Some(json_str) = extract_json_result(stdout, "NC_RESULT_START", "NC_RESULT_END") {
        if let Ok(result) = serde_json::from_str::<serde_json::Value>(&json_str) {
            let status = result["status"].as_str().unwrap_or("error");
            let output_diff = result["output_max_diff"].as_f64().unwrap_or(1.0);
            let output_mse = result["output_mse"].as_f64().unwrap_or(1.0);

            if status == "error" {
                let error = result["error"]
                    .as_str()
                    .unwrap_or("Unknown error")
                    .to_string();
                return (VerificationStatus::Unknown { reason: error }, None);
            }

            // Use the model_optimization helper to build a structured counterexample
            let counterexample = crate::counterexample::build_quantization_counterexample(
                &result,
                "Neural Compressor",
            );

            if output_diff < 0.1 && output_mse < 0.01 {
                return (VerificationStatus::Proven, None);
            } else if output_diff < 0.5 {
                return (
                    VerificationStatus::Partial {
                        verified_percentage: 90.0,
                    },
                    counterexample,
                );
            } else {
                return (VerificationStatus::Disproven, counterexample);
            }
        }
    }

    if stdout.contains("NC_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("NC_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 90.0,
            },
            None,
        )
    } else if stdout.contains("NC_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse Neural Compressor output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after_start = &output[start + start_marker.len()..];
        if let Some(end) = after_start.find(end_marker) {
            return Some(after_start[..end].trim().to_string());
        }
    }
    None
}
