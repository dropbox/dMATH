//! RobustBench script generation and output parsing

use super::config::RobustBenchConfig;
use crate::counterexample::StructuredCounterexample;
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::ast::{ComparisonOp, Expr, Property};
use dashprove_usl::typecheck::TypedSpec;

/// Generate a RobustBench evaluation script from USL spec
pub fn generate_robustbench_script(
    spec: &TypedSpec,
    config: &RobustBenchConfig,
) -> Result<String, BackendError> {
    let epsilon = extract_epsilon_from_spec(spec).unwrap_or(config.epsilon);
    let dataset = config.dataset.dataset_name();
    let threat_model = config.threat_model.model_name();
    let num_samples = config.num_samples;
    let batch_size = config.batch_size;
    let use_autoattack = config.use_autoattack;

    // Extract model name from USL spec or use config
    let model_name = extract_model_from_spec(spec)
        .or_else(|| config.model_name.clone())
        .unwrap_or_else(|| "Carmon2019Unlabeled".to_string());

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
RobustBench adversarial robustness evaluation script
Generated by DashProve
"""

import sys
import json
import numpy as np

try:
    import torch
    from robustbench.data import load_cifar10, load_cifar100, load_imagenet
    from robustbench.utils import load_model
except ImportError as e:
    print(f"ROBUSTBENCH_ERROR: Missing dependency: {{e}}")
    sys.exit(1)

def load_dataset(dataset_name: str, n_examples: int):
    """Load a dataset from RobustBench."""
    try:
        if dataset_name == "cifar10":
            x_test, y_test = load_cifar10(n_examples=n_examples)
        elif dataset_name == "cifar100":
            x_test, y_test = load_cifar100(n_examples=n_examples)
        elif dataset_name == "imagenet":
            x_test, y_test = load_imagenet(n_examples=n_examples)
        else:
            print(f"ROBUSTBENCH_ERROR: Unknown dataset: {{dataset_name}}")
            return None, None
        return x_test, y_test
    except Exception as e:
        print(f"ROBUSTBENCH_ERROR: Failed to load dataset: {{e}}")
        return None, None

def main():
    dataset_name = "{dataset}"
    threat_model = "{threat_model}"
    model_name = "{model_name}"
    epsilon = {epsilon}
    n_examples = {num_samples}
    batch_size = {batch_size}
    use_autoattack = {use_autoattack}

    print(f"ROBUSTBENCH_INFO: Loading model {{model_name}}")

    # Load model from RobustBench
    try:
        model = load_model(model_name=model_name, dataset=dataset_name, threat_model=threat_model)
        model.eval()
        device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        model = model.to(device)
    except Exception as e:
        print(f"ROBUSTBENCH_ERROR: Failed to load model: {{e}}")
        sys.exit(1)

    print(f"ROBUSTBENCH_INFO: Loading dataset {{dataset_name}}")

    # Load dataset
    x_test, y_test = load_dataset(dataset_name, n_examples)
    if x_test is None:
        sys.exit(1)

    x_test = x_test.to(device)
    y_test = y_test.to(device)

    # Get clean accuracy
    with torch.no_grad():
        outputs = model(x_test)
        clean_predictions = outputs.argmax(dim=1)
        clean_accuracy = (clean_predictions == y_test).float().mean().item()

    print(f"ROBUSTBENCH_INFO: Clean accuracy: {{clean_accuracy:.2%}}")

    # Evaluate robust accuracy
    if use_autoattack:
        print("ROBUSTBENCH_INFO: Running AutoAttack evaluation")
        try:
            from autoattack import AutoAttack
            adversary = AutoAttack(model, norm=threat_model, eps=epsilon, version='standard')
            x_adv = adversary.run_standard_evaluation(x_test, y_test, bs=batch_size)
        except ImportError:
            print("ROBUSTBENCH_INFO: AutoAttack not available, using PGD")
            x_adv = pgd_attack(model, x_test, y_test, epsilon, threat_model)
    else:
        print("ROBUSTBENCH_INFO: Using PGD attack")
        x_adv = pgd_attack(model, x_test, y_test, epsilon, threat_model)

    # Calculate adversarial accuracy
    with torch.no_grad():
        adv_outputs = model(x_adv)
        adv_predictions = adv_outputs.argmax(dim=1)
        adv_accuracy = (adv_predictions == y_test).float().mean().item()

    # Find adversarial examples
    adv_indices = torch.where(clean_predictions != adv_predictions)[0]
    num_adversarial = len(adv_indices)

    # Calculate perturbation stats
    perturbations = (x_adv - x_test).cpu().numpy()
    max_perturbation = float(np.max(np.abs(perturbations)))
    mean_perturbation = float(np.mean(np.abs(perturbations)))

    # Output results
    print("ROBUSTBENCH_RESULT_START")
    result = {{
        "clean_accuracy": float(clean_accuracy),
        "robust_accuracy": float(adv_accuracy),
        "attack_success_rate": float(1.0 - adv_accuracy / max(clean_accuracy, 1e-10)),
        "num_adversarial_examples": int(num_adversarial),
        "max_perturbation": float(max_perturbation),
        "mean_perturbation": float(mean_perturbation),
        "epsilon": float(epsilon),
        "dataset": dataset_name,
        "threat_model": threat_model,
        "model_name": model_name,
        "num_samples": int(n_examples),
        "used_autoattack": use_autoattack
    }}

    # Add first adversarial example if found
    if num_adversarial > 0:
        idx = adv_indices[0].item()
        result["adversarial_example"] = {{
            "original_input": x_test[idx].flatten()[:10].cpu().tolist(),
            "adversarial_input": x_adv[idx].flatten()[:10].cpu().tolist(),
            "original_prediction": int(clean_predictions[idx].cpu().item()),
            "adversarial_prediction": int(adv_predictions[idx].cpu().item()),
            "true_label": int(y_test[idx].cpu().item()),
            "perturbation_norm": float(torch.norm(x_adv[idx] - x_test[idx], p=float('inf')).cpu().item())
        }}

    print(json.dumps(result, indent=2))
    print("ROBUSTBENCH_RESULT_END")

    # Print summary
    print(f"\\nROBUSTBENCH_SUMMARY: Clean accuracy: {{clean_accuracy:.2%}}")
    print(f"ROBUSTBENCH_SUMMARY: Robust accuracy: {{adv_accuracy:.2%}}")
    print(f"ROBUSTBENCH_SUMMARY: Found {{num_adversarial}} adversarial examples")

    # Determine status
    if adv_accuracy >= 0.90 * clean_accuracy:
        print("ROBUSTBENCH_STATUS: ROBUST")
    elif adv_accuracy >= 0.50 * clean_accuracy:
        print("ROBUSTBENCH_STATUS: PARTIALLY_ROBUST")
    else:
        print("ROBUSTBENCH_STATUS: NOT_ROBUST")

def pgd_attack(model, x, y, eps, norm, steps=20, step_size=None):
    """Simple PGD attack implementation."""
    if step_size is None:
        step_size = eps / 4

    x_adv = x.clone().detach()
    x_adv.requires_grad = True

    for _ in range(steps):
        outputs = model(x_adv)
        loss = torch.nn.functional.cross_entropy(outputs, y)
        loss.backward()

        with torch.no_grad():
            if norm == "Linf":
                x_adv = x_adv + step_size * x_adv.grad.sign()
                x_adv = torch.max(torch.min(x_adv, x + eps), x - eps)
            else:  # L2
                grad = x_adv.grad
                grad_norm = torch.norm(grad.view(grad.size(0), -1), dim=1, keepdim=True)
                grad_norm = grad_norm.view(-1, 1, 1, 1)
                x_adv = x_adv + step_size * grad / (grad_norm + 1e-10)
                delta = x_adv - x
                delta_norm = torch.norm(delta.view(delta.size(0), -1), dim=1, keepdim=True)
                delta_norm = delta_norm.view(-1, 1, 1, 1)
                factor = torch.min(torch.ones_like(delta_norm), eps / (delta_norm + 1e-10))
                x_adv = x + delta * factor

            x_adv = torch.clamp(x_adv, 0, 1)
            x_adv.requires_grad = True

    return x_adv.detach()

if __name__ == "__main__":
    main()
"#
    ))
}

/// Extract epsilon value from USL spec properties
fn extract_epsilon_from_spec(spec: &TypedSpec) -> Option<f64> {
    for prop in &spec.spec.properties {
        let expr = match prop {
            Property::Invariant(inv) => Some(&inv.body),
            Property::Theorem(thm) => Some(&thm.body),
            Property::Security(sec) => Some(&sec.body),
            Property::Probabilistic(prob) => Some(&prob.condition),
            _ => None,
        };
        if let Some(e) = expr {
            if let Some(eps) = extract_epsilon(e) {
                return Some(eps);
            }
        }
    }
    None
}

/// Extract epsilon from expression
fn extract_epsilon(expr: &Expr) -> Option<f64> {
    match expr {
        Expr::Compare(lhs, op, rhs) => {
            if let Expr::Var(name) = lhs.as_ref() {
                let lower = name.to_lowercase();
                if (lower.contains("epsilon") || lower == "eps")
                    && matches!(op, ComparisonOp::Le | ComparisonOp::Lt)
                {
                    return extract_numeric_value(rhs);
                }
            }
            if let Expr::Var(name) = rhs.as_ref() {
                let lower = name.to_lowercase();
                if (lower.contains("epsilon") || lower == "eps")
                    && matches!(op, ComparisonOp::Ge | ComparisonOp::Gt)
                {
                    return extract_numeric_value(lhs);
                }
            }
            if matches!(op, ComparisonOp::Le | ComparisonOp::Lt) {
                if let Some(val) = extract_numeric_value(rhs) {
                    if val > 0.0 && val < 1.0 {
                        return Some(val);
                    }
                }
            }
            extract_epsilon(lhs).or_else(|| extract_epsilon(rhs))
        }
        Expr::And(lhs, rhs) | Expr::Or(lhs, rhs) | Expr::Implies(lhs, rhs) => {
            extract_epsilon(lhs).or_else(|| extract_epsilon(rhs))
        }
        Expr::Not(inner) | Expr::Neg(inner) => extract_epsilon(inner),
        Expr::Binary(lhs, _, rhs) => extract_epsilon(lhs).or_else(|| extract_epsilon(rhs)),
        Expr::ForAll { body, .. }
        | Expr::Exists { body, .. }
        | Expr::ForAllIn { body, .. }
        | Expr::ExistsIn { body, .. } => extract_epsilon(body),
        Expr::FieldAccess(obj, _) => extract_epsilon(obj),
        Expr::MethodCall { receiver, args, .. } => {
            extract_epsilon(receiver).or_else(|| args.iter().find_map(extract_epsilon))
        }
        Expr::App(_, args) => args.iter().find_map(extract_epsilon),
        Expr::Var(_) | Expr::Int(_) | Expr::Float(_) | Expr::String(_) | Expr::Bool(_) => None,
    }
}

/// Extract numeric value from expression
fn extract_numeric_value(expr: &Expr) -> Option<f64> {
    match expr {
        Expr::Float(f) => Some(*f),
        Expr::Int(i) => Some(*i as f64),
        Expr::Neg(inner) => extract_numeric_value(inner).map(|v| -v),
        _ => None,
    }
}

/// Extract model name from USL spec
fn extract_model_from_spec(spec: &TypedSpec) -> Option<String> {
    for typedef in &spec.spec.types {
        let lower = typedef.name.to_lowercase();
        if lower.contains("model") || lower.contains("network") {
            return Some(typedef.name.clone());
        }
    }
    None
}

/// Parse RobustBench script output
pub fn parse_robustbench_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    // Check for errors
    if stdout.contains("ROBUSTBENCH_ERROR:") || stderr.contains("ROBUSTBENCH_ERROR:") {
        let error_msg = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("ROBUSTBENCH_ERROR:"))
            .map(|l| l.replace("ROBUSTBENCH_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown RobustBench error".to_string());

        return (VerificationStatus::Unknown { reason: error_msg }, None);
    }

    // Parse JSON result
    if let Some(json_str) = extract_json_result(stdout) {
        if let Ok(result) = serde_json::from_str::<serde_json::Value>(&json_str) {
            let clean_accuracy = result["clean_accuracy"].as_f64().unwrap_or(0.0);
            let robust_accuracy = result["robust_accuracy"].as_f64().unwrap_or(0.0);

            // Use the robustness helper to build a structured counterexample
            let counterexample = crate::counterexample::build_adversarial_attack_counterexample(
                &result,
                "RobustBench",
            );

            // Determine status based on robust accuracy relative to clean accuracy
            let robustness_ratio = robust_accuracy / clean_accuracy.max(1e-10);
            if robustness_ratio >= 0.90 {
                return (VerificationStatus::Proven, None);
            } else if robustness_ratio >= 0.50 {
                return (
                    VerificationStatus::Partial {
                        verified_percentage: robust_accuracy * 100.0,
                    },
                    counterexample,
                );
            } else {
                return (VerificationStatus::Disproven, counterexample);
            }
        }
    }

    // Fallback: parse status line
    if stdout.contains("ROBUSTBENCH_STATUS: ROBUST") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("ROBUSTBENCH_STATUS: PARTIALLY_ROBUST") {
        (
            VerificationStatus::Partial {
                verified_percentage: 75.0,
            },
            None,
        )
    } else if stdout.contains("ROBUSTBENCH_STATUS: NOT_ROBUST") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse RobustBench output".to_string(),
            },
            None,
        )
    }
}

/// Extract JSON result from output
fn extract_json_result(output: &str) -> Option<String> {
    let start_marker = "ROBUSTBENCH_RESULT_START";
    let end_marker = "ROBUSTBENCH_RESULT_END";

    if let Some(start) = output.find(start_marker) {
        let after_start = &output[start + start_marker.len()..];
        if let Some(end) = after_start.find(end_marker) {
            return Some(after_start[..end].trim().to_string());
        }
    }
    None
}
