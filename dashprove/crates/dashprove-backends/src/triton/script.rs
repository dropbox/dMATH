//! Triton script generation and output parsing

use super::config::TritonConfig;
use crate::counterexample::StructuredCounterexample;
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;

/// Generate a Triton kernel verification script
pub fn generate_triton_script(
    _spec: &TypedSpec,
    config: &TritonConfig,
) -> Result<String, BackendError> {
    let compilation_mode = config.compilation_mode.as_str();
    let target = config.target.as_str();
    let opt_level = config.opt_level.as_int();
    let num_warps = config.num_warps;
    let num_stages = config.num_stages;
    let autotune = if config.autotune { "True" } else { "False" };
    let warmup_iters = config.warmup_iterations;
    let bench_iters = config.benchmark_iterations;

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
Triton kernel verification script
Generated by DashProve

Validates Triton kernel correctness and performance.
"""

import sys
import json
import time
import numpy as np

try:
    import triton
    import triton.language as tl
except ImportError as e:
    print(f"TRITON_ERROR: Missing Triton: {{e}}")
    print("TRITON_ERROR: Install with: pip install triton")
    sys.exit(1)

# Check for CUDA/GPU availability
try:
    import torch
    HAS_TORCH = True
    HAS_CUDA = torch.cuda.is_available()
except ImportError:
    HAS_TORCH = False
    HAS_CUDA = False


# Simple vector addition kernel for verification
@triton.jit
def vector_add_kernel(
    x_ptr,
    y_ptr,
    output_ptr,
    n_elements,
    BLOCK_SIZE: tl.constexpr,
):
    pid = tl.program_id(axis=0)
    block_start = pid * BLOCK_SIZE
    offsets = block_start + tl.arange(0, BLOCK_SIZE)
    mask = offsets < n_elements

    x = tl.load(x_ptr + offsets, mask=mask)
    y = tl.load(y_ptr + offsets, mask=mask)
    output = x + y
    tl.store(output_ptr + offsets, output, mask=mask)


# Matrix multiplication kernel
@triton.jit
def matmul_kernel(
    a_ptr, b_ptr, c_ptr,
    M, N, K,
    stride_am, stride_ak,
    stride_bk, stride_bn,
    stride_cm, stride_cn,
    BLOCK_SIZE_M: tl.constexpr,
    BLOCK_SIZE_N: tl.constexpr,
    BLOCK_SIZE_K: tl.constexpr,
):
    pid_m = tl.program_id(axis=0)
    pid_n = tl.program_id(axis=1)

    offs_m = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_n = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    offs_k = tl.arange(0, BLOCK_SIZE_K)

    a_ptrs = a_ptr + offs_m[:, None] * stride_am + offs_k[None, :] * stride_ak
    b_ptrs = b_ptr + offs_k[:, None] * stride_bk + offs_n[None, :] * stride_bn

    accumulator = tl.zeros((BLOCK_SIZE_M, BLOCK_SIZE_N), dtype=tl.float32)
    for k in range(0, K, BLOCK_SIZE_K):
        a = tl.load(a_ptrs, mask=offs_k[None, :] < K - k, other=0.0)
        b = tl.load(b_ptrs, mask=offs_k[:, None] < K - k, other=0.0)
        accumulator += tl.dot(a, b)
        a_ptrs += BLOCK_SIZE_K * stride_ak
        b_ptrs += BLOCK_SIZE_K * stride_bk

    offs_cm = pid_m * BLOCK_SIZE_M + tl.arange(0, BLOCK_SIZE_M)
    offs_cn = pid_n * BLOCK_SIZE_N + tl.arange(0, BLOCK_SIZE_N)
    c_ptrs = c_ptr + offs_cm[:, None] * stride_cm + offs_cn[None, :] * stride_cn
    c_mask = (offs_cm[:, None] < M) & (offs_cn[None, :] < N)
    tl.store(c_ptrs, accumulator, mask=c_mask)


def run_cpu_verification():
    """Run verification without GPU using numpy reference."""
    compilation_mode = "{compilation_mode}"
    opt_level = {opt_level}
    num_warps = {num_warps}
    num_stages = {num_stages}
    autotune = {autotune}
    warmup_iters = {warmup_iters}
    bench_iters = {bench_iters}

    # Test vector addition on CPU reference
    np.random.seed(42)
    n = 1024
    x = np.random.randn(n).astype(np.float32)
    y = np.random.randn(n).astype(np.float32)
    expected = x + y

    # Simulate kernel timing with numpy
    latencies = []
    outputs = []
    for _ in range(warmup_iters):
        _ = x + y

    for _ in range(bench_iters):
        start = time.perf_counter()
        result = x + y
        latencies.append((time.perf_counter() - start) * 1000)
        outputs.append(result.copy())

    reference = outputs[0]
    consistent = True
    max_diff = 0.0
    for out in outputs[1:]:
        diff = np.max(np.abs(out - reference))
        max_diff = max(max_diff, diff)
        if diff > 1e-5:
            consistent = False

    # Check against expected
    expected_diff = np.max(np.abs(reference - expected))

    latencies = np.array(latencies)
    mean_lat = float(np.mean(latencies))
    std_lat = float(np.std(latencies))
    p50_lat = float(np.percentile(latencies, 50))
    p95_lat = float(np.percentile(latencies, 95))
    p99_lat = float(np.percentile(latencies, 99))
    throughput = 1000.0 / mean_lat if mean_lat > 0 else 0

    return {{
        "mode": "cpu_reference",
        "n_elements": n,
        "compilation_mode": compilation_mode,
        "opt_level": opt_level,
        "num_warps": num_warps,
        "num_stages": num_stages,
        "autotune_enabled": autotune,
        "consistent_outputs": consistent,
        "max_output_diff": float(max_diff),
        "expected_diff": float(expected_diff),
        "latency_ms": {{
            "mean": mean_lat,
            "std": std_lat,
            "p50": p50_lat,
            "p95": p95_lat,
            "p99": p99_lat
        }},
        "throughput_ops": throughput,
        "triton_version": triton.__version__
    }}


def run_gpu_verification():
    """Run verification on GPU if available."""
    compilation_mode = "{compilation_mode}"
    target = "{target}"
    opt_level = {opt_level}
    num_warps = {num_warps}
    num_stages = {num_stages}
    autotune = {autotune}
    warmup_iters = {warmup_iters}
    bench_iters = {bench_iters}

    # Vector addition test
    n = 98304  # Large enough to show performance
    x = torch.randn(n, device='cuda', dtype=torch.float32)
    y = torch.randn(n, device='cuda', dtype=torch.float32)
    output = torch.empty_like(x)

    BLOCK_SIZE = 1024
    grid = lambda meta: (triton.cdiv(n, meta['BLOCK_SIZE']),)

    # Compile and run
    try:
        vector_add_kernel[grid](x, y, output, n, BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps)
        torch.cuda.synchronize()
    except Exception as e:
        return {{"error": str(e), "mode": "gpu"}}

    # Reference check
    expected = x + y
    diff_from_expected = torch.max(torch.abs(output - expected)).item()

    # Warmup
    for _ in range(warmup_iters):
        vector_add_kernel[grid](x, y, output, n, BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps)
    torch.cuda.synchronize()

    # Benchmark
    latencies = []
    outputs = []
    for _ in range(bench_iters):
        torch.cuda.synchronize()
        start = time.perf_counter()
        vector_add_kernel[grid](x, y, output, n, BLOCK_SIZE=BLOCK_SIZE, num_warps=num_warps)
        torch.cuda.synchronize()
        latencies.append((time.perf_counter() - start) * 1000)
        outputs.append(output.clone())

    reference = outputs[0]
    consistent = True
    max_diff = 0.0
    for out in outputs[1:]:
        diff = torch.max(torch.abs(out - reference)).item()
        max_diff = max(max_diff, diff)
        if diff > 1e-5:
            consistent = False

    latencies = np.array(latencies)
    mean_lat = float(np.mean(latencies))
    std_lat = float(np.std(latencies))
    p50_lat = float(np.percentile(latencies, 50))
    p95_lat = float(np.percentile(latencies, 95))
    p99_lat = float(np.percentile(latencies, 99))

    # Bandwidth calculation
    bytes_accessed = 3 * n * 4  # 2 reads + 1 write, float32
    bandwidth_gbps = bytes_accessed / (mean_lat / 1000) / 1e9 if mean_lat > 0 else 0
    throughput = 1000.0 / mean_lat if mean_lat > 0 else 0

    return {{
        "mode": "gpu",
        "target": target,
        "device": torch.cuda.get_device_name(0),
        "n_elements": n,
        "compilation_mode": compilation_mode,
        "opt_level": opt_level,
        "num_warps": num_warps,
        "num_stages": num_stages,
        "autotune_enabled": autotune,
        "consistent_outputs": consistent,
        "max_output_diff": float(max_diff),
        "diff_from_expected": float(diff_from_expected),
        "latency_ms": {{
            "mean": mean_lat,
            "std": std_lat,
            "p50": p50_lat,
            "p95": p95_lat,
            "p99": p99_lat
        }},
        "bandwidth_gbps": bandwidth_gbps,
        "throughput_ops": throughput,
        "triton_version": triton.__version__
    }}


def main():
    if HAS_TORCH and HAS_CUDA:
        print("TRITON_INFO: Running GPU verification")
        result = run_gpu_verification()
    else:
        print("TRITON_INFO: No GPU available, running CPU reference verification")
        result = run_cpu_verification()

    if "error" in result:
        print(f"TRITON_ERROR: {{result['error']}}")
        sys.exit(1)

    print("TRITON_RESULT_START")
    print(json.dumps(result, indent=2))
    print("TRITON_RESULT_END")

    mean_lat = result["latency_ms"]["mean"]
    std_lat = result["latency_ms"]["std"]
    p95_lat = result["latency_ms"]["p95"]
    consistent = result["consistent_outputs"]
    max_diff = result["max_output_diff"]

    print(f"\\nTRITON_SUMMARY: Mean latency: {{mean_lat:.4f}}ms (std: {{std_lat:.4f}}ms)")
    print(f"TRITON_SUMMARY: P95: {{p95_lat:.4f}}ms")
    print(f"TRITON_SUMMARY: Output consistency: {{'PASS' if consistent else 'FAIL'}}")

    if "bandwidth_gbps" in result:
        print(f"TRITON_SUMMARY: Bandwidth: {{result['bandwidth_gbps']:.1f}} GB/s")

    if consistent and max_diff < 1e-5:
        print("TRITON_STATUS: VERIFIED")
    elif consistent:
        print("TRITON_STATUS: PARTIALLY_VERIFIED")
    else:
        print("TRITON_STATUS: NOT_VERIFIED")


if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse Triton output
pub fn parse_triton_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("TRITON_ERROR:") || stderr.contains("TRITON_ERROR:") {
        let error_msg = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("TRITON_ERROR:"))
            .map(|l| l.replace("TRITON_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown Triton error".to_string());
        return (VerificationStatus::Unknown { reason: error_msg }, None);
    }

    if let Some(json_str) = extract_json_result(stdout, "TRITON_RESULT_START", "TRITON_RESULT_END")
    {
        if let Ok(result) = serde_json::from_str::<serde_json::Value>(&json_str) {
            let consistent = result["consistent_outputs"].as_bool().unwrap_or(false);
            let max_diff = result["max_output_diff"].as_f64().unwrap_or(1.0);

            // Use the model_optimization helper to build a structured counterexample
            let counterexample =
                crate::counterexample::build_compiler_counterexample(&result, "Triton");

            if consistent && max_diff < 1e-5 {
                return (VerificationStatus::Proven, None);
            } else if consistent {
                return (
                    VerificationStatus::Partial {
                        verified_percentage: 95.0,
                    },
                    counterexample,
                );
            } else {
                return (VerificationStatus::Disproven, counterexample);
            }
        }
    }

    if stdout.contains("TRITON_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("TRITON_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 95.0,
            },
            None,
        )
    } else if stdout.contains("TRITON_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse Triton output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after_start = &output[start + start_marker.len()..];
        if let Some(end) = after_start.find(end_marker) {
            return Some(after_start[..end].trim().to_string());
        }
    }
    None
}
