//! GuardrailsAI script generation and output parsing

use super::config::GuardrailsAIConfig;
use crate::counterexample::StructuredCounterexample;
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;
use serde_json::Value;

/// Generate GuardrailsAI verification script
pub fn generate_guardrails_script(
    _spec: &TypedSpec,
    config: &GuardrailsAIConfig,
) -> Result<String, BackendError> {
    let guardrail_type = config.guardrail_type.as_str();
    let strictness = config.strictness.as_str();
    let use_fallbacks = if config.use_fallbacks {
        "True"
    } else {
        "False"
    };
    let max_retries = config.max_retries;
    let pass_threshold = config.pass_rate_threshold;

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
GuardrailsAI LLM output validation script generated by DashProve
Tests guardrail validation pass rates and schema compliance.
"""

import json
import sys
import time

try:
    import guardrails as gd
    from guardrails.validators import (
        ValidLength,
        ValidRange,
        ValidChoices,
    )
except ImportError as e:
    print(f"GUARDRAILS_ERROR: Missing dependencies: {{e}}")
    sys.exit(1)


def run_schema_validation(strictness, use_fallbacks, max_retries):
    """Test schema-based validation using Guardrails."""
    from pydantic import BaseModel, Field
    from typing import Optional

    class TestOutput(BaseModel):
        name: str = Field(description="A name string")
        score: int = Field(ge=0, le=100, description="Score from 0-100")
        category: Optional[str] = Field(default=None, description="Optional category")

    # Create a guard from the Pydantic model
    guard = gd.Guard.for_pydantic(TestOutput)

    # Test cases - mix of valid and invalid
    test_cases = [
        {{"name": "Alice", "score": 85, "category": "A"}},
        {{"name": "Bob", "score": 42}},
        {{"name": "Charlie", "score": 101}},  # Invalid: score > 100
        {{"name": "", "score": 50}},  # Edge case: empty name
        {{"name": "Diana", "score": -5}},  # Invalid: score < 0
        {{"name": "Eve", "score": 100, "category": "B"}},
        {{"score": 75}},  # Invalid: missing name
        {{"name": "Frank", "score": 0}},  # Edge case: min score
    ]

    passed = 0
    failed = 0
    errors = []

    for i, test_case in enumerate(test_cases):
        try:
            # Simulate LLM output as JSON string
            llm_output = json.dumps(test_case)
            result = guard.parse(llm_output)
            if result.validated_output is not None:
                passed += 1
            else:
                failed += 1
                errors.append(f"Case {{i}}: validation returned None")
        except Exception as e:
            failed += 1
            if strictness == "strict":
                errors.append(f"Case {{i}}: {{str(e)[:100]}}")

    return passed, failed, errors, len(test_cases)


def run_quality_validation(strictness, use_fallbacks, max_retries):
    """Test quality-based validation."""
    # Simulate quality checks on text outputs
    test_outputs = [
        "This is a well-formed response with clear information.",
        "ok",  # Too short
        "This response provides comprehensive details about the topic at hand.",
        "",  # Empty
        "A reasonable answer to the question.",
        "???" * 50,  # Nonsensical
        "The answer is 42, which represents the meaning of life.",
        "I don't know.",  # Low quality response
    ]

    passed = 0
    failed = 0
    errors = []

    min_length = 10 if strictness == "strict" else 5
    max_length = 500

    for i, output in enumerate(test_outputs):
        try:
            length = len(output)
            if length < min_length:
                failed += 1
                errors.append(f"Case {{i}}: too short ({{length}} < {{min_length}})")
            elif length > max_length:
                failed += 1
                errors.append(f"Case {{i}}: too long ({{length}} > {{max_length}})")
            elif output.strip() and not all(c == output[0] for c in output):
                passed += 1
            else:
                failed += 1
                errors.append(f"Case {{i}}: low quality content")
        except Exception as e:
            failed += 1
            errors.append(f"Case {{i}}: {{str(e)[:50]}}")

    return passed, failed, errors, len(test_outputs)


def run_safety_validation(strictness, use_fallbacks, max_retries):
    """Test safety/toxicity filtering."""
    # Simulate safety checks (simplified - real impl would use ML)
    test_outputs = [
        "Here's how to make a cake: mix flour, eggs, and sugar.",
        "The weather today is sunny and pleasant.",
        "I cannot help with that request.",
        "Let me explain the concept clearly.",
        "This is a helpful response about gardening.",
        "Error: request blocked for safety.",
        "Happy to assist with your question!",
        "The process involves several safe steps.",
    ]

    passed = 0
    failed = 0
    errors = []

    # Simple heuristic safety checks
    blocked_patterns = ["cannot help", "blocked", "Error:"]

    for i, output in enumerate(test_outputs):
        try:
            is_blocked = any(p.lower() in output.lower() for p in blocked_patterns)
            if is_blocked:
                failed += 1
                errors.append(f"Case {{i}}: safety filter triggered")
            elif len(output) > 0:
                passed += 1
            else:
                failed += 1
                errors.append(f"Case {{i}}: empty response")
        except Exception as e:
            failed += 1
            errors.append(f"Case {{i}}: {{str(e)[:50]}}")

    return passed, failed, errors, len(test_outputs)


def main():
    guardrail_type = "{guardrail_type}"
    strictness = "{strictness}"
    use_fallbacks = {use_fallbacks}
    max_retries = {max_retries}
    pass_threshold = {pass_threshold}

    start_time = time.perf_counter()

    try:
        if guardrail_type == "schema":
            passed, failed, errors, total = run_schema_validation(
                strictness, use_fallbacks, max_retries
            )
        elif guardrail_type == "quality":
            passed, failed, errors, total = run_quality_validation(
                strictness, use_fallbacks, max_retries
            )
        elif guardrail_type == "safety":
            passed, failed, errors, total = run_safety_validation(
                strictness, use_fallbacks, max_retries
            )
        else:
            # Default to schema validation
            passed, failed, errors, total = run_schema_validation(
                strictness, use_fallbacks, max_retries
            )

        pass_rate = passed / total if total > 0 else 0.0

        result = {{
            "status": "success",
            "guardrail_type": guardrail_type,
            "strictness": strictness,
            "use_fallbacks": use_fallbacks,
            "passed": passed,
            "failed": failed,
            "total": total,
            "pass_rate": pass_rate,
            "pass_threshold": pass_threshold,
            "errors": errors[:5],  # Limit error output
            "duration_s": time.perf_counter() - start_time,
        }}

        print("GUARDRAILS_RESULT_START")
        print(json.dumps(result, indent=2, default=str))
        print("GUARDRAILS_RESULT_END")

        if pass_rate >= pass_threshold:
            print("GUARDRAILS_STATUS: VERIFIED")
        elif pass_rate >= pass_threshold * 0.7:
            print("GUARDRAILS_STATUS: PARTIALLY_VERIFIED")
        else:
            print("GUARDRAILS_STATUS: NOT_VERIFIED")
    except Exception as e:
        print(f"GUARDRAILS_ERROR: {{e}}")


if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse GuardrailsAI output into verification status
pub fn parse_guardrails_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("GUARDRAILS_ERROR:") || stderr.contains("GUARDRAILS_ERROR:") {
        let reason = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("GUARDRAILS_ERROR:"))
            .map(|l| l.replace("GUARDRAILS_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown GuardrailsAI error".to_string());
        return (VerificationStatus::Unknown { reason }, None);
    }

    if let Some(json_str) =
        extract_json_result(stdout, "GUARDRAILS_RESULT_START", "GUARDRAILS_RESULT_END")
    {
        if let Ok(val) = serde_json::from_str::<Value>(&json_str) {
            let status = val["status"].as_str().unwrap_or("error");
            let pass_rate = val["pass_rate"].as_f64().unwrap_or(0.0);
            let threshold = val["pass_threshold"].as_f64().unwrap_or(0.8);

            if status == "error" {
                let reason = val["error"]
                    .as_str()
                    .unwrap_or("Unknown GuardrailsAI failure")
                    .to_string();
                return (VerificationStatus::Unknown { reason }, None);
            }

            let counterexample = crate::counterexample::build_guardrails_ai_counterexample(&val);

            if pass_rate >= threshold {
                (VerificationStatus::Proven, counterexample)
            } else if pass_rate >= threshold * 0.7 {
                (
                    VerificationStatus::Partial {
                        verified_percentage: (pass_rate / threshold) * 100.0,
                    },
                    counterexample,
                )
            } else {
                (VerificationStatus::Disproven, counterexample)
            }
        } else {
            (
                VerificationStatus::Unknown {
                    reason: "Failed to parse GuardrailsAI output".to_string(),
                },
                None,
            )
        }
    } else if stdout.contains("GUARDRAILS_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("GUARDRAILS_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 80.0,
            },
            None,
        )
    } else if stdout.contains("GUARDRAILS_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse GuardrailsAI output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after = &output[start + start_marker.len()..];
        if let Some(end) = after.find(end_marker) {
            return Some(after[..end].trim().to_string());
        }
    }
    None
}
