//! ONNX Runtime script generation and output parsing

use super::config::OnnxRuntimeConfig;
use crate::counterexample::StructuredCounterexample;
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;

/// Generate an ONNX Runtime optimization verification script
pub fn generate_onnxruntime_script(
    _spec: &TypedSpec,
    config: &OnnxRuntimeConfig,
) -> Result<String, BackendError> {
    let model_path = config
        .model_path
        .as_ref()
        .map(|p| p.display().to_string())
        .unwrap_or_else(|| "model.onnx".to_string());
    let execution_provider = config.execution_provider.as_str();
    let opt_level = config.optimization_level.as_ort_level();
    let enable_mem_pattern = if config.enable_memory_pattern {
        "True"
    } else {
        "False"
    };
    let enable_mem_arena = if config.enable_mem_arena {
        "True"
    } else {
        "False"
    };
    let intra_threads = config
        .intra_op_threads
        .map_or("None".to_string(), |n| n.to_string());
    let inter_threads = config
        .inter_op_threads
        .map_or("None".to_string(), |n| n.to_string());
    let warmup_iters = config.warmup_iterations;
    let bench_iters = config.benchmark_iterations;

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
ONNX Runtime model optimization verification script
Generated by DashProve

Validates model inference correctness and measures performance.
"""

import sys
import json
import time
import numpy as np

try:
    import onnxruntime as ort
except ImportError as e:
    print(f"ORT_ERROR: Missing dependency: {{e}}")
    print("ORT_ERROR: Install with: pip install onnxruntime")
    sys.exit(1)

def create_test_model():
    """Create a simple ONNX model for testing."""
    try:
        import onnx
        from onnx import helper, TensorProto

        # Simple MLP: input -> FC -> ReLU -> FC -> output
        X = helper.make_tensor_value_info('input', TensorProto.FLOAT, [1, 10])
        Y = helper.make_tensor_value_info('output', TensorProto.FLOAT, [1, 5])

        W1 = helper.make_tensor('W1', TensorProto.FLOAT, [10, 20], np.random.randn(10, 20).flatten().astype(np.float32).tolist())
        B1 = helper.make_tensor('B1', TensorProto.FLOAT, [20], np.zeros(20).astype(np.float32).tolist())
        W2 = helper.make_tensor('W2', TensorProto.FLOAT, [20, 5], np.random.randn(20, 5).flatten().astype(np.float32).tolist())
        B2 = helper.make_tensor('B2', TensorProto.FLOAT, [5], np.zeros(5).astype(np.float32).tolist())

        fc1 = helper.make_node('Gemm', ['input', 'W1', 'B1'], ['fc1_out'], name='fc1')
        relu = helper.make_node('Relu', ['fc1_out'], ['relu_out'], name='relu')
        fc2 = helper.make_node('Gemm', ['relu_out', 'W2', 'B2'], ['output'], name='fc2')

        graph = helper.make_graph([fc1, relu, fc2], 'test_model', [X], [Y], [W1, B1, W2, B2])
        model = helper.make_model(graph, opset_imports=[helper.make_opsetid('', 13)])

        return model.SerializeToString()
    except ImportError:
        return None

def load_model(model_path: str):
    """Load ONNX model."""
    try:
        with open(model_path, 'rb') as f:
            return f.read()
    except FileNotFoundError:
        print(f"ORT_INFO: Model file not found: {{model_path}}")
        return None

def main():
    model_path = "{model_path}"
    execution_provider = "{execution_provider}"
    opt_level = {opt_level}
    enable_mem_pattern = {enable_mem_pattern}
    enable_mem_arena = {enable_mem_arena}
    intra_threads = {intra_threads}
    inter_threads = {inter_threads}
    warmup_iters = {warmup_iters}
    bench_iters = {bench_iters}

    # Load or create model
    model_bytes = load_model(model_path)
    if model_bytes is None:
        print("ORT_INFO: Using synthetic test model")
        model_bytes = create_test_model()
        if model_bytes is None:
            print("ORT_ERROR: Could not create test model (install onnx package)")
            sys.exit(1)

    # Configure session options
    sess_options = ort.SessionOptions()
    sess_options.graph_optimization_level = ort.GraphOptimizationLevel(opt_level)
    sess_options.enable_mem_pattern = enable_mem_pattern
    sess_options.enable_mem_reuse = enable_mem_arena

    if intra_threads is not None:
        sess_options.intra_op_num_threads = intra_threads
    if inter_threads is not None:
        sess_options.inter_op_num_threads = inter_threads

    # Select execution provider
    providers = []
    if execution_provider == "CUDAExecutionProvider":
        providers = ['CUDAExecutionProvider', 'CPUExecutionProvider']
    elif execution_provider == "TensorRTExecutionProvider":
        providers = ['TensorRTExecutionProvider', 'CUDAExecutionProvider', 'CPUExecutionProvider']
    elif execution_provider == "OpenVINOExecutionProvider":
        providers = ['OpenVINOExecutionProvider', 'CPUExecutionProvider']
    elif execution_provider == "DmlExecutionProvider":
        providers = ['DmlExecutionProvider', 'CPUExecutionProvider']
    elif execution_provider == "CoreMLExecutionProvider":
        providers = ['CoreMLExecutionProvider', 'CPUExecutionProvider']
    else:
        providers = ['CPUExecutionProvider']

    # Create session
    try:
        session = ort.InferenceSession(model_bytes, sess_options, providers=providers)
    except Exception as e:
        print(f"ORT_ERROR: Failed to create session: {{e}}")
        sys.exit(1)

    # Get input/output info
    input_info = session.get_inputs()[0]
    output_info = session.get_outputs()[0]
    input_name = input_info.name
    input_shape = input_info.shape

    # Handle dynamic dimensions
    concrete_shape = []
    for dim in input_shape:
        if isinstance(dim, str) or dim is None:
            concrete_shape.append(1)
        else:
            concrete_shape.append(dim)

    # Generate test inputs
    np.random.seed(42)
    test_input = np.random.randn(*concrete_shape).astype(np.float32)

    # Warmup
    for _ in range(warmup_iters):
        session.run(None, {{input_name: test_input}})

    # Benchmark
    latencies = []
    outputs = []
    for _ in range(bench_iters):
        start = time.perf_counter()
        output = session.run(None, {{input_name: test_input}})
        latencies.append((time.perf_counter() - start) * 1000)  # ms
        outputs.append(output[0])

    # Check consistency
    reference_output = outputs[0]
    consistent = True
    max_diff = 0.0
    for out in outputs[1:]:
        diff = np.max(np.abs(out - reference_output))
        max_diff = max(max_diff, diff)
        if diff > 1e-5:
            consistent = False

    # Statistics
    latencies = np.array(latencies)
    mean_latency = float(np.mean(latencies))
    std_latency = float(np.std(latencies))
    p50_latency = float(np.percentile(latencies, 50))
    p95_latency = float(np.percentile(latencies, 95))
    p99_latency = float(np.percentile(latencies, 99))
    min_latency = float(np.min(latencies))
    max_latency = float(np.max(latencies))
    throughput = 1000.0 / mean_latency  # inferences per second

    # Get actual provider used
    actual_providers = session.get_providers()

    print("ORT_RESULT_START")
    result = {{
        "model_path": model_path,
        "execution_provider_requested": execution_provider,
        "execution_providers_available": actual_providers,
        "optimization_level": opt_level,
        "input_shape": list(concrete_shape),
        "output_shape": list(reference_output.shape),
        "warmup_iterations": warmup_iters,
        "benchmark_iterations": bench_iters,
        "consistent_outputs": consistent,
        "max_output_diff": float(max_diff),
        "latency_ms": {{
            "mean": mean_latency,
            "std": std_latency,
            "min": min_latency,
            "max": max_latency,
            "p50": p50_latency,
            "p95": p95_latency,
            "p99": p99_latency
        }},
        "throughput_ips": throughput
    }}
    print(json.dumps(result, indent=2))
    print("ORT_RESULT_END")

    print(f"\\nORT_SUMMARY: Mean latency: {{mean_latency:.3f}}ms (std: {{std_latency:.3f}}ms)")
    print(f"ORT_SUMMARY: P95 latency: {{p95_latency:.3f}}ms, P99: {{p99_latency:.3f}}ms")
    print(f"ORT_SUMMARY: Throughput: {{throughput:.1f}} inferences/sec")
    print(f"ORT_SUMMARY: Output consistency: {{'PASS' if consistent else 'FAIL'}} (max diff: {{max_diff:.2e}})")

    if consistent and max_diff < 1e-5:
        print("ORT_STATUS: VERIFIED")
    elif consistent:
        print("ORT_STATUS: PARTIALLY_VERIFIED")
    else:
        print("ORT_STATUS: NOT_VERIFIED")

if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse ONNX Runtime output
pub fn parse_onnxruntime_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("ORT_ERROR:") || stderr.contains("ORT_ERROR:") {
        let error_msg = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("ORT_ERROR:"))
            .map(|l| l.replace("ORT_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown ONNX Runtime error".to_string());
        return (VerificationStatus::Unknown { reason: error_msg }, None);
    }

    if let Some(json_str) = extract_json_result(stdout, "ORT_RESULT_START", "ORT_RESULT_END") {
        if let Ok(result) = serde_json::from_str::<serde_json::Value>(&json_str) {
            let consistent = result["consistent_outputs"].as_bool().unwrap_or(false);
            let max_diff = result["max_output_diff"].as_f64().unwrap_or(1.0);

            // Use the model_optimization helper to build a structured counterexample
            let counterexample =
                crate::counterexample::build_inference_counterexample(&result, "ONNX Runtime");

            if consistent && max_diff < 1e-5 {
                return (VerificationStatus::Proven, None);
            } else if consistent {
                return (
                    VerificationStatus::Partial {
                        verified_percentage: 95.0,
                    },
                    counterexample,
                );
            } else {
                return (VerificationStatus::Disproven, counterexample);
            }
        }
    }

    if stdout.contains("ORT_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("ORT_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 95.0,
            },
            None,
        )
    } else if stdout.contains("ORT_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse ONNX Runtime output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after_start = &output[start + start_marker.len()..];
        if let Some(end) = after_start.find(end_marker) {
            return Some(after_start[..end].trim().to_string());
        }
    }
    None
}
