//! AIF360 script generation and output parsing

use super::config::AIF360Config;
use crate::counterexample::{build_aif360_counterexample, StructuredCounterexample};
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;

/// Generate an AIF360 bias assessment script
pub fn generate_aif360_script(
    _spec: &TypedSpec,
    config: &AIF360Config,
) -> Result<String, BackendError> {
    let bias_metric = config.bias_metric.as_str();
    let mitigation_algorithm = config.mitigation_algorithm.as_str();
    let fairness_threshold = config.fairness_threshold;
    let disparate_impact_threshold = config.disparate_impact_threshold;
    let n_samples = config.n_samples;

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
AIF360 bias assessment script
Generated by DashProve

Evaluates algorithmic fairness using IBM AIF360.
"""

import sys
import json
import time
import numpy as np

try:
    import aif360
    from aif360.datasets import BinaryLabelDataset
    from aif360.metrics import BinaryLabelDatasetMetric, ClassificationMetric
except ImportError as e:
    print(f"AIF360_ERROR: Missing AIF360: {{e}}")
    print("AIF360_ERROR: Install with: pip install aif360")
    sys.exit(1)

try:
    import pandas as pd
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split
except ImportError as e:
    print(f"AIF360_ERROR: Dependencies required: {{e}}")
    sys.exit(1)


def create_test_data(n_samples):
    """Create test data with protected attribute."""
    np.random.seed(42)

    # Create DataFrame
    df = pd.DataFrame({{
        "feature_1": np.random.normal(0, 1, n_samples),
        "feature_2": np.random.uniform(0, 10, n_samples),
        "feature_3": np.random.exponential(1, n_samples),
        "protected": np.random.choice([0, 1], n_samples, p=[0.6, 0.4]),
    }})

    # Target with some correlation
    df["label"] = ((df["feature_1"] + df["feature_2"] > 5) & (np.random.random(n_samples) > 0.2)).astype(int)

    return df


def main():
    bias_metric = "{bias_metric}"
    mitigation_algorithm = "{mitigation_algorithm}"
    fairness_threshold = {fairness_threshold}
    disparate_impact_threshold = {disparate_impact_threshold}
    n_samples = {n_samples}

    # Create test data
    df = create_test_data(n_samples)

    # Split data
    train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)

    start_time = time.perf_counter()

    try:
        # Create AIF360 datasets
        train_dataset = BinaryLabelDataset(
            df=train_df,
            label_names=["label"],
            protected_attribute_names=["protected"],
            favorable_label=1,
            unfavorable_label=0
        )

        test_dataset = BinaryLabelDataset(
            df=test_df,
            label_names=["label"],
            protected_attribute_names=["protected"],
            favorable_label=1,
            unfavorable_label=0
        )

        # Calculate dataset metrics
        dataset_metric = BinaryLabelDatasetMetric(
            train_dataset,
            unprivileged_groups=[{{"protected": 0}}],
            privileged_groups=[{{"protected": 1}}]
        )

        spd = dataset_metric.statistical_parity_difference()
        di = dataset_metric.disparate_impact()

        # Train model
        feature_cols = ["feature_1", "feature_2", "feature_3", "protected"]
        X_train = train_df[feature_cols].values
        y_train = train_df["label"].values
        X_test = test_df[feature_cols].values
        y_test = test_df["label"].values

        model = RandomForestClassifier(n_estimators=50, random_state=42)
        model.fit(X_train, y_train)

        # Predict
        y_pred = model.predict(X_test)

        # Create predicted dataset
        pred_df = test_df.copy()
        pred_df["label"] = y_pred

        pred_dataset = BinaryLabelDataset(
            df=pred_df,
            label_names=["label"],
            protected_attribute_names=["protected"],
            favorable_label=1,
            unfavorable_label=0
        )

        # Calculate classification metrics
        class_metric = ClassificationMetric(
            test_dataset, pred_dataset,
            unprivileged_groups=[{{"protected": 0}}],
            privileged_groups=[{{"protected": 1}}]
        )

        # Get various metrics
        pred_spd = class_metric.statistical_parity_difference()
        pred_di = class_metric.disparate_impact()
        avg_odds_diff = class_metric.average_odds_difference()
        eq_opp_diff = class_metric.equal_opportunity_difference()

        # Determine primary metric
        if bias_metric == "statistical_parity_difference":
            primary_metric = pred_spd
        elif bias_metric == "disparate_impact":
            primary_metric = pred_di
        elif bias_metric == "average_odds_difference":
            primary_metric = avg_odds_diff
        elif bias_metric == "equal_opportunity_difference":
            primary_metric = eq_opp_diff
        else:
            primary_metric = pred_spd

        # Check fairness
        if bias_metric == "disparate_impact":
            is_fair = primary_metric >= disparate_impact_threshold
        else:
            is_fair = abs(primary_metric) <= fairness_threshold

        assessment_time = time.perf_counter() - start_time

        status = "success"
        error_message = None

    except Exception as e:
        status = "error"
        error_message = str(e)
        assessment_time = time.perf_counter() - start_time
        spd = None
        di = None
        pred_spd = None
        pred_di = None
        avg_odds_diff = None
        eq_opp_diff = None
        primary_metric = None
        is_fair = False

    print("AIF360_RESULT_START")
    result = {{
        "status": status,
        "bias_metric": bias_metric,
        "mitigation_algorithm": mitigation_algorithm,
        "n_samples": n_samples,
        "dataset_statistical_parity_diff": spd,
        "dataset_disparate_impact": di,
        "prediction_statistical_parity_diff": pred_spd,
        "prediction_disparate_impact": pred_di,
        "average_odds_difference": avg_odds_diff,
        "equal_opportunity_difference": eq_opp_diff,
        "primary_metric_value": primary_metric,
        "fairness_threshold": fairness_threshold,
        "disparate_impact_threshold": disparate_impact_threshold,
        "is_fair": is_fair,
        "assessment_time_s": assessment_time,
        "aif360_version": aif360.__version__,
        "error": error_message
    }}
    print(json.dumps(result, indent=2, default=str))
    print("AIF360_RESULT_END")

    if status == "success":
        print(f"\nAIF360_SUMMARY: Bias metric: {{bias_metric}}")
        print(f"AIF360_SUMMARY: Primary metric value: {{primary_metric:.4f}}")
        print(f"AIF360_SUMMARY: Statistical parity diff: {{pred_spd:.4f}}")
        print(f"AIF360_SUMMARY: Disparate impact: {{pred_di:.4f}}")
        print(f"AIF360_SUMMARY: Is fair: {{is_fair}}")

        if is_fair:
            print("AIF360_STATUS: VERIFIED")
        elif bias_metric == "disparate_impact":
            if primary_metric >= disparate_impact_threshold * 0.9:
                print("AIF360_STATUS: PARTIALLY_VERIFIED")
            else:
                print("AIF360_STATUS: NOT_VERIFIED")
        else:
            if abs(primary_metric) <= fairness_threshold * 2:
                print("AIF360_STATUS: PARTIALLY_VERIFIED")
            else:
                print("AIF360_STATUS: NOT_VERIFIED")
    else:
        print(f"AIF360_ERROR: {{error_message}}")


if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse AIF360 output
pub fn parse_aif360_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("AIF360_ERROR:") || stderr.contains("AIF360_ERROR:") {
        let error_msg = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("AIF360_ERROR:"))
            .map(|l| l.replace("AIF360_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown AIF360 error".to_string());
        return (VerificationStatus::Unknown { reason: error_msg }, None);
    }

    if let Some(json_str) = extract_json_result(stdout, "AIF360_RESULT_START", "AIF360_RESULT_END")
    {
        if let Ok(result) = serde_json::from_str::<serde_json::Value>(&json_str) {
            let status = result["status"].as_str().unwrap_or("error");
            let is_fair = result["is_fair"].as_bool().unwrap_or(false);

            if status == "error" {
                let error = result["error"]
                    .as_str()
                    .unwrap_or("Unknown error")
                    .to_string();
                return (VerificationStatus::Unknown { reason: error }, None);
            }

            // Use the shared fairness counterexample helper with populated failed_checks
            let counterexample = build_aif360_counterexample(&result);

            if is_fair {
                return (VerificationStatus::Proven, None);
            } else if stdout.contains("PARTIALLY_VERIFIED") {
                return (
                    VerificationStatus::Partial {
                        verified_percentage: 75.0,
                    },
                    counterexample,
                );
            } else {
                return (VerificationStatus::Disproven, counterexample);
            }
        }
    }

    if stdout.contains("AIF360_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("AIF360_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 75.0,
            },
            None,
        )
    } else if stdout.contains("AIF360_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse AIF360 output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after_start = &output[start + start_marker.len()..];
        if let Some(end) = after_start.find(end_marker) {
            return Some(after_start[..end].trim().to_string());
        }
    }
    None
}
