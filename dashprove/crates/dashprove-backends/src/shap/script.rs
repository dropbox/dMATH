//! SHAP script generation and output parsing

use super::config::ShapConfig;
use crate::counterexample::{build_shap_counterexample, StructuredCounterexample};
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;
use serde_json::Value;

/// Generate a SHAP explanation script
pub fn generate_shap_script(
    _spec: &TypedSpec,
    config: &ShapConfig,
) -> Result<String, BackendError> {
    let explainer = config.explainer.as_str();
    let model_type = config.model_type.as_str();
    let sample_size = config.sample_size;
    let background_size = config.background_size;
    let max_features = config.max_features;
    let importance_threshold = config.importance_threshold;
    let evaluate_stability = if config.evaluate_stability {
        "True"
    } else {
        "False"
    };

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
SHAP interpretability verification script generated by DashProve
Evaluates feature attributions and stability for a simple model.
"""

import json
import sys
import numpy as np
import time

try:
    import shap
    from sklearn.datasets import make_classification, make_regression
    from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
    from sklearn.model_selection import train_test_split
    from sklearn.preprocessing import StandardScaler
except ImportError as e:
    print(f"SHAP_ERROR: Missing dependencies: {{e}}")
    sys.exit(1)


def prepare_data(model_type, n_samples):
    if model_type == "classification":
        X, y = make_classification(
            n_samples=n_samples,
            n_features=8,
            n_informative=5,
            n_redundant=1,
            n_clusters_per_class=2,
            class_sep=1.2,
            random_state=42,
        )
    else:
        X, y = make_regression(
            n_samples=n_samples,
            n_features=8,
            n_informative=5,
            noise=0.25,
            random_state=42,
        )

    scaler = StandardScaler()
    X = scaler.fit_transform(X)
    return X, y


def main():
    explainer_type = "{explainer}"
    model_type = "{model_type}"
    sample_size = {sample_size}
    background_size = {background_size}
    max_features = {max_features}
    importance_threshold = {importance_threshold}
    evaluate_stability = {evaluate_stability}

    X, y = prepare_data(model_type, sample_size)

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=7, stratify=y if model_type == "classification" else None
    )

    start_time = time.perf_counter()

    try:
        if model_type == "classification":
            model = RandomForestClassifier(
                n_estimators=120,
                max_depth=None,
                random_state=7,
                class_weight="balanced",
            )
        else:
            model = RandomForestRegressor(
                n_estimators=120,
                random_state=7,
            )

        model.fit(X_train, y_train)

        background = X_train[:max(background_size, 10)]
        eval_samples = min(len(X_test), max(20, background_size))
        X_eval = X_test[:eval_samples]

        if explainer_type == "tree":
            explainer = shap.TreeExplainer(model)
            shap_values = explainer.shap_values(X_eval)
        elif explainer_type == "linear":
            explainer = shap.LinearExplainer(model, background)
            shap_values = explainer.shap_values(X_eval)
        elif explainer_type == "deep":
            explainer = shap.DeepExplainer(model, background)
            shap_values = explainer.shap_values(X_eval)
        elif explainer_type == "gradient":
            explainer = shap.GradientExplainer(model, background)
            shap_values = explainer.shap_values(X_eval)
        else:
            explainer = shap.KernelExplainer(model.predict, background)
            shap_values = explainer.shap_values(X_eval, nsamples=min(200, sample_size))

        if isinstance(shap_values, list):
            shap_array = np.array(shap_values[1] if len(shap_values) > 1 else shap_values[0])
        else:
            shap_array = np.array(shap_values)

        per_feature = np.mean(np.abs(shap_array), axis=0)
        mean_abs = float(np.mean(np.abs(shap_array)))
        max_importance = float(np.max(per_feature))
        top_feature = int(np.argmax(per_feature))

        stability_gap = None
        if evaluate_stability:
            noise = X_eval + np.random.normal(0, 0.05, X_eval.shape)
            alt_values = explainer.shap_values(noise)
            if isinstance(alt_values, list):
                alt_array = np.array(alt_values[1] if len(alt_values) > 1 else alt_values[0])
            else:
                alt_array = np.array(alt_values)
            alt_per_feature = np.mean(np.abs(alt_array), axis=0)
            stability_gap = float(np.mean(np.abs(per_feature - alt_per_feature)))

        result = {{
            "status": "success",
            "explainer": explainer_type,
            "model_type": model_type,
            "sample_size": int(sample_size),
            "background_size": int(background_size),
            "eval_samples": int(eval_samples),
            "mean_abs_shap": mean_abs,
            "max_importance": max_importance,
            "top_feature": top_feature,
            "per_feature_importance": [float(x) for x in per_feature[:max_features].tolist()],
            "stability_gap": stability_gap,
            "importance_threshold": importance_threshold,
            "duration_s": time.perf_counter() - start_time,
            "model_score": float(model.score(X_test, y_test)),
        }}

        print("SHAP_RESULT_START")
        print(json.dumps(result, indent=2, default=str))
        print("SHAP_RESULT_END")

        if mean_abs >= importance_threshold:
            print("SHAP_STATUS: VERIFIED")
        elif mean_abs >= (importance_threshold * 0.4) - 1e-6:
            print("SHAP_STATUS: PARTIALLY_VERIFIED")
        else:
            print("SHAP_STATUS: NOT_VERIFIED")
    except Exception as e:
        print(f"SHAP_ERROR: {{e}}")


if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse SHAP output markers into verification status
pub fn parse_shap_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("SHAP_ERROR:") || stderr.contains("SHAP_ERROR:") {
        let reason = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("SHAP_ERROR:"))
            .map(|l| l.replace("SHAP_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown SHAP error".to_string());
        return (VerificationStatus::Unknown { reason }, None);
    }

    if let Some(json_str) = extract_json_result(stdout, "SHAP_RESULT_START", "SHAP_RESULT_END") {
        if let Ok(value) = serde_json::from_str::<Value>(&json_str) {
            let status = value["status"].as_str().unwrap_or("error");
            let mean_abs = value["mean_abs_shap"].as_f64().unwrap_or(0.0);
            let threshold = value["importance_threshold"].as_f64().unwrap_or(0.0);

            if status == "error" {
                let reason = value["error"]
                    .as_str()
                    .unwrap_or("Unknown SHAP failure")
                    .to_string();
                return (VerificationStatus::Unknown { reason }, None);
            }

            let ce = build_shap_counterexample(&value);

            let partial_threshold = threshold * 0.4;

            if mean_abs >= threshold {
                (VerificationStatus::Proven, ce)
            } else if mean_abs + f64::EPSILON >= partial_threshold {
                (
                    VerificationStatus::Partial {
                        verified_percentage: (mean_abs / threshold) * 100.0,
                    },
                    ce,
                )
            } else {
                (VerificationStatus::Disproven, ce)
            }
        } else {
            (
                VerificationStatus::Unknown {
                    reason: "Failed to parse SHAP JSON output".to_string(),
                },
                None,
            )
        }
    } else if stdout.contains("SHAP_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("SHAP_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 50.0,
            },
            None,
        )
    } else if stdout.contains("SHAP_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse SHAP output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after_start = &output[start + start_marker.len()..];
        if let Some(end) = after_start.find(end_marker) {
            return Some(after_start[..end].trim().to_string());
        }
    }
    None
}
