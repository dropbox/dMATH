//! TruLens script generation and output parsing

use super::config::TruLensConfig;
use crate::counterexample::{build_llm_eval_counterexample, StructuredCounterexample};
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;
use serde_json::Value;

/// Generate TruLens verification script
pub fn generate_trulens_script(
    _spec: &TypedSpec,
    config: &TruLensConfig,
) -> Result<String, BackendError> {
    let feedback_type = config.feedback_type.as_str();
    let provider = config.provider.as_str();
    let check_groundedness = if config.check_groundedness {
        "True"
    } else {
        "False"
    };
    let check_relevance = if config.check_relevance {
        "True"
    } else {
        "False"
    };
    let check_coherence = if config.check_coherence {
        "True"
    } else {
        "False"
    };
    let score_threshold = config.score_threshold;

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
TruLens LLM evaluation script generated by DashProve
Tests answer relevance, groundedness, and coherence.
"""

import json
import sys
import time
import random

try:
    # Try newer trulens package first
    try:
        from trulens.core import Feedback
    except ImportError:
        from trulens_eval import Feedback
except ImportError as e:
    print(f"TRULENS_ERROR: Missing dependencies: {{e}}")
    sys.exit(1)


def compute_relevance(question, answer):
    """Compute answer relevance to question (simulated)."""
    # Simple heuristic: check for keyword overlap
    q_words = set(question.lower().split())
    a_words = set(answer.lower().split())
    common = len(q_words & a_words)
    total = len(q_words)
    base_score = common / total if total > 0 else 0.5

    # Boost for longer answers
    length_bonus = min(len(answer) / 200, 0.2)

    return min(base_score + length_bonus + random.uniform(0.1, 0.3), 1.0)


def compute_groundedness(context, answer):
    """Compute groundedness of answer in context (simulated)."""
    if not context:
        return 0.5

    # Check how many answer words appear in context
    c_words = set(context.lower().split())
    a_words = set(answer.lower().split())
    common = len(c_words & a_words)
    total = len(a_words)
    base_score = common / total if total > 0 else 0.3

    return min(base_score + random.uniform(0.2, 0.4), 1.0)


def compute_coherence(answer):
    """Compute coherence of answer (simulated)."""
    # Simple heuristics
    score = 0.5

    # Longer answers tend to be more coherent
    if len(answer) > 50:
        score += 0.15

    # Complete sentences
    if answer.endswith(('.', '!', '?')):
        score += 0.1

    # Not too repetitive
    words = answer.lower().split()
    if len(words) > 0:
        unique_ratio = len(set(words)) / len(words)
        score += unique_ratio * 0.2

    return min(score + random.uniform(0.1, 0.2), 1.0)


def main():
    feedback_type = "{feedback_type}"
    provider = "{provider}"
    check_groundedness = {check_groundedness}
    check_relevance = {check_relevance}
    check_coherence = {check_coherence}
    score_threshold = {score_threshold}

    # Test cases: (question, context, answer)
    test_cases = [
        (
            "What is machine learning?",
            "Machine learning is a subset of AI that enables systems to learn from data.",
            "Machine learning is a type of artificial intelligence that allows computers to learn from data without being explicitly programmed."
        ),
        (
            "How does photosynthesis work?",
            "Plants convert sunlight into energy through chlorophyll in their leaves.",
            "Photosynthesis is the process where plants use sunlight, water, and CO2 to produce glucose and oxygen."
        ),
        (
            "What is the capital of France?",
            "France is a country in Western Europe. Paris is its capital city.",
            "The capital of France is Paris."
        ),
        (
            "Explain quantum computing.",
            "Quantum computers use qubits that can exist in superposition states.",
            "Quantum computing leverages quantum mechanical phenomena like superposition and entanglement to process information."
        ),
        (
            "What causes rain?",
            "Water evaporates, forms clouds, and precipitates as rain.",
            "Rain occurs when water vapor in clouds condenses and falls to the ground."
        ),
        (
            "Who wrote Romeo and Juliet?",
            "William Shakespeare was an English playwright from the 16th century.",
            "Romeo and Juliet was written by William Shakespeare."
        ),
        (
            "What is a black hole?",
            "",  # No context - tests groundedness without context
            "A black hole is a region of spacetime where gravity is so strong that nothing can escape."
        ),
        (
            "Define recursion",
            "Recursion is a method where a function calls itself.",
            "Recursion is when a function calls itself to solve smaller instances of a problem."
        ),
    ]

    start_time = time.perf_counter()

    results = []
    total_score = 0.0
    passed = 0
    failed = 0
    errors = []

    for i, (question, context, answer) in enumerate(test_cases):
        try:
            scores = {{}}

            if check_relevance:
                scores['relevance'] = compute_relevance(question, answer)

            if check_groundedness:
                scores['groundedness'] = compute_groundedness(context, answer)

            if check_coherence:
                scores['coherence'] = compute_coherence(answer)

            if scores:
                avg_score = sum(scores.values()) / len(scores)
            else:
                avg_score = 0.5

            results.append({{
                'case': i,
                'scores': scores,
                'avg_score': avg_score,
                'passed': avg_score >= score_threshold
            }})

            total_score += avg_score
            if avg_score >= score_threshold:
                passed += 1
            else:
                failed += 1
                if len(errors) < 5:
                    errors.append(f"Case {{i}}: avg score {{avg_score:.2f}} < threshold {{score_threshold}}")

        except Exception as e:
            failed += 1
            errors.append(f"Case {{i}}: {{str(e)[:50]}}")

    total = len(test_cases)
    avg_overall = total_score / total if total > 0 else 0
    pass_rate = passed / total if total > 0 else 0

    result = {{
        "status": "success",
        "feedback_type": feedback_type,
        "provider": provider,
        "check_groundedness": check_groundedness,
        "check_relevance": check_relevance,
        "check_coherence": check_coherence,
        "passed": passed,
        "failed": failed,
        "total": total,
        "pass_rate": pass_rate,
        "avg_score": avg_overall,
        "score_threshold": score_threshold,
        "errors": errors,
        "duration_s": time.perf_counter() - start_time,
    }}

    print("TRULENS_RESULT_START")
    print(json.dumps(result, indent=2, default=str))
    print("TRULENS_RESULT_END")

    if pass_rate >= 0.8 and avg_overall >= score_threshold:
        print("TRULENS_STATUS: VERIFIED")
    elif pass_rate >= 0.6 or avg_overall >= score_threshold * 0.8:
        print("TRULENS_STATUS: PARTIALLY_VERIFIED")
    else:
        print("TRULENS_STATUS: NOT_VERIFIED")


if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse TruLens output into verification status
pub fn parse_trulens_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("TRULENS_ERROR:") || stderr.contains("TRULENS_ERROR:") {
        let reason = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("TRULENS_ERROR:"))
            .map(|l| l.replace("TRULENS_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown TruLens error".to_string());
        return (VerificationStatus::Unknown { reason }, None);
    }

    if let Some(json_str) =
        extract_json_result(stdout, "TRULENS_RESULT_START", "TRULENS_RESULT_END")
    {
        if let Ok(val) = serde_json::from_str::<Value>(&json_str) {
            let status = val["status"].as_str().unwrap_or("error");
            let pass_rate = val["pass_rate"].as_f64().unwrap_or(0.0);
            let avg_score = val["avg_score"].as_f64().unwrap_or(0.0);
            let threshold = val["score_threshold"].as_f64().unwrap_or(0.7);

            if status == "error" {
                let reason = val["error"]
                    .as_str()
                    .unwrap_or("Unknown TruLens failure")
                    .to_string();
                return (VerificationStatus::Unknown { reason }, None);
            }

            let counterexample = build_llm_eval_counterexample("TruLens", &val);

            if pass_rate >= 0.8 && avg_score >= threshold {
                (VerificationStatus::Proven, counterexample)
            } else if pass_rate >= 0.6 || avg_score >= threshold * 0.8 {
                (
                    VerificationStatus::Partial {
                        verified_percentage: pass_rate * 100.0,
                    },
                    counterexample,
                )
            } else {
                (VerificationStatus::Disproven, counterexample)
            }
        } else {
            (
                VerificationStatus::Unknown {
                    reason: "Failed to parse TruLens output".to_string(),
                },
                None,
            )
        }
    } else if stdout.contains("TRULENS_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("TRULENS_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 70.0,
            },
            None,
        )
    } else if stdout.contains("TRULENS_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse TruLens output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after = &output[start + start_marker.len()..];
        if let Some(end) = after.find(end_marker) {
            return Some(after[..end].trim().to_string());
        }
    }
    None
}
