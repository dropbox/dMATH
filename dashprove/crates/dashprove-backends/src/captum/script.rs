//! Captum script generation and output parsing

use super::config::CaptumConfig;
use crate::counterexample::{build_captum_counterexample, StructuredCounterexample};
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;
use serde_json::Value;

/// Generate Captum attribution script
pub fn generate_captum_script(
    _spec: &TypedSpec,
    config: &CaptumConfig,
) -> Result<String, BackendError> {
    let method = config.method.as_str();
    let steps = config.steps;
    let use_noise_tunnel = if config.use_noise_tunnel {
        "True"
    } else {
        "False"
    };
    let top_k = config.top_k;
    let threshold = config.attribution_threshold;

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
Captum interpretability verification generated by DashProve.
Computes attributions for a small neural network and checks magnitude/stability.
"""

import json
import sys
import time
import torch
from torch import nn
from torch.utils.data import TensorDataset, DataLoader

try:
    import captum
    from captum.attr import (
        IntegratedGradients,
        Saliency,
        DeepLift,
        GradientShap,
        NoiseTunnel,
    )
except ImportError as e:
    print(f"CAPTUM_ERROR: Missing dependencies: {{e}}")
    sys.exit(1)


class SimpleNet(nn.Module):
    def __init__(self, in_features: int = 6):
        super().__init__()
        self.net = nn.Sequential(
            nn.Linear(in_features, 12),
            nn.ReLU(),
            nn.Linear(12, 6),
            nn.ReLU(),
            nn.Linear(6, 1),
        )

    def forward(self, x):
        return self.net(x)


def train_model(model, loader, epochs=6):
    optim = torch.optim.Adam(model.parameters(), lr=0.01)
    loss_fn = nn.BCEWithLogitsLoss()
    model.train()
    for _ in range(epochs):
        for xb, yb in loader:
            optim.zero_grad()
            out = model(xb).squeeze(-1)
            loss = loss_fn(out, yb)
            loss.backward()
            optim.step()
    return model


def main():
    torch.manual_seed(0)
    method = "{method}"
    steps = {steps}
    use_noise_tunnel = {use_noise_tunnel}
    top_k = {top_k}
    attribution_threshold = {threshold}

    start_time = time.perf_counter()

    try:
        # Synthetic binary classification data
        num_samples = 512
        features = 6
        X = torch.randn(num_samples, features)
        weights = torch.tensor([0.8, -1.1, 0.4, 0.6, -0.3, 0.9])
        logits = (X * weights).sum(dim=1)
        y = torch.sigmoid(logits + 0.1 * torch.randn(num_samples))
        y = (y > 0.5).float()

        dataset = TensorDataset(X, y)
        loader = DataLoader(dataset, batch_size=64, shuffle=True)

        model = SimpleNet(in_features=features)
        model = train_model(model, loader, epochs=8)
        model.eval()

        sample = X[:16].clone().requires_grad_(True)
        target = y[:16].clone()

        if method == "integrated_gradients":
            attr_method = IntegratedGradients(model)
            attr = attr_method.attribute(sample, baselines=torch.zeros_like(sample), target=None, n_steps=steps)
        elif method == "saliency":
            attr_method = Saliency(model)
            attr = attr_method.attribute(sample, target=None)
        elif method == "deeplift":
            attr_method = DeepLift(model)
            attr = attr_method.attribute(sample, baselines=torch.zeros_like(sample), target=None)
        else:
            attr_method = GradientShap(model)
            baselines = torch.randn(8, features) * 0.1
            attr = attr_method.attribute(sample, baselines=baselines, target=None, n_samples=steps)

        if use_noise_tunnel:
            smoother = NoiseTunnel(attr_method)
            attr = smoother.attribute(sample, nt_samples=20, nt_type="smoothgrad_sq", target=None)

        attr = attr.detach()
        attribution_mean = float(attr.abs().mean().item())
        attribution_max = float(attr.abs().max().item())
        per_feature = attr.abs().mean(dim=0)
        top_feature = int(torch.argmax(per_feature).item())

        sparsity = float((attr.abs() < attribution_threshold).float().mean().item())
        stability_baseline = torch.randn_like(sample) * 0.02
        if method == "integrated_gradients":
            alt = attr_method.attribute(
                sample + stability_baseline, baselines=torch.zeros_like(sample), target=None, n_steps=max(10, steps // 2)
            )
        else:
            alt = attr_method.attribute(sample + stability_baseline, target=None)
        stability_gap = float(torch.mean(torch.abs(attr - alt)).item())

        result = {{
            "status": "success",
            "method": method,
            "steps": steps,
            "use_noise_tunnel": use_noise_tunnel,
            "top_k": top_k,
            "attribution_mean": attribution_mean,
            "attribution_max": attribution_max,
            "top_feature": top_feature,
            "sparsity": sparsity,
            "stability_gap": stability_gap,
            "attribution_threshold": attribution_threshold,
            "duration_s": time.perf_counter() - start_time,
        }}

        print("CAPTUM_RESULT_START")
        print(json.dumps(result, indent=2, default=float))
        print("CAPTUM_RESULT_END")

        if attribution_mean >= attribution_threshold and stability_gap < max(attribution_threshold * 3, 0.05):
            print("CAPTUM_STATUS: VERIFIED")
        elif attribution_mean >= (attribution_threshold * 0.4) - 1e-6:
            print("CAPTUM_STATUS: PARTIALLY_VERIFIED")
        else:
            print("CAPTUM_STATUS: NOT_VERIFIED")
    except Exception as e:
        print(f"CAPTUM_ERROR: {{e}}")


if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse Captum output into verification status
pub fn parse_captum_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("CAPTUM_ERROR:") || stderr.contains("CAPTUM_ERROR:") {
        let reason = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("CAPTUM_ERROR:"))
            .map(|l| l.replace("CAPTUM_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown Captum error".to_string());
        return (VerificationStatus::Unknown { reason }, None);
    }

    if let Some(json_str) = extract_json_result(stdout, "CAPTUM_RESULT_START", "CAPTUM_RESULT_END")
    {
        if let Ok(val) = serde_json::from_str::<Value>(&json_str) {
            let status = val["status"].as_str().unwrap_or("error");
            let attribution_mean = val["attribution_mean"].as_f64().unwrap_or(0.0);
            let stability_gap = val["stability_gap"].as_f64().unwrap_or(0.0);
            let threshold = val["attribution_threshold"].as_f64().unwrap_or(0.0);

            if status == "error" {
                let reason = val["error"]
                    .as_str()
                    .unwrap_or("Unknown Captum failure")
                    .to_string();
                return (VerificationStatus::Unknown { reason }, None);
            }

            let counterexample = build_captum_counterexample(&val);

            let partial_threshold = threshold * 0.4;

            if attribution_mean >= threshold && stability_gap <= threshold * 3.0 {
                (VerificationStatus::Proven, counterexample)
            } else if attribution_mean + f64::EPSILON >= partial_threshold {
                (
                    VerificationStatus::Partial {
                        verified_percentage: (attribution_mean / threshold) * 100.0,
                    },
                    counterexample,
                )
            } else {
                (VerificationStatus::Disproven, counterexample)
            }
        } else {
            (
                VerificationStatus::Unknown {
                    reason: "Failed to parse Captum output".to_string(),
                },
                None,
            )
        }
    } else if stdout.contains("CAPTUM_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("CAPTUM_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 60.0,
            },
            None,
        )
    } else if stdout.contains("CAPTUM_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse Captum output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after = &output[start + start_marker.len()..];
        if let Some(end) = after.find(end_marker) {
            return Some(after[..end].trim().to_string());
        }
    }
    None
}
