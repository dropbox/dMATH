//! Deepchecks script generation and output parsing

use super::config::DeepchecksConfig;
use crate::counterexample::StructuredCounterexample;
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;

/// Generate a Deepchecks validation script
pub fn generate_deepchecks_script(
    _spec: &TypedSpec,
    config: &DeepchecksConfig,
) -> Result<String, BackendError> {
    let suite_type = config.suite_type.as_str();
    let task_type = config.task_type.as_str();
    let with_conditions = if config.with_conditions {
        "True"
    } else {
        "False"
    };
    let n_samples = config.n_samples;

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
Deepchecks ML validation script
Generated by DashProve

Validates data and model quality using Deepchecks suites.
"""

import sys
import json
import time
import numpy as np

try:
    import deepchecks
    from deepchecks.tabular import Dataset
    from deepchecks.tabular.suites import (
        data_integrity,
        train_test_validation,
        model_evaluation,
        full_suite,
    )
except ImportError as e:
    print(f"DEEPCHECKS_ERROR: Missing Deepchecks: {{e}}")
    print("DEEPCHECKS_ERROR: Install with: pip install deepchecks")
    sys.exit(1)

try:
    import pandas as pd
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split
except ImportError as e:
    print(f"DEEPCHECKS_ERROR: Dependencies required: {{e}}")
    sys.exit(1)


def create_test_data(n_samples, task_type):
    """Create test data for validation."""
    np.random.seed(42)

    # Features
    X = pd.DataFrame({{
        "feature_1": np.random.normal(0, 1, n_samples),
        "feature_2": np.random.uniform(0, 10, n_samples),
        "feature_3": np.random.choice(["A", "B", "C"], n_samples),
        "feature_4": np.random.exponential(1, n_samples),
        "feature_5": np.random.randint(0, 100, n_samples),
    }})

    # Target based on task type
    if task_type in ["binary", "multiclass"]:
        if task_type == "binary":
            y = (X["feature_1"] + X["feature_2"] > 5).astype(int)
        else:
            y = pd.cut(X["feature_1"] + X["feature_2"], bins=3, labels=[0, 1, 2]).astype(int)
    else:  # regression
        y = X["feature_1"] * 2 + X["feature_2"] + np.random.normal(0, 0.1, n_samples)

    return X, y


def main():
    suite_type = "{suite_type}"
    task_type = "{task_type}"
    with_conditions = {with_conditions}
    n_samples = {n_samples}

    # Create test data
    X, y = create_test_data(n_samples, task_type)

    # Split data
    X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.3, random_state=42)

    # Create Deepchecks datasets
    label_name = "target"
    cat_features = ["feature_3"]

    train_df = X_train.copy()
    train_df[label_name] = y_train
    test_df = X_test.copy()
    test_df[label_name] = y_test

    ds_train = Dataset(train_df, label=label_name, cat_features=cat_features)
    ds_test = Dataset(test_df, label=label_name, cat_features=cat_features)

    start_time = time.perf_counter()

    try:
        # Train a simple model for model evaluation suites
        model = None
        if suite_type in ["model_evaluation", "full_suite"]:
            # Encode categorical features
            X_train_enc = pd.get_dummies(X_train, columns=cat_features)
            X_test_enc = pd.get_dummies(X_test, columns=cat_features)

            # Align columns
            X_train_enc, X_test_enc = X_train_enc.align(X_test_enc, join='outer', axis=1, fill_value=0)

            model = RandomForestClassifier(n_estimators=10, random_state=42)
            model.fit(X_train_enc, y_train)

        # Select and run suite
        if suite_type == "data_integrity":
            suite = data_integrity()
            result = suite.run(ds_train)
        elif suite_type == "train_test_validation":
            suite = train_test_validation()
            result = suite.run(ds_train, ds_test)
        elif suite_type == "model_evaluation":
            suite = model_evaluation()
            result = suite.run(ds_train, ds_test, model)
        else:  # full_suite
            suite = full_suite()
            result = suite.run(ds_train, ds_test, model)

        validation_time = time.perf_counter() - start_time

        # Process results
        check_results = []
        passed = 0
        failed = 0

        for check_result in result.results:
            check_name = check_result.get_header()
            check_passed = not check_result.have_conditions() or check_result.passed_conditions()

            check_results.append({{
                "check": check_name,
                "passed": check_passed,
                "has_conditions": check_result.have_conditions(),
            }})

            if check_passed:
                passed += 1
            else:
                failed += 1

        total = passed + failed
        success_rate = passed / total if total > 0 else 0.0

        status = "success"
        error_message = None

    except Exception as e:
        status = "error"
        error_message = str(e)
        validation_time = time.perf_counter() - start_time
        passed = 0
        failed = 0
        success_rate = 0.0
        check_results = []

    print("DEEPCHECKS_RESULT_START")
    result_dict = {{
        "status": status,
        "suite_type": suite_type,
        "task_type": task_type,
        "train_samples": len(X_train),
        "test_samples": len(X_test),
        "checks_evaluated": passed + failed,
        "checks_passed": passed,
        "checks_failed": failed,
        "success_rate": success_rate,
        "validation_time_s": validation_time,
        "check_results": check_results[:20],  # Limit output
        "deepchecks_version": deepchecks.__version__,
        "error": error_message
    }}
    print(json.dumps(result_dict, indent=2, default=str))
    print("DEEPCHECKS_RESULT_END")

    if status == "success":
        print(f"\nDEEPCHECKS_SUMMARY: Suite: {{suite_type}}")
        print(f"DEEPCHECKS_SUMMARY: Checks: {{passed + failed}} evaluated")
        print(f"DEEPCHECKS_SUMMARY: Passed: {{passed}}, Failed: {{failed}}")
        print(f"DEEPCHECKS_SUMMARY: Success rate: {{success_rate:.1%}}")

        if success_rate >= 1.0:
            print("DEEPCHECKS_STATUS: VERIFIED")
        elif success_rate >= 0.8:
            print("DEEPCHECKS_STATUS: PARTIALLY_VERIFIED")
        else:
            print("DEEPCHECKS_STATUS: NOT_VERIFIED")
    else:
        print(f"DEEPCHECKS_ERROR: {{error_message}}")


if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse Deepchecks output
pub fn parse_deepchecks_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("DEEPCHECKS_ERROR:") || stderr.contains("DEEPCHECKS_ERROR:") {
        let error_msg = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("DEEPCHECKS_ERROR:"))
            .map(|l| l.replace("DEEPCHECKS_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown Deepchecks error".to_string());
        return (VerificationStatus::Unknown { reason: error_msg }, None);
    }

    if let Some(json_str) =
        extract_json_result(stdout, "DEEPCHECKS_RESULT_START", "DEEPCHECKS_RESULT_END")
    {
        if let Ok(result) = serde_json::from_str::<serde_json::Value>(&json_str) {
            let status = result["status"].as_str().unwrap_or("error");
            let success_rate = result["success_rate"].as_f64().unwrap_or(0.0);

            if status == "error" {
                let error = result["error"]
                    .as_str()
                    .unwrap_or("Unknown error")
                    .to_string();
                return (VerificationStatus::Unknown { reason: error }, None);
            }

            let counterexample = crate::counterexample::build_deepchecks_counterexample(&result);

            if success_rate >= 1.0 {
                return (VerificationStatus::Proven, None);
            } else if success_rate >= 0.8 {
                return (
                    VerificationStatus::Partial {
                        verified_percentage: success_rate * 100.0,
                    },
                    counterexample,
                );
            } else {
                return (VerificationStatus::Disproven, counterexample);
            }
        }
    }

    if stdout.contains("DEEPCHECKS_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("DEEPCHECKS_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 80.0,
            },
            None,
        )
    } else if stdout.contains("DEEPCHECKS_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse Deepchecks output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after_start = &output[start + start_marker.len()..];
        if let Some(end) = after_start.find(end_marker) {
            return Some(after_start[..end].trim().to_string());
        }
    }
    None
}
