//! OpenVINO script generation and output parsing

use super::config::OpenVINOConfig;
use crate::counterexample::StructuredCounterexample;
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;

/// Generate an OpenVINO optimization verification script
pub fn generate_openvino_script(
    _spec: &TypedSpec,
    config: &OpenVINOConfig,
) -> Result<String, BackendError> {
    let model_path = config
        .model_path
        .as_ref()
        .map(|p| p.display().to_string())
        .unwrap_or_else(|| "model.onnx".to_string());
    let device = config.device.as_str();
    let precision = config.precision.as_str();
    let perf_hint = config.performance_hint.as_str();
    let num_threads = config.num_threads;
    let num_streams = config.num_streams;
    let enable_dynamic = if config.enable_dynamic_shapes {
        "True"
    } else {
        "False"
    };
    let warmup_iters = config.warmup_iterations;
    let bench_iters = config.benchmark_iterations;

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
OpenVINO model optimization verification script
Generated by DashProve

Validates model inference on Intel hardware.
"""

import sys
import json
import time
import numpy as np

try:
    from openvino import Core, Type, PartialShape
    from openvino.runtime import properties
except ImportError as e:
    print(f"OV_ERROR: Missing OpenVINO: {{e}}")
    print("OV_ERROR: Install with: pip install openvino")
    sys.exit(1)

def create_test_model():
    """Create a simple OpenVINO model for testing."""
    try:
        from openvino import Core
        from openvino.runtime import opset13 as opset

        # Simple MLP: input -> matmul -> relu -> matmul -> output
        param = opset.parameter([1, 10], Type.f32, "input")
        w1 = opset.constant(np.random.randn(10, 20).astype(np.float32))
        matmul1 = opset.matmul(param, w1, False, False)
        relu = opset.relu(matmul1)
        w2 = opset.constant(np.random.randn(20, 5).astype(np.float32))
        matmul2 = opset.matmul(relu, w2, False, False)
        result = opset.result(matmul2)
        result.set_names({{"output"}})

        from openvino import Model
        model = Model([result], [param], "test_model")
        return model
    except Exception as e:
        print(f"OV_INFO: Failed to create test model: {{e}}")
        return None

def load_model(core, model_path: str):
    """Load model from file."""
    try:
        model = core.read_model(model_path)
        return model
    except Exception as e:
        print(f"OV_INFO: Failed to load model: {{e}}")
        return None

def main():
    model_path = "{model_path}"
    device = "{device}"
    precision = "{precision}"
    perf_hint = "{perf_hint}"
    num_threads = {num_threads}
    num_streams = {num_streams}
    enable_dynamic = {enable_dynamic}
    warmup_iters = {warmup_iters}
    bench_iters = {bench_iters}

    core = Core()

    # List available devices
    available_devices = core.available_devices
    print(f"OV_INFO: Available devices: {{available_devices}}")

    # Load or create model
    model = load_model(core, model_path)
    if model is None:
        print("OV_INFO: Using synthetic test model")
        model = create_test_model()
        if model is None:
            print("OV_ERROR: Failed to create test model")
            sys.exit(1)

    # Configure model
    config = {{}}
    if perf_hint != "UNDEFINED":
        config[properties.hint.performance_mode()] = perf_hint
    if num_threads > 0:
        config[properties.inference_num_threads()] = num_threads
    if num_streams > 1:
        config[properties.num_streams()] = num_streams

    # Check if device is available
    target_device = device
    if device not in available_devices:
        print(f"OV_INFO: Device {{device}} not available, falling back to CPU")
        target_device = "CPU"

    # Compile model
    try:
        compiled_model = core.compile_model(model, target_device, config)
    except Exception as e:
        print(f"OV_ERROR: Failed to compile model: {{e}}")
        sys.exit(1)

    # Create inference request
    infer_request = compiled_model.create_infer_request()

    # Get input/output info
    input_layer = compiled_model.input(0)
    output_layer = compiled_model.output(0)

    input_shape = list(input_layer.shape)
    output_shape = list(output_layer.shape)

    # Handle dynamic dimensions
    for i, dim in enumerate(input_shape):
        if dim <= 0:
            input_shape[i] = 1
    for i, dim in enumerate(output_shape):
        if dim <= 0:
            output_shape[i] = 1

    # Generate test input
    np.random.seed(42)
    test_input = np.random.randn(*input_shape).astype(np.float32)

    # Warmup
    for _ in range(warmup_iters):
        infer_request.infer({{input_layer: test_input}})

    # Benchmark
    latencies = []
    outputs = []
    for _ in range(bench_iters):
        start = time.perf_counter()
        infer_request.infer({{input_layer: test_input}})
        latencies.append((time.perf_counter() - start) * 1000)
        outputs.append(infer_request.get_output_tensor().data.copy())

    # Check consistency
    reference = outputs[0]
    consistent = True
    max_diff = 0.0
    for out in outputs[1:]:
        diff = np.max(np.abs(out - reference))
        max_diff = max(max_diff, diff)
        if diff > 1e-5:
            consistent = False

    # Statistics
    latencies = np.array(latencies)
    mean_lat = float(np.mean(latencies))
    std_lat = float(np.std(latencies))
    p50_lat = float(np.percentile(latencies, 50))
    p95_lat = float(np.percentile(latencies, 95))
    p99_lat = float(np.percentile(latencies, 99))
    min_lat = float(np.min(latencies))
    max_lat = float(np.max(latencies))
    throughput = 1000.0 / mean_lat

    print("OV_RESULT_START")
    result = {{
        "model_path": model_path,
        "device_requested": device,
        "device_used": target_device,
        "available_devices": available_devices,
        "precision": precision,
        "performance_hint": perf_hint,
        "input_shape": input_shape,
        "output_shape": output_shape,
        "warmup_iterations": warmup_iters,
        "benchmark_iterations": bench_iters,
        "consistent_outputs": consistent,
        "max_output_diff": float(max_diff),
        "latency_ms": {{
            "mean": mean_lat,
            "std": std_lat,
            "min": min_lat,
            "max": max_lat,
            "p50": p50_lat,
            "p95": p95_lat,
            "p99": p99_lat
        }},
        "throughput_ips": throughput
    }}
    print(json.dumps(result, indent=2))
    print("OV_RESULT_END")

    print(f"\\nOV_SUMMARY: Mean latency: {{mean_lat:.3f}}ms (std: {{std_lat:.3f}}ms)")
    print(f"OV_SUMMARY: P95: {{p95_lat:.3f}}ms, P99: {{p99_lat:.3f}}ms")
    print(f"OV_SUMMARY: Throughput: {{throughput:.1f}} inf/sec")
    print(f"OV_SUMMARY: Output consistency: {{'PASS' if consistent else 'FAIL'}}")

    if consistent and max_diff < 1e-5:
        print("OV_STATUS: VERIFIED")
    elif consistent:
        print("OV_STATUS: PARTIALLY_VERIFIED")
    else:
        print("OV_STATUS: NOT_VERIFIED")

if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse OpenVINO output
pub fn parse_openvino_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("OV_ERROR:") || stderr.contains("OV_ERROR:") {
        let error_msg = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("OV_ERROR:"))
            .map(|l| l.replace("OV_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown OpenVINO error".to_string());
        return (VerificationStatus::Unknown { reason: error_msg }, None);
    }

    if let Some(json_str) = extract_json_result(stdout, "OV_RESULT_START", "OV_RESULT_END") {
        if let Ok(result) = serde_json::from_str::<serde_json::Value>(&json_str) {
            let consistent = result["consistent_outputs"].as_bool().unwrap_or(false);
            let max_diff = result["max_output_diff"].as_f64().unwrap_or(1.0);

            let counterexample =
                crate::counterexample::build_inference_counterexample(&result, "OpenVINO");

            if consistent && max_diff < 1e-5 {
                return (VerificationStatus::Proven, None);
            } else if consistent {
                return (
                    VerificationStatus::Partial {
                        verified_percentage: 95.0,
                    },
                    counterexample,
                );
            } else {
                return (VerificationStatus::Disproven, counterexample);
            }
        }
    }

    if stdout.contains("OV_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("OV_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 95.0,
            },
            None,
        )
    } else if stdout.contains("OV_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse OpenVINO output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after_start = &output[start + start_marker.len()..];
        if let Some(end) = after_start.find(end_marker) {
            return Some(after_start[..end].trim().to_string());
        }
    }
    None
}
