//! Aequitas script generation and output parsing

use super::config::AequitasConfig;
use crate::counterexample::{build_aequitas_counterexample, StructuredCounterexample};
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;

/// Generate an Aequitas bias audit script
pub fn generate_aequitas_script(
    _spec: &TypedSpec,
    config: &AequitasConfig,
) -> Result<String, BackendError> {
    let fairness_metric = config.fairness_metric.as_str();
    let reference_group = config.reference_group.as_str();
    let disparity_tolerance = config.disparity_tolerance;
    let n_samples = config.n_samples;

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
Aequitas bias audit script
Generated by DashProve

Audits algorithmic fairness using Aequitas.
"""

import sys
import json
import time
import numpy as np

try:
    import aequitas
    from aequitas.group import Group
    from aequitas.bias import Bias
    from aequitas.fairness import Fairness
except ImportError as e:
    print(f"AEQUITAS_ERROR: Missing Aequitas: {{e}}")
    print("AEQUITAS_ERROR: Install with: pip install aequitas")
    sys.exit(1)

try:
    import pandas as pd
    from sklearn.ensemble import RandomForestClassifier
    from sklearn.model_selection import train_test_split
except ImportError as e:
    print(f"AEQUITAS_ERROR: Dependencies required: {{e}}")
    sys.exit(1)


def create_test_data(n_samples):
    """Create test data with protected attributes."""
    np.random.seed(42)

    df = pd.DataFrame({{
        "feature_1": np.random.normal(0, 1, n_samples),
        "feature_2": np.random.uniform(0, 10, n_samples),
        "race": np.random.choice(["white", "black", "hispanic", "asian"], n_samples, p=[0.5, 0.2, 0.2, 0.1]),
        "sex": np.random.choice(["male", "female"], n_samples, p=[0.5, 0.5]),
    }})

    # Target with some correlation
    df["label_value"] = ((df["feature_1"] + df["feature_2"] > 5) & (np.random.random(n_samples) > 0.2)).astype(int)

    return df


def main():
    fairness_metric = "{fairness_metric}"
    reference_group = "{reference_group}"
    disparity_tolerance = {disparity_tolerance}
    n_samples = {n_samples}

    # Create test data
    df = create_test_data(n_samples)

    # Split data
    train_df, test_df = train_test_split(df, test_size=0.3, random_state=42)

    start_time = time.perf_counter()

    try:
        # Train model
        feature_cols = ["feature_1", "feature_2"]
        X_train = train_df[feature_cols].values
        y_train = train_df["label_value"].values
        X_test = test_df[feature_cols].values

        model = RandomForestClassifier(n_estimators=50, random_state=42)
        model.fit(X_train, y_train)

        # Predict
        y_pred = model.predict(X_test)

        # Prepare data for Aequitas
        audit_df = test_df.copy()
        audit_df["score"] = y_pred

        # Run Aequitas audit
        g = Group()
        xtab, _ = g.get_crosstabs(audit_df)

        # Calculate bias metrics
        b = Bias()
        bdf = b.get_disparity_predefined_groups(
            xtab,
            original_df=audit_df,
            ref_groups_dict={{'race': 'white', 'sex': 'male'}},
            alpha=0.05
        )

        # Calculate fairness
        f = Fairness()
        fdf = f.get_group_value_fairness(bdf)

        # Extract key metrics
        metrics_by_group = {{}}
        disparity_ratios = []

        for _, row in bdf.iterrows():
            attr = row.get("attribute_name", "unknown")
            group = row.get("attribute_value", "unknown")
            key = f"{{attr}}_{{group}}"

            pprev_disparity = row.get("pprev_disparity", 1.0)
            ppr_disparity = row.get("ppr_disparity", 1.0)
            fpr_disparity = row.get("fpr_disparity", 1.0)
            fnr_disparity = row.get("fnr_disparity", 1.0)

            metrics_by_group[key] = {{
                "pprev_disparity": pprev_disparity,
                "ppr_disparity": ppr_disparity,
                "fpr_disparity": fpr_disparity,
                "fnr_disparity": fnr_disparity,
            }}

            if fairness_metric == "predictive_parity":
                disparity_ratios.append(ppr_disparity)
            elif fairness_metric == "fpr_parity":
                disparity_ratios.append(fpr_disparity)
            elif fairness_metric == "fnr_parity":
                disparity_ratios.append(fnr_disparity)
            else:
                disparity_ratios.append(ppr_disparity)

        # Check fairness (all disparities should be within tolerance)
        valid_ratios = [r for r in disparity_ratios if pd.notna(r) and r > 0]
        if valid_ratios:
            min_disparity = min(valid_ratios)
            max_disparity = max(valid_ratios)
            avg_disparity = np.mean(valid_ratios)
        else:
            min_disparity = 1.0
            max_disparity = 1.0
            avg_disparity = 1.0

        # Fair if all disparities are within tolerance
        is_fair = all(r >= disparity_tolerance for r in valid_ratios if pd.notna(r))

        audit_time = time.perf_counter() - start_time

        status = "success"
        error_message = None

    except Exception as e:
        status = "error"
        error_message = str(e)
        audit_time = time.perf_counter() - start_time
        min_disparity = None
        max_disparity = None
        avg_disparity = None
        is_fair = False
        metrics_by_group = {{}}

    print("AEQUITAS_RESULT_START")
    result = {{
        "status": status,
        "fairness_metric": fairness_metric,
        "reference_group": reference_group,
        "n_samples": n_samples,
        "min_disparity_ratio": min_disparity,
        "max_disparity_ratio": max_disparity,
        "avg_disparity_ratio": avg_disparity,
        "disparity_tolerance": disparity_tolerance,
        "is_fair": is_fair,
        "metrics_by_group": metrics_by_group,
        "audit_time_s": audit_time,
        "aequitas_version": aequitas.__version__,
        "error": error_message
    }}
    print(json.dumps(result, indent=2, default=str))
    print("AEQUITAS_RESULT_END")

    if status == "success":
        print(f"\nAEQUITAS_SUMMARY: Fairness metric: {{fairness_metric}}")
        print(f"AEQUITAS_SUMMARY: Min disparity ratio: {{min_disparity:.4f}}")
        print(f"AEQUITAS_SUMMARY: Avg disparity ratio: {{avg_disparity:.4f}}")
        print(f"AEQUITAS_SUMMARY: Tolerance: {{disparity_tolerance}}")
        print(f"AEQUITAS_SUMMARY: Is fair: {{is_fair}}")

        if is_fair:
            print("AEQUITAS_STATUS: VERIFIED")
        elif min_disparity >= disparity_tolerance * 0.9:
            print("AEQUITAS_STATUS: PARTIALLY_VERIFIED")
        else:
            print("AEQUITAS_STATUS: NOT_VERIFIED")
    else:
        print(f"AEQUITAS_ERROR: {{error_message}}")


if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse Aequitas output
pub fn parse_aequitas_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("AEQUITAS_ERROR:") || stderr.contains("AEQUITAS_ERROR:") {
        let error_msg = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("AEQUITAS_ERROR:"))
            .map(|l| l.replace("AEQUITAS_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown Aequitas error".to_string());
        return (VerificationStatus::Unknown { reason: error_msg }, None);
    }

    if let Some(json_str) =
        extract_json_result(stdout, "AEQUITAS_RESULT_START", "AEQUITAS_RESULT_END")
    {
        if let Ok(result) = serde_json::from_str::<serde_json::Value>(&json_str) {
            let status = result["status"].as_str().unwrap_or("error");
            let is_fair = result["is_fair"].as_bool().unwrap_or(false);
            let min_disparity = result["min_disparity_ratio"].as_f64();
            let tolerance = result["disparity_tolerance"].as_f64().unwrap_or(0.8);

            if status == "error" {
                let error = result["error"]
                    .as_str()
                    .unwrap_or("Unknown error")
                    .to_string();
                return (VerificationStatus::Unknown { reason: error }, None);
            }

            // Use the shared fairness counterexample helper with populated failed_checks
            let counterexample = build_aequitas_counterexample(&result);

            if is_fair {
                return (VerificationStatus::Proven, None);
            } else if let Some(min_disp) = min_disparity {
                if min_disp >= tolerance * 0.9 {
                    return (
                        VerificationStatus::Partial {
                            verified_percentage: (min_disp / tolerance) * 100.0,
                        },
                        counterexample,
                    );
                }
            }
            return (VerificationStatus::Disproven, counterexample);
        }
    }

    if stdout.contains("AEQUITAS_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("AEQUITAS_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 80.0,
            },
            None,
        )
    } else if stdout.contains("AEQUITAS_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse Aequitas output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after_start = &output[start + start_marker.len()..];
        if let Some(end) = after_start.find(end_marker) {
            return Some(after_start[..end].trim().to_string());
        }
    }
    None
}
