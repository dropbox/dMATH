//! LIME script generation and output parsing

use super::config::LimeConfig;
use crate::counterexample::{build_lime_counterexample, StructuredCounterexample};
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;
use serde_json::Value;

/// Generate LIME explanation script
pub fn generate_lime_script(
    _spec: &TypedSpec,
    config: &LimeConfig,
) -> Result<String, BackendError> {
    let task_type = config.task_type.as_str();
    let num_features = config.num_features;
    let num_samples = config.num_samples;
    let kernel_width = config.kernel_width.to_python();
    let discretize = if config.discretize_continuous {
        "True"
    } else {
        "False"
    };
    let fidelity_threshold = config.fidelity_threshold;

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
LIME interpretability verification script generated by DashProve
Evaluates local fidelity and explanation coverage.
"""

import json
import sys
import numpy as np
import time

try:
    import lime
    from lime.lime_tabular import LimeTabularExplainer
    from sklearn.datasets import make_classification, make_regression
    from sklearn.ensemble import RandomForestClassifier, RandomForestRegressor
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, r2_score
    from sklearn.preprocessing import StandardScaler
except ImportError as e:
    print(f"LIME_ERROR: Missing dependencies: {{e}}")
    sys.exit(1)


def prepare_data(task_type, n_samples):
    if task_type == "classification":
        X, y = make_classification(
            n_samples=n_samples,
            n_features=8,
            n_informative=5,
            n_redundant=1,
            random_state=21,
        )
    else:
        X, y = make_regression(
            n_samples=n_samples,
            n_features=8,
            n_informative=5,
            noise=0.3,
            random_state=21,
        )

    scaler = StandardScaler()
    X = scaler.fit_transform(X)
    return X, y


def main():
    task_type = "{task_type}"
    num_features = {num_features}
    num_samples = {num_samples}
    kernel_width = {kernel_width}
    discretize_continuous = {discretize}
    fidelity_threshold = {fidelity_threshold}

    X, y = prepare_data(task_type, max(600, num_samples))

    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=0.25, random_state=11, stratify=y if task_type == "classification" else None
    )

    start_time = time.perf_counter()

    try:
        if task_type == "classification":
            model = RandomForestClassifier(
                n_estimators=80,
                random_state=11,
                class_weight="balanced",
            )
        else:
            model = RandomForestRegressor(
                n_estimators=80,
                random_state=11,
            )

        model.fit(X_train, y_train)

        class_names = None
        if task_type == "classification":
            classes = np.unique(y_train)
            class_names = [str(int(c)) for c in classes]

        explainer = LimeTabularExplainer(
            X_train,
            mode="classification" if task_type == "classification" else "regression",
            class_names=class_names,
            discretize_continuous=discretize_continuous,
            kernel_width=kernel_width,
            feature_names=[f"f{{i}}" for i in range(X_train.shape[1])],
        )

        idx = min(10, len(X_test) - 1)
        instance = X_test[idx]

        explanation = explainer.explain_instance(
            instance,
            model.predict_proba if task_type == "classification" else model.predict,
            num_features=num_features,
            num_samples=num_samples,
        )

        local_pred = (
            model.predict([instance])[0]
            if task_type == "regression"
            else model.predict_proba([instance])[0].tolist()
        )

        fidelity = explanation.score
        weights = explanation.as_list()
        feature_importance = [abs(w[1]) for w in weights]
        coverage = len(feature_importance) / num_features if num_features > 0 else 0.0

        model_score = (
            float(accuracy_score(y_test, model.predict(X_test)))
            if task_type == "classification"
            else float(r2_score(y_test, model.predict(X_test)))
        )

        result = {{
            "status": "success",
            "task_type": task_type,
            "num_features": num_features,
            "num_samples": num_samples,
            "kernel_width": kernel_width,
            "fidelity": float(fidelity),
            "coverage": float(coverage),
            "mean_weight": float(np.mean(feature_importance)) if feature_importance else 0.0,
            "max_weight": float(np.max(feature_importance)) if feature_importance else 0.0,
            "top_feature": weights[0][0] if weights else None,
            "model_score": model_score,
            "local_prediction": local_pred,
            "duration_s": time.perf_counter() - start_time,
            "fidelity_threshold": fidelity_threshold,
        }}

        print("LIME_RESULT_START")
        print(json.dumps(result, indent=2, default=str))
        print("LIME_RESULT_END")

        if fidelity >= fidelity_threshold and coverage >= 0.6:
            print("LIME_STATUS: VERIFIED")
        elif fidelity >= fidelity_threshold * 0.7:
            print("LIME_STATUS: PARTIALLY_VERIFIED")
        else:
            print("LIME_STATUS: NOT_VERIFIED")
    except Exception as e:
        print(f"LIME_ERROR: {{e}}")


if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse LIME output into verification status
pub fn parse_lime_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("LIME_ERROR:") || stderr.contains("LIME_ERROR:") {
        let reason = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("LIME_ERROR:"))
            .map(|l| l.replace("LIME_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown LIME error".to_string());
        return (VerificationStatus::Unknown { reason }, None);
    }

    if let Some(json_str) = extract_json_result(stdout, "LIME_RESULT_START", "LIME_RESULT_END") {
        if let Ok(val) = serde_json::from_str::<Value>(&json_str) {
            let status = val["status"].as_str().unwrap_or("error");
            let fidelity = val["fidelity"].as_f64().unwrap_or(0.0);
            let threshold = val["fidelity_threshold"].as_f64().unwrap_or(0.0);
            let coverage = val["coverage"].as_f64().unwrap_or(0.0);

            if status == "error" {
                let reason = val["error"]
                    .as_str()
                    .unwrap_or("Unknown LIME failure")
                    .to_string();
                return (VerificationStatus::Unknown { reason }, None);
            }

            let counterexample = build_lime_counterexample(&val);

            if fidelity >= threshold && coverage >= 0.6 {
                (VerificationStatus::Proven, counterexample)
            } else if fidelity >= threshold * 0.7 {
                (
                    VerificationStatus::Partial {
                        verified_percentage: (fidelity / threshold) * 100.0,
                    },
                    counterexample,
                )
            } else {
                (VerificationStatus::Disproven, counterexample)
            }
        } else {
            (
                VerificationStatus::Unknown {
                    reason: "Failed to parse LIME output".to_string(),
                },
                None,
            )
        }
    } else if stdout.contains("LIME_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("LIME_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 80.0,
            },
            None,
        )
    } else if stdout.contains("LIME_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse LIME output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after = &output[start + start_marker.len()..];
        if let Some(end) = after.find(end_marker) {
            return Some(after[..end].trim().to_string());
        }
    }
    None
}
