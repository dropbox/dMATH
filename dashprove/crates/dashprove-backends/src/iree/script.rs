//! IREE script generation and output parsing

use super::config::IREEConfig;
use crate::counterexample::StructuredCounterexample;
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;

/// Generate an IREE compilation verification script
pub fn generate_iree_script(
    _spec: &TypedSpec,
    config: &IREEConfig,
) -> Result<String, BackendError> {
    let model_path = config
        .model_path
        .as_ref()
        .map(|p| p.display().to_string())
        .unwrap_or_else(|| "model.mlir".to_string());
    let target = config.target.as_str();
    let input_format = config.input_format.as_str();
    let execution_mode = config.execution_mode.as_str();
    let enable_optimization = if config.enable_optimization {
        "True"
    } else {
        "False"
    };
    let warmup_iters = config.warmup_iterations;
    let bench_iters = config.benchmark_iterations;
    let enable_tracing = if config.enable_tracing {
        "True"
    } else {
        "False"
    };

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
IREE model compilation verification script
Generated by DashProve

Validates IREE compilation and inference correctness.
"""

import sys
import json
import time
import numpy as np

try:
    import iree.runtime as rt
    import iree.compiler as compiler
except ImportError as e:
    print(f"IREE_ERROR: Missing IREE: {{e}}")
    print("IREE_ERROR: Install with: pip install iree-compiler iree-runtime")
    sys.exit(1)

def create_simple_mlir():
    """Create a simple MLIR module for testing."""
    return '''
module @test {{
  func.func @main(%arg0: tensor<1x10xf32>) -> tensor<1x5xf32> {{
    %cst = arith.constant dense<1.0> : tensor<10x5xf32>
    %0 = "stablehlo.dot"(%arg0, %cst) : (tensor<1x10xf32>, tensor<10x5xf32>) -> tensor<1x5xf32>
    return %0 : tensor<1x5xf32>
  }}
}}
'''

def main():
    model_path = "{model_path}"
    target_backend = "{target}"
    input_format = "{input_format}"
    execution_mode = "{execution_mode}"
    enable_optimization = {enable_optimization}
    warmup_iters = {warmup_iters}
    bench_iters = {bench_iters}
    enable_tracing = {enable_tracing}

    # Try loading model or use synthetic
    try:
        with open(model_path, 'r') as f:
            mlir_source = f.read()
        print(f"IREE_INFO: Loaded model from {{model_path}}")
    except FileNotFoundError:
        print("IREE_INFO: Using synthetic test model")
        mlir_source = create_simple_mlir()

    # Compile with IREE
    try:
        # Set compilation flags
        compile_flags = [f"--iree-hal-target-backends={{target_backend}}"]
        if enable_optimization:
            compile_flags.append("--iree-opt-data-tiling")
            compile_flags.append("--iree-opt-const-expr-hoisting")

        # Try target-specific compilation
        try:
            compiled_module = compiler.compile_str(
                mlir_source,
                target_backends=[target_backend],
                extra_args=compile_flags if enable_optimization else []
            )
            target_used = target_backend
        except Exception as e:
            print(f"IREE_INFO: Target '{{target_backend}}' failed ({{e}}), falling back to llvm-cpu")
            compiled_module = compiler.compile_str(
                mlir_source,
                target_backends=["llvm-cpu"]
            )
            target_used = "llvm-cpu"

    except Exception as e:
        print(f"IREE_ERROR: Compilation failed: {{e}}")
        sys.exit(1)

    # Create runtime
    try:
        config = rt.Config(driver_name=target_used.replace("-", "_") if "cpu" not in target_used else "local-sync")
        instance = rt.SystemContext(config=config)

        # Load compiled module
        vm_module = rt.VmModule.copy_buffer(instance.instance, compiled_module)
        context = rt.SystemContext(config=config)
        context.add_vm_module(vm_module)
    except Exception as e:
        print(f"IREE_INFO: Runtime init failed ({{e}}), using default config")
        config = rt.Config("local-sync")
        instance = rt.SystemContext(config=config)
        vm_module = rt.VmModule.copy_buffer(instance.instance, compiled_module)
        context = rt.SystemContext(config=config)
        context.add_vm_module(vm_module)

    # Get function
    try:
        f = context.modules.module["main"]
    except (KeyError, AttributeError):
        # Try alternative naming
        try:
            f = context.modules.test["main"]
        except Exception:
            print("IREE_ERROR: Could not find 'main' function in compiled module")
            sys.exit(1)

    # Generate test input
    np.random.seed(42)
    test_input = np.random.randn(1, 10).astype(np.float32)

    # Warmup
    for _ in range(warmup_iters):
        _ = f(test_input)

    # Benchmark
    latencies = []
    outputs = []
    for _ in range(bench_iters):
        start = time.perf_counter()
        result = f(test_input)
        latencies.append((time.perf_counter() - start) * 1000)
        outputs.append(np.array(result).copy())

    # Check consistency
    reference = outputs[0]
    consistent = True
    max_diff = 0.0
    for out in outputs[1:]:
        diff = np.max(np.abs(out - reference))
        max_diff = max(max_diff, diff)
        if diff > 1e-5:
            consistent = False

    # Statistics
    latencies = np.array(latencies)
    mean_lat = float(np.mean(latencies))
    std_lat = float(np.std(latencies))
    p50_lat = float(np.percentile(latencies, 50))
    p95_lat = float(np.percentile(latencies, 95))
    p99_lat = float(np.percentile(latencies, 99))
    min_lat = float(np.min(latencies))
    max_lat = float(np.max(latencies))
    throughput = 1000.0 / mean_lat

    output_shape = list(reference.shape)

    print("IREE_RESULT_START")
    result = {{
        "model_path": model_path,
        "target_requested": target_backend,
        "target_used": target_used,
        "input_format": input_format,
        "execution_mode": execution_mode,
        "iree_version": rt.version,
        "input_shape": [1, 10],
        "output_shape": output_shape,
        "warmup_iterations": warmup_iters,
        "benchmark_iterations": bench_iters,
        "consistent_outputs": consistent,
        "max_output_diff": float(max_diff),
        "latency_ms": {{
            "mean": mean_lat,
            "std": std_lat,
            "min": min_lat,
            "max": max_lat,
            "p50": p50_lat,
            "p95": p95_lat,
            "p99": p99_lat
        }},
        "throughput_ips": throughput
    }}
    print(json.dumps(result, indent=2))
    print("IREE_RESULT_END")

    print(f"\\nIREE_SUMMARY: Mean latency: {{mean_lat:.3f}}ms (std: {{std_lat:.3f}}ms)")
    print(f"IREE_SUMMARY: P95: {{p95_lat:.3f}}ms, P99: {{p99_lat:.3f}}ms")
    print(f"IREE_SUMMARY: Throughput: {{throughput:.1f}} inf/sec")
    print(f"IREE_SUMMARY: Output consistency: {{'PASS' if consistent else 'FAIL'}}")

    if consistent and max_diff < 1e-5:
        print("IREE_STATUS: VERIFIED")
    elif consistent:
        print("IREE_STATUS: PARTIALLY_VERIFIED")
    else:
        print("IREE_STATUS: NOT_VERIFIED")

if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse IREE output
pub fn parse_iree_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("IREE_ERROR:") || stderr.contains("IREE_ERROR:") {
        let error_msg = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("IREE_ERROR:"))
            .map(|l| l.replace("IREE_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown IREE error".to_string());
        return (VerificationStatus::Unknown { reason: error_msg }, None);
    }

    if let Some(json_str) = extract_json_result(stdout, "IREE_RESULT_START", "IREE_RESULT_END") {
        if let Ok(result) = serde_json::from_str::<serde_json::Value>(&json_str) {
            let consistent = result["consistent_outputs"].as_bool().unwrap_or(false);
            let max_diff = result["max_output_diff"].as_f64().unwrap_or(1.0);

            let counterexample =
                crate::counterexample::build_compiler_counterexample(&result, "IREE");

            if consistent && max_diff < 1e-5 {
                return (VerificationStatus::Proven, None);
            } else if consistent {
                return (
                    VerificationStatus::Partial {
                        verified_percentage: 95.0,
                    },
                    counterexample,
                );
            } else {
                return (VerificationStatus::Disproven, counterexample);
            }
        }
    }

    if stdout.contains("IREE_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("IREE_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 95.0,
            },
            None,
        )
    } else if stdout.contains("IREE_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse IREE output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after_start = &output[start + start_marker.len()..];
        if let Some(end) = after_start.find(end_marker) {
            return Some(after_start[..end].trim().to_string());
        }
    }
    None
}
