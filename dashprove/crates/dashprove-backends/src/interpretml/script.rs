//! InterpretML script generation and output parsing

use super::config::InterpretMlConfig;
use crate::counterexample::{build_interpretml_counterexample, StructuredCounterexample};
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;
use serde_json::Value;

/// Generate InterpretML verification script
pub fn generate_interpretml_script(
    _spec: &TypedSpec,
    config: &InterpretMlConfig,
) -> Result<String, BackendError> {
    let explainer = config.explainer.as_str();
    let task = config.task.as_str();
    let max_bins = config.max_bins;
    let max_interactions = config.max_interactions;
    let validation_fraction = config.validation_fraction;
    let importance_threshold = config.importance_threshold;

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
InterpretML interpretability verification generated by DashProve.
Trains a simple glassbox model and reports feature importances.
"""

import json
import sys
import time
import numpy as np

try:
    from interpret.glassbox import (
        ExplainableBoostingClassifier,
        ExplainableBoostingRegressor,
        LogisticRegression,
        LinearRegression,
        ClassificationTree,
        RegressionTree,
    )
    from sklearn.datasets import make_classification, make_regression
    from sklearn.model_selection import train_test_split
    from sklearn.metrics import accuracy_score, r2_score
    from sklearn.preprocessing import StandardScaler
except ImportError as e:
    print(f"INTERPRET_ERROR: Missing dependencies: {{e}}")
    sys.exit(1)


def prepare_data(task_type, n_samples=800):
    if task_type == "classification":
        X, y = make_classification(
            n_samples=n_samples,
            n_features=10,
            n_informative=6,
            n_redundant=2,
            random_state=17,
        )
    else:
        X, y = make_regression(
            n_samples=n_samples,
            n_features=10,
            n_informative=6,
            noise=0.25,
            random_state=17,
        )
    scaler = StandardScaler()
    X = scaler.fit_transform(X)
    return X, y


def build_model(explainer, task, max_bins, max_interactions):
    if explainer == "linear" and task == "classification":
        return LogisticRegression(random_state=17)
    if explainer == "linear":
        return LinearRegression(random_state=17)

    if explainer == "tree" and task == "classification":
        return ClassificationTree(random_state=17, max_depth=4)
    if explainer == "tree":
        return RegressionTree(random_state=17, max_depth=4)

    if task == "classification":
        return ExplainableBoostingClassifier(
            interactions=max_interactions,
            max_bins=max_bins,
            random_state=17,
        )
    return ExplainableBoostingRegressor(
        interactions=max_interactions,
        max_bins=max_bins,
        random_state=17,
    )


def extract_importance(global_exp):
    data = global_exp.data() if hasattr(global_exp, "data") else {{}}
    scores = data.get("scores") or data.get("importance") or []
    names = data.get("names") or [f"f{{i}}" for i in range(len(scores))]
    return names, scores


def main():
    explainer = "{explainer}"
    task = "{task}"
    max_bins = {max_bins}
    max_interactions = {max_interactions}
    validation_fraction = {validation_fraction}
    importance_threshold = {importance_threshold}

    X, y = prepare_data(task)
    X_train, X_test, y_train, y_test = train_test_split(
        X, y, test_size=validation_fraction, random_state=5, stratify=y if task == "classification" else None
    )

    start_time = time.perf_counter()

    try:
        model = build_model(explainer, task, max_bins, max_interactions)
        model.fit(X_train, y_train)

        if hasattr(model, "explain_global"):
            global_exp = model.explain_global()
            names, scores = extract_importance(global_exp)
        else:
            # Fallback importance from coefficients
            if hasattr(model, "coef_"):
                scores = model.coef_.ravel()
                names = [f"f{{i}}" for i in range(len(scores))]
            else:
                scores = []
                names = []

        scores = np.array(scores) if len(scores) else np.zeros(X_train.shape[1])
        mean_importance = float(np.mean(np.abs(scores))) if len(scores) else 0.0
        max_importance = float(np.max(np.abs(scores))) if len(scores) else 0.0
        top_feature_idx = int(np.argmax(np.abs(scores))) if len(scores) else 0
        top_feature_name = names[top_feature_idx] if names else None

        model_score = (
            float(accuracy_score(y_test, model.predict(X_test)))
            if task == "classification"
            else float(r2_score(y_test, model.predict(X_test)))
        )

        result = {{
            "status": "success",
            "explainer": explainer,
            "task": task,
            "max_bins": max_bins,
            "max_interactions": max_interactions,
            "importance_threshold": importance_threshold,
            "mean_importance": mean_importance,
            "max_importance": max_importance,
            "top_feature": top_feature_name,
            "model_score": model_score,
            "duration_s": time.perf_counter() - start_time,
        }}

        print("INTERPRET_RESULT_START")
        print(json.dumps(result, indent=2, default=float))
        print("INTERPRET_RESULT_END")

        if mean_importance >= importance_threshold:
            print("INTERPRET_STATUS: VERIFIED")
        elif mean_importance >= importance_threshold * 0.5:
            print("INTERPRET_STATUS: PARTIALLY_VERIFIED")
        else:
            print("INTERPRET_STATUS: NOT_VERIFIED")
    except Exception as e:
        print(f"INTERPRET_ERROR: {{e}}")


if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse InterpretML output
pub fn parse_interpretml_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("INTERPRET_ERROR:") || stderr.contains("INTERPRET_ERROR:") {
        let reason = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("INTERPRET_ERROR:"))
            .map(|l| l.replace("INTERPRET_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown InterpretML error".to_string());
        return (VerificationStatus::Unknown { reason }, None);
    }

    if let Some(json_str) =
        extract_json_result(stdout, "INTERPRET_RESULT_START", "INTERPRET_RESULT_END")
    {
        if let Ok(val) = serde_json::from_str::<Value>(&json_str) {
            let status = val["status"].as_str().unwrap_or("error");
            let mean_importance = val["mean_importance"].as_f64().unwrap_or(0.0);
            let threshold = val["importance_threshold"].as_f64().unwrap_or(0.0);

            if status == "error" {
                let reason = val["error"]
                    .as_str()
                    .unwrap_or("Unknown InterpretML failure")
                    .to_string();
                return (VerificationStatus::Unknown { reason }, None);
            }

            let counterexample = build_interpretml_counterexample(&val);

            if mean_importance >= threshold {
                (VerificationStatus::Proven, counterexample)
            } else if mean_importance >= threshold * 0.5 {
                (
                    VerificationStatus::Partial {
                        verified_percentage: (mean_importance / threshold) * 100.0,
                    },
                    counterexample,
                )
            } else {
                (VerificationStatus::Disproven, counterexample)
            }
        } else {
            (
                VerificationStatus::Unknown {
                    reason: "Failed to parse InterpretML output".to_string(),
                },
                None,
            )
        }
    } else if stdout.contains("INTERPRET_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("INTERPRET_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 60.0,
            },
            None,
        )
    } else if stdout.contains("INTERPRET_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse InterpretML output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after = &output[start + start_marker.len()..];
        if let Some(end) = after.find(end_marker) {
            return Some(after[..end].trim().to_string());
        }
    }
    None
}
