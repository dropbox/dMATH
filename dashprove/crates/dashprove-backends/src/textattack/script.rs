//! TextAttack script generation and output parsing

use super::config::TextAttackConfig;
use crate::counterexample::StructuredCounterexample;
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::ast::Property;
use dashprove_usl::typecheck::TypedSpec;

/// Generate a TextAttack evaluation script from USL spec
pub fn generate_textattack_script(
    spec: &TypedSpec,
    config: &TextAttackConfig,
) -> Result<String, BackendError> {
    let recipe = config.attack_recipe.recipe_name();
    let num_examples = config.num_examples;
    let max_percent = config.max_percent_words;
    let min_similarity = config.min_similarity;

    // Extract model name from USL spec or use config
    let model_name = extract_model_from_spec(spec)
        .or_else(|| config.model_name.clone())
        .unwrap_or_else(|| "textattack/bert-base-uncased-SST-2".to_string());

    // Extract dataset name from spec or config
    let dataset_name = extract_dataset_from_spec(spec)
        .or_else(|| config.dataset_name.clone())
        .unwrap_or_else(|| "sst2".to_string());

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
TextAttack NLP adversarial robustness evaluation script
Generated by DashProve
"""

import sys
import json

try:
    import textattack
    from textattack.attack_recipes import AttackRecipe
    from textattack.datasets import HuggingFaceDataset
    from textattack.models.wrappers import HuggingFaceModelWrapper
    from textattack import Attacker
    from textattack.attack_results import SuccessfulAttackResult, FailedAttackResult, SkippedAttackResult
except ImportError as e:
    print(f"TEXTATTACK_ERROR: Missing dependency: {{e}}")
    sys.exit(1)

def load_attack_recipe(recipe_name: str):
    """Load an attack recipe by name."""
    recipes = {{
        "textfooler": "textattack.attack_recipes.TextFoolerJin2019",
        "bert-attack": "textattack.attack_recipes.BERTAttackLi2020",
        "bae": "textattack.attack_recipes.BAEGarg2019",
        "deepwordbug": "textattack.attack_recipes.DeepWordBugGao2018",
        "textbugger": "textattack.attack_recipes.TextBuggerLi2018",
        "pwws": "textattack.attack_recipes.PWWSRen2019",
        "checklist": "textattack.attack_recipes.CheckList2020",
        "a2t": "textattack.attack_recipes.A2TYoo2021",
        "clare": "textattack.attack_recipes.CLARE2020",
    }}

    if recipe_name not in recipes:
        print(f"TEXTATTACK_ERROR: Unknown recipe: {{recipe_name}}")
        return None

    module_path = recipes[recipe_name]
    module_name, class_name = module_path.rsplit(".", 1)

    try:
        import importlib
        module = importlib.import_module(module_name)
        recipe_class = getattr(module, class_name)
        return recipe_class
    except Exception as e:
        print(f"TEXTATTACK_ERROR: Failed to load recipe {{recipe_name}}: {{e}}")
        return None

def main():
    recipe_name = "{recipe}"
    model_name = "{model_name}"
    dataset_name = "{dataset_name}"
    num_examples = {num_examples}
    max_percent = {max_percent}
    min_similarity = {min_similarity}

    print(f"TEXTATTACK_INFO: Loading model {{model_name}}")

    try:
        # Load model
        from transformers import AutoModelForSequenceClassification, AutoTokenizer
        model = AutoModelForSequenceClassification.from_pretrained(model_name)
        tokenizer = AutoTokenizer.from_pretrained(model_name)
        model_wrapper = HuggingFaceModelWrapper(model, tokenizer)
    except Exception as e:
        print(f"TEXTATTACK_ERROR: Failed to load model: {{e}}")
        sys.exit(1)

    print(f"TEXTATTACK_INFO: Loading dataset {{dataset_name}}")

    try:
        # Load dataset
        dataset = HuggingFaceDataset(dataset_name, split="test")
        if len(dataset) > num_examples:
            dataset = dataset[:num_examples]
    except Exception as e:
        print(f"TEXTATTACK_ERROR: Failed to load dataset: {{e}}")
        sys.exit(1)

    print(f"TEXTATTACK_INFO: Building attack recipe {{recipe_name}}")

    # Load attack recipe
    recipe_class = load_attack_recipe(recipe_name)
    if recipe_class is None:
        sys.exit(1)

    try:
        attack = recipe_class.build(model_wrapper)
    except Exception as e:
        print(f"TEXTATTACK_ERROR: Failed to build attack: {{e}}")
        sys.exit(1)

    print(f"TEXTATTACK_INFO: Running attack on {{num_examples}} examples")

    # Run attack
    attacker = Attacker(attack, dataset)
    attack_results = attacker.attack_dataset()

    # Count results
    successful_attacks = 0
    failed_attacks = 0
    skipped_attacks = 0
    total_queries = 0
    adversarial_examples = []

    for result in attack_results:
        if isinstance(result, SuccessfulAttackResult):
            successful_attacks += 1
            total_queries += result.num_queries
            # Store first few adversarial examples
            if len(adversarial_examples) < 5:
                adversarial_examples.append({{
                    "original_text": result.original_text(),
                    "perturbed_text": result.perturbed_text(),
                    "original_output": int(result.original_result.output),
                    "perturbed_output": int(result.perturbed_result.output),
                    "num_queries": result.num_queries,
                    "num_words_changed": result.original_result.attacked_text.words_diff_ratio(result.perturbed_result.attacked_text) if hasattr(result.original_result, 'attacked_text') else 0.0,
                }})
        elif isinstance(result, FailedAttackResult):
            failed_attacks += 1
            total_queries += result.num_queries
        elif isinstance(result, SkippedAttackResult):
            skipped_attacks += 1

    # Calculate metrics
    total_attacked = successful_attacks + failed_attacks
    attack_success_rate = successful_attacks / max(total_attacked, 1)
    robust_accuracy = failed_attacks / max(total_attacked, 1)
    avg_queries = total_queries / max(total_attacked, 1)

    # Output results in parseable format
    print("TEXTATTACK_RESULT_START")
    result = {{
        "total_examples": num_examples,
        "successful_attacks": successful_attacks,
        "failed_attacks": failed_attacks,
        "skipped_attacks": skipped_attacks,
        "attack_success_rate": float(attack_success_rate),
        "robust_accuracy": float(robust_accuracy),
        "average_queries": float(avg_queries),
        "attack_recipe": recipe_name,
        "model_name": model_name,
        "dataset_name": dataset_name,
    }}

    # Add adversarial examples
    if adversarial_examples:
        result["adversarial_examples"] = adversarial_examples

    print(json.dumps(result, indent=2))
    print("TEXTATTACK_RESULT_END")

    # Print summary
    print(f"\\nTEXTATTACK_SUMMARY: Attack success rate: {{attack_success_rate:.2%}}")
    print(f"TEXTATTACK_SUMMARY: Robust accuracy: {{robust_accuracy:.2%}}")
    print(f"TEXTATTACK_SUMMARY: Successful: {{successful_attacks}}, Failed: {{failed_attacks}}, Skipped: {{skipped_attacks}}")

    # Determine if model is robust
    if attack_success_rate < 0.05:
        print("TEXTATTACK_STATUS: ROBUST")
    elif attack_success_rate < 0.20:
        print("TEXTATTACK_STATUS: PARTIALLY_ROBUST")
    else:
        print("TEXTATTACK_STATUS: NOT_ROBUST")

if __name__ == "__main__":
    main()
"#
    ))
}

/// Extract model name from USL spec
fn extract_model_from_spec(spec: &TypedSpec) -> Option<String> {
    // Look for model name in property annotations or type fields
    for prop in &spec.spec.properties {
        let name = match prop {
            Property::Invariant(inv) => &inv.name,
            Property::Theorem(thm) => &thm.name,
            Property::Security(sec) => &sec.name,
            _ => continue,
        };

        // Check if property name contains model reference
        if name.to_lowercase().contains("bert")
            || name.to_lowercase().contains("roberta")
            || name.to_lowercase().contains("gpt")
        {
            return Some(name.clone());
        }
    }
    None
}

/// Extract dataset name from USL spec
fn extract_dataset_from_spec(spec: &TypedSpec) -> Option<String> {
    // Look for dataset name in type definitions
    for typedef in &spec.spec.types {
        let lower = typedef.name.to_lowercase();
        if lower.contains("dataset") || lower.contains("corpus") {
            return Some(typedef.name.clone());
        }
    }
    None
}

/// Parse TextAttack script output
pub fn parse_textattack_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    // Check for errors
    if stdout.contains("TEXTATTACK_ERROR:") || stderr.contains("TEXTATTACK_ERROR:") {
        let error_msg = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("TEXTATTACK_ERROR:"))
            .map(|l| l.replace("TEXTATTACK_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown TextAttack error".to_string());

        return (VerificationStatus::Unknown { reason: error_msg }, None);
    }

    // Parse JSON result
    if let Some(json_str) = extract_json_result(stdout) {
        if let Ok(result) = serde_json::from_str::<serde_json::Value>(&json_str) {
            let attack_success_rate = result["attack_success_rate"].as_f64().unwrap_or(1.0);
            let robust_accuracy = result["robust_accuracy"].as_f64().unwrap_or(0.0);

            // Build counterexample if adversarial examples were found
            let counterexample =
                crate::counterexample::build_text_attack_counterexample(&result, "TextAttack");

            // Determine status based on attack success rate
            if attack_success_rate < 0.05 {
                return (VerificationStatus::Proven, None);
            } else if attack_success_rate < 0.20 {
                return (
                    VerificationStatus::Partial {
                        verified_percentage: robust_accuracy * 100.0,
                    },
                    counterexample,
                );
            } else {
                return (VerificationStatus::Disproven, counterexample);
            }
        }
    }

    // Fallback: parse status line
    if stdout.contains("TEXTATTACK_STATUS: ROBUST") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("TEXTATTACK_STATUS: PARTIALLY_ROBUST") {
        (
            VerificationStatus::Partial {
                verified_percentage: 80.0,
            },
            None,
        )
    } else if stdout.contains("TEXTATTACK_STATUS: NOT_ROBUST") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse TextAttack output".to_string(),
            },
            None,
        )
    }
}

/// Extract JSON result from output
fn extract_json_result(output: &str) -> Option<String> {
    let start_marker = "TEXTATTACK_RESULT_START";
    let end_marker = "TEXTATTACK_RESULT_END";

    if let Some(start) = output.find(start_marker) {
        let after_start = &output[start + start_marker.len()..];
        if let Some(end) = after_start.find(end_marker) {
            return Some(after_start[..end].trim().to_string());
        }
    }
    None
}
