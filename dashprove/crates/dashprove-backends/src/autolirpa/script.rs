//! AutoLiRPA script generation and output parsing

use super::config::AutoLirpaConfig;
use crate::counterexample::StructuredCounterexample;
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::ast::{ComparisonOp, Expr, Property};
use dashprove_usl::typecheck::TypedSpec;

/// Generate an AutoLiRPA verification script
pub fn generate_autolirpa_script(
    spec: &TypedSpec,
    config: &AutoLirpaConfig,
) -> Result<String, BackendError> {
    let epsilon = extract_epsilon_from_spec(spec).unwrap_or(config.epsilon);
    let model_path = config
        .model_path
        .as_ref()
        .map(|p| p.display().to_string())
        .unwrap_or_else(|| "model.pt".to_string());
    let bound_method = config.bound_method.as_str();
    let use_gpu = if config.use_gpu { "True" } else { "False" };
    let opt_iterations = config.opt_iterations;

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
Auto-LiRPA neural network bound propagation script
Generated by DashProve

Auto-LiRPA computes certified bounds using linear relaxation.
"""

import sys
import json
import numpy as np

try:
    import torch
    import torch.nn as nn
    from auto_LiRPA import BoundedModule, BoundedTensor
    from auto_LiRPA.perturbations import PerturbationLpNorm
except ImportError as e:
    print(f"AUTOLIRPA_ERROR: Missing dependency: {{e}}")
    print("AUTOLIRPA_ERROR: Install with: pip install auto_lirpa")
    sys.exit(1)

def load_network(model_path: str):
    """Load PyTorch model for Auto-LiRPA."""
    try:
        model = torch.load(model_path, map_location='cpu')
        model.eval()
        return model
    except Exception as e:
        print(f"AUTOLIRPA_ERROR: Failed to load network: {{e}}")
        return None

def create_test_network():
    """Create a simple test network."""
    model = nn.Sequential(
        nn.Linear(2, 4),
        nn.ReLU(),
        nn.Linear(4, 2)
    )
    model.eval()
    return model

def main():
    model_path = "{model_path}"
    epsilon = {epsilon}
    bound_method = "{bound_method}"
    use_gpu = {use_gpu}
    opt_iterations = {opt_iterations}

    device = torch.device('cuda' if use_gpu and torch.cuda.is_available() else 'cpu')

    model = load_network(model_path)
    if model is None:
        print("AUTOLIRPA_INFO: Using synthetic test network")
        model = create_test_network()
        input_dim = 2
        output_dim = 2
    else:
        # Infer dimensions from first layer
        input_dim = 2
        output_dim = 2

    model = model.to(device)

    np.random.seed(42)
    num_samples = 10
    test_inputs = np.random.rand(num_samples, input_dim).astype(np.float32)

    verified_count = 0
    total_count = num_samples
    counterexamples = []

    for i, x in enumerate(test_inputs):
        try:
            x_tensor = torch.tensor(x, device=device).unsqueeze(0)

            # Get clean prediction
            with torch.no_grad():
                output = model(x_tensor)
                true_label = output.argmax().item()

            # Create bounded module
            bounded_model = BoundedModule(model, x_tensor)

            # Create perturbation
            ptb = PerturbationLpNorm(norm=float('inf'), eps=epsilon)
            bounded_x = BoundedTensor(x_tensor, ptb)

            # Compute bounds based on method
            if bound_method == "IBP":
                lb, ub = bounded_model.compute_bounds(x=(bounded_x,), method='IBP')
            elif bound_method == "CROWN":
                lb, ub = bounded_model.compute_bounds(x=(bounded_x,), method='backward')
            elif bound_method == "IBP+CROWN":
                lb, ub = bounded_model.compute_bounds(x=(bounded_x,), method='IBP+backward')
            elif bound_method == "Forward+CROWN":
                lb, ub = bounded_model.compute_bounds(x=(bounded_x,), method='forward+backward')
            elif bound_method == "alpha-CROWN":
                lb, ub = bounded_model.compute_bounds(
                    x=(bounded_x,),
                    method='alpha-crown',
                    return_A=False,
                    bound_upper=True
                )
            else:
                lb, ub = bounded_model.compute_bounds(x=(bounded_x,), method='IBP')

            # Check if classification is preserved
            # For robustness: lb[true_label] > ub[other_labels]
            lb_np = lb.detach().cpu().numpy()[0]
            ub_np = ub.detach().cpu().numpy()[0]

            is_verified = True
            for j in range(output_dim):
                if j != true_label:
                    if lb_np[true_label] <= ub_np[j]:
                        is_verified = False
                        break

            if is_verified:
                verified_count += 1
            else:
                # Generate counterexample hint
                counterexamples.append({{
                    "sample_index": i,
                    "original_input": x.tolist(),
                    "true_label": int(true_label),
                    "lower_bounds": lb_np.tolist(),
                    "upper_bounds": ub_np.tolist()
                }})
        except Exception as e:
            print(f"AUTOLIRPA_INFO: Sample {{i}} error: {{e}}")

    verification_rate = verified_count / total_count if total_count > 0 else 0.0

    print("AUTOLIRPA_RESULT_START")
    result = {{
        "verified_count": verified_count,
        "total_count": total_count,
        "verification_rate": float(verification_rate),
        "epsilon": float(epsilon),
        "bound_method": bound_method,
        "input_dim": int(input_dim),
        "output_dim": int(output_dim),
        "num_counterexamples": len(counterexamples)
    }}

    if counterexamples:
        result["counterexample"] = counterexamples[0]

    print(json.dumps(result, indent=2))
    print("AUTOLIRPA_RESULT_END")

    print(f"\\nAUTOLIRPA_SUMMARY: Verified {{verified_count}}/{{total_count}} ({{verification_rate:.2%}})")
    print(f"AUTOLIRPA_SUMMARY: Method: {{bound_method}}, Epsilon: {{epsilon}}")

    if verification_rate >= 0.99:
        print("AUTOLIRPA_STATUS: VERIFIED")
    elif verification_rate >= 0.90:
        print("AUTOLIRPA_STATUS: PARTIALLY_VERIFIED")
    else:
        print("AUTOLIRPA_STATUS: NOT_VERIFIED")

if __name__ == "__main__":
    main()
"#
    ))
}

fn extract_epsilon_from_spec(spec: &TypedSpec) -> Option<f64> {
    for prop in &spec.spec.properties {
        let expr = match prop {
            Property::Invariant(inv) => Some(&inv.body),
            Property::Theorem(thm) => Some(&thm.body),
            _ => None,
        };
        if let Some(e) = expr {
            if let Some(eps) = extract_epsilon(e) {
                return Some(eps);
            }
        }
    }
    None
}

fn extract_epsilon(expr: &Expr) -> Option<f64> {
    match expr {
        Expr::Compare(lhs, op, rhs) => {
            if let Expr::Var(name) = lhs.as_ref() {
                let lower = name.to_lowercase();
                if (lower.contains("epsilon") || lower == "eps")
                    && matches!(op, ComparisonOp::Le | ComparisonOp::Lt)
                {
                    return extract_numeric_value(rhs);
                }
            }
            extract_epsilon(lhs).or_else(|| extract_epsilon(rhs))
        }
        Expr::And(lhs, rhs) | Expr::Or(lhs, rhs) => {
            extract_epsilon(lhs).or_else(|| extract_epsilon(rhs))
        }
        _ => None,
    }
}

fn extract_numeric_value(expr: &Expr) -> Option<f64> {
    match expr {
        Expr::Float(f) => Some(*f),
        Expr::Int(i) => Some(*i as f64),
        _ => None,
    }
}

/// Parse AutoLiRPA output
pub fn parse_autolirpa_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("AUTOLIRPA_ERROR:") || stderr.contains("AUTOLIRPA_ERROR:") {
        let error_msg = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("AUTOLIRPA_ERROR:"))
            .map(|l| l.replace("AUTOLIRPA_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown Auto-LiRPA error".to_string());
        return (VerificationStatus::Unknown { reason: error_msg }, None);
    }

    if let Some(json_str) =
        extract_json_result(stdout, "AUTOLIRPA_RESULT_START", "AUTOLIRPA_RESULT_END")
    {
        if let Ok(result) = serde_json::from_str::<serde_json::Value>(&json_str) {
            let verification_rate = result["verification_rate"].as_f64().unwrap_or(0.0);

            let counterexample =
                crate::counterexample::build_nn_verification_counterexample(&result, "Auto-LiRPA");

            if verification_rate >= 0.99 {
                return (VerificationStatus::Proven, None);
            } else if verification_rate >= 0.90 {
                return (
                    VerificationStatus::Partial {
                        verified_percentage: verification_rate * 100.0,
                    },
                    counterexample,
                );
            } else {
                return (VerificationStatus::Disproven, counterexample);
            }
        }
    }

    if stdout.contains("AUTOLIRPA_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("AUTOLIRPA_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 90.0,
            },
            None,
        )
    } else if stdout.contains("AUTOLIRPA_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse Auto-LiRPA output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after_start = &output[start + start_marker.len()..];
        if let Some(end) = after_start.find(end_marker) {
            return Some(after_start[..end].trim().to_string());
        }
    }
    None
}
