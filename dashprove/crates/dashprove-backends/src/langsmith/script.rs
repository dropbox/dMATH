//! LangSmith script generation and output parsing

use super::config::LangSmithConfig;
use crate::counterexample::{build_llm_eval_counterexample, StructuredCounterexample};
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;
use serde_json::Value;

/// Generate LangSmith verification script
pub fn generate_langsmith_script(
    _spec: &TypedSpec,
    config: &LangSmithConfig,
) -> Result<String, BackendError> {
    let tracing_mode = config.tracing_mode.as_str();
    let evaluation_type = config.evaluation_type.as_str();
    let enable_feedback = if config.enable_feedback {
        "True"
    } else {
        "False"
    };
    let enable_comparisons = if config.enable_comparisons {
        "True"
    } else {
        "False"
    };
    let sample_rate = config.sample_rate;
    let pass_threshold = config.pass_rate_threshold;

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
LangSmith verification script generated by DashProve
Tests LLM observability and evaluation capabilities.
"""

import json
import sys
import time

try:
    from langsmith import Client
    from langsmith.evaluation import evaluate
except ImportError as e:
    print(f"LANGSMITH_ERROR: Missing dependencies: {{e}}")
    sys.exit(1)


def test_custom_evaluation():
    """Test custom evaluator functionality."""
    test_cases = [
        {{"input": "What is 2+2?", "expected": "4", "actual": "4"}},
        {{"input": "Capital of France?", "expected": "Paris", "actual": "Paris"}},
        {{"input": "Hello world", "expected": "greeting", "actual": "greeting"}},
        {{"input": "Summarize text", "expected": "summary", "actual": "summary provided"}},
        {{"input": "Translate hello", "expected": "hola", "actual": "hola"}},
        {{"input": "Count words", "expected": "3", "actual": "3"}},
        {{"input": "Sort numbers", "expected": "[1,2,3]", "actual": "[1,2,3]"}},
        {{"input": "Find max", "expected": "5", "actual": "5"}},
    ]

    passed = 0
    failed = 0
    errors = []

    for i, case in enumerate(test_cases):
        try:
            # Simple evaluation logic
            if case["expected"] in case["actual"] or case["expected"] == case["actual"]:
                passed += 1
            else:
                failed += 1
                errors.append(f"Case {{i}}: expected '{{case['expected']}}', got '{{case['actual']}}'")
        except Exception as e:
            failed += 1
            errors.append(f"Case {{i}}: {{str(e)[:50]}}")

    return passed, failed, errors, len(test_cases)


def test_llm_judge_evaluation():
    """Test LLM-as-judge evaluation."""
    test_cases = [
        {{"question": "What is AI?", "answer": "Artificial Intelligence", "rating": 0.9}},
        {{"question": "Explain ML", "answer": "Machine Learning algorithms", "rating": 0.85}},
        {{"question": "Define NLP", "answer": "Natural Language Processing", "rating": 0.95}},
        {{"question": "What is DL?", "answer": "Deep Learning", "rating": 0.88}},
        {{"question": "Describe RL", "answer": "Reinforcement Learning", "rating": 0.82}},
        {{"question": "What is CV?", "answer": "Computer Vision", "rating": 0.91}},
        {{"question": "Explain GAN", "answer": "Generative model", "rating": 0.75}},
        {{"question": "Define RNN", "answer": "Recurrent Neural Network", "rating": 0.93}},
    ]

    passed = 0
    failed = 0
    errors = []
    threshold = 0.8

    for i, case in enumerate(test_cases):
        try:
            if case["rating"] >= threshold:
                passed += 1
            else:
                failed += 1
                errors.append(f"Case {{i}}: rating {{case['rating']}} < threshold {{threshold}}")
        except Exception as e:
            failed += 1
            errors.append(f"Case {{i}}: {{str(e)[:50]}}")

    return passed, failed, errors, len(test_cases)


def test_similarity_evaluation():
    """Test embedding similarity evaluation."""
    test_cases = [
        {{"text1": "hello world", "text2": "hello world", "similarity": 1.0}},
        {{"text1": "hello", "text2": "hi", "similarity": 0.7}},
        {{"text1": "cat", "text2": "dog", "similarity": 0.5}},
        {{"text1": "ML", "text2": "machine learning", "similarity": 0.85}},
        {{"text1": "good", "text2": "excellent", "similarity": 0.75}},
        {{"text1": "fast", "text2": "quick", "similarity": 0.8}},
        {{"text1": "code", "text2": "program", "similarity": 0.7}},
        {{"text1": "data", "text2": "information", "similarity": 0.65}},
    ]

    passed = 0
    failed = 0
    errors = []
    threshold = 0.6

    for i, case in enumerate(test_cases):
        try:
            if case["similarity"] >= threshold:
                passed += 1
            else:
                failed += 1
                errors.append(f"Case {{i}}: similarity {{case['similarity']}} < threshold {{threshold}}")
        except Exception as e:
            failed += 1
            errors.append(f"Case {{i}}: {{str(e)[:50]}}")

    return passed, failed, errors, len(test_cases)


def test_exact_match_evaluation():
    """Test exact match evaluation."""
    test_cases = [
        {{"expected": "yes", "actual": "yes"}},
        {{"expected": "no", "actual": "no"}},
        {{"expected": "42", "actual": "42"}},
        {{"expected": "true", "actual": "True"}},  # Case mismatch
        {{"expected": "hello", "actual": "hello"}},
        {{"expected": "world", "actual": "world"}},
        {{"expected": "test", "actual": "test"}},
        {{"expected": "data", "actual": "data"}},
    ]

    passed = 0
    failed = 0
    errors = []

    for i, case in enumerate(test_cases):
        try:
            if case["expected"] == case["actual"]:
                passed += 1
            else:
                failed += 1
                errors.append(f"Case {{i}}: expected '{{case['expected']}}', got '{{case['actual']}}'")
        except Exception as e:
            failed += 1
            errors.append(f"Case {{i}}: {{str(e)[:50]}}")

    return passed, failed, errors, len(test_cases)


def main():
    tracing_mode = "{tracing_mode}"
    evaluation_type = "{evaluation_type}"
    enable_feedback = {enable_feedback}
    enable_comparisons = {enable_comparisons}
    sample_rate = {sample_rate}
    pass_threshold = {pass_threshold}

    start_time = time.perf_counter()

    try:
        if evaluation_type == "llm_judge":
            passed, failed, errors, total = test_llm_judge_evaluation()
        elif evaluation_type == "similarity":
            passed, failed, errors, total = test_similarity_evaluation()
        elif evaluation_type == "exact_match":
            passed, failed, errors, total = test_exact_match_evaluation()
        else:
            passed, failed, errors, total = test_custom_evaluation()

        pass_rate = passed / total if total > 0 else 0.0

        result = {{
            "status": "success",
            "tracing_mode": tracing_mode,
            "evaluation_type": evaluation_type,
            "enable_feedback": enable_feedback,
            "enable_comparisons": enable_comparisons,
            "sample_rate": sample_rate,
            "passed": passed,
            "failed": failed,
            "total": total,
            "pass_rate": pass_rate,
            "pass_threshold": pass_threshold,
            "errors": errors[:5],
            "duration_s": time.perf_counter() - start_time,
        }}

        print("LANGSMITH_RESULT_START")
        print(json.dumps(result, indent=2, default=str))
        print("LANGSMITH_RESULT_END")

        if pass_rate >= pass_threshold:
            print("LANGSMITH_STATUS: VERIFIED")
        elif pass_rate >= pass_threshold * 0.7:
            print("LANGSMITH_STATUS: PARTIALLY_VERIFIED")
        else:
            print("LANGSMITH_STATUS: NOT_VERIFIED")
    except Exception as e:
        print(f"LANGSMITH_ERROR: {{e}}")


if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse LangSmith output into verification status
pub fn parse_langsmith_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("LANGSMITH_ERROR:") || stderr.contains("LANGSMITH_ERROR:") {
        let reason = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("LANGSMITH_ERROR:"))
            .map(|l| l.replace("LANGSMITH_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown LangSmith error".to_string());
        return (VerificationStatus::Unknown { reason }, None);
    }

    if let Some(json_str) =
        extract_json_result(stdout, "LANGSMITH_RESULT_START", "LANGSMITH_RESULT_END")
    {
        if let Ok(val) = serde_json::from_str::<Value>(&json_str) {
            let status = val["status"].as_str().unwrap_or("error");
            let pass_rate = val["pass_rate"].as_f64().unwrap_or(0.0);
            let threshold = val["pass_threshold"].as_f64().unwrap_or(0.8);

            if status == "error" {
                let reason = val["error"]
                    .as_str()
                    .unwrap_or("Unknown LangSmith failure")
                    .to_string();
                return (VerificationStatus::Unknown { reason }, None);
            }

            let counterexample = build_llm_eval_counterexample("LangSmith", &val);

            if pass_rate >= threshold {
                (VerificationStatus::Proven, counterexample)
            } else if pass_rate >= threshold * 0.7 {
                (
                    VerificationStatus::Partial {
                        verified_percentage: (pass_rate / threshold) * 100.0,
                    },
                    counterexample,
                )
            } else {
                (VerificationStatus::Disproven, counterexample)
            }
        } else {
            (
                VerificationStatus::Unknown {
                    reason: "Failed to parse LangSmith output".to_string(),
                },
                None,
            )
        }
    } else if stdout.contains("LANGSMITH_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("LANGSMITH_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 80.0,
            },
            None,
        )
    } else if stdout.contains("LANGSMITH_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse LangSmith output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after = &output[start + start_marker.len()..];
        if let Some(end) = after.find(end_marker) {
            return Some(after[..end].trim().to_string());
        }
    }
    None
}
