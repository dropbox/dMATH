//! AIMET script generation and output parsing

use super::config::AimetConfig;
use crate::counterexample::StructuredCounterexample;
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;

/// Generate an AIMET quantization verification script
pub fn generate_aimet_script(
    _spec: &TypedSpec,
    config: &AimetConfig,
) -> Result<String, BackendError> {
    let quant_scheme = config.quant_scheme.as_str();
    let bit_width = config.bit_width.as_str();
    let weight_bits = config.bit_width.weight_bits();
    let activation_bits = config.bit_width.activation_bits();
    let rounding_mode = config.rounding_mode.as_str();
    let compression_mode = config.compression_mode.as_str();
    let num_batches = config.num_batches;
    let per_channel = if config.per_channel { "True" } else { "False" };

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
AIMET quantization verification script
Generated by DashProve

Validates model quantization correctness with AIMET.
"""

import sys
import json
import time
import numpy as np

try:
    import aimet_common
    from aimet_common.defs import QuantScheme
except ImportError as e:
    print(f"AIMET_ERROR: Missing AIMET: {{e}}")
    print("AIMET_ERROR: Install from: https://github.com/quic/aimet")
    sys.exit(1)

# Require PyTorch
try:
    import torch
    import torch.nn as nn
    from aimet_torch.quantsim import QuantizationSimModel
    from aimet_torch.cross_layer_equalization import equalize_model
    HAS_TORCH = True
except ImportError as e:
    print(f"AIMET_ERROR: PyTorch and AIMET-torch required: {{e}}")
    sys.exit(1)


class SimpleModel(nn.Module):
    """Simple test model for quantization."""
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(16)
        self.relu1 = nn.ReLU()
        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(32)
        self.relu2 = nn.ReLU()
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(32, 10)

    def forward(self, x):
        x = self.relu1(self.bn1(self.conv1(x)))
        x = self.relu2(self.bn2(self.conv2(x)))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x


def create_calibration_data(num_batches, batch_size=4):
    """Create calibration data generator."""
    np.random.seed(42)
    for _ in range(num_batches):
        yield torch.randn(batch_size, 3, 32, 32)


def calibration_callback(model, args):
    """Callback for calibration during quantization."""
    data_loader = args
    model.eval()
    with torch.no_grad():
        for batch in data_loader:
            _ = model(batch)


def main():
    quant_scheme = "{quant_scheme}"
    bit_width = "{bit_width}"
    weight_bits = {weight_bits}
    activation_bits = {activation_bits}
    rounding_mode = "{rounding_mode}"
    compression_mode = "{compression_mode}"
    num_batches = {num_batches}
    per_channel = {per_channel}

    # Create test model
    model = SimpleModel()
    model.eval()

    # Get original model size
    original_size = sum(p.numel() * p.element_size() for p in model.parameters())
    total_params = sum(p.numel() for p in model.parameters())

    # Generate test input
    np.random.seed(42)
    test_input = torch.randn(1, 3, 32, 32)

    # Get original output
    with torch.no_grad():
        original_output = model(test_input).numpy()

    # Create calibration data
    calib_data = list(create_calibration_data(num_batches))

    try:
        start_time = time.perf_counter()

        # Apply cross-layer equalization if requested
        if quant_scheme == "cle":
            model = equalize_model(model, (1, 3, 32, 32))

        # Select quantization scheme
        if quant_scheme in ["post_training", "cle"]:
            aimet_quant_scheme = QuantScheme.post_training_tf_enhanced
        elif quant_scheme == "adaround":
            aimet_quant_scheme = QuantScheme.post_training_tf_enhanced
        else:
            aimet_quant_scheme = QuantScheme.training_range_learning_with_tf_init

        # Create quantization simulation model
        dummy_input = torch.randn(1, 3, 32, 32)
        sim = QuantizationSimModel(
            model,
            dummy_input=dummy_input,
            quant_scheme=aimet_quant_scheme,
            default_param_bw=weight_bits,
            default_output_bw=activation_bits,
        )

        # Compute encodings (calibration)
        def forward_pass(m, args):
            data = args
            m.eval()
            with torch.no_grad():
                for batch in data:
                    _ = m(batch)

        sim.compute_encodings(forward_pass, calib_data)

        quantization_time = time.perf_counter() - start_time

        # Get quantized output
        sim.model.eval()
        with torch.no_grad():
            quant_output = sim.model(test_input).numpy()

        # Calculate output difference
        output_diff = np.max(np.abs(original_output - quant_output))
        output_mse = np.mean((original_output - quant_output) ** 2)

        # Estimate compression ratio
        avg_bits = (weight_bits + activation_bits) / 2.0
        compression_ratio = 32.0 / avg_bits

        status = "success"
        error_message = None

    except Exception as e:
        status = "error"
        error_message = str(e)
        quantization_time = 0.0
        output_diff = float('inf')
        output_mse = float('inf')
        compression_ratio = 1.0
        sim = None

    # Benchmark if successful
    if status == "success" and sim is not None:
        latencies = []
        for _ in range(100):
            start = time.perf_counter()
            with torch.no_grad():
                _ = sim.model(test_input)
            latencies.append((time.perf_counter() - start) * 1000)

        latencies = np.array(latencies)
        mean_lat = float(np.mean(latencies))
        std_lat = float(np.std(latencies))
        p50_lat = float(np.percentile(latencies, 50))
        p95_lat = float(np.percentile(latencies, 95))
    else:
        mean_lat = std_lat = p50_lat = p95_lat = 0.0

    print("AIMET_RESULT_START")
    result = {{
        "status": status,
        "quant_scheme": quant_scheme,
        "bit_width": bit_width,
        "weight_bits": weight_bits,
        "activation_bits": activation_bits,
        "rounding_mode": rounding_mode,
        "compression_mode": compression_mode,
        "num_batches": num_batches,
        "per_channel": per_channel,
        "original_size_bytes": original_size,
        "total_params": total_params,
        "compression_ratio": compression_ratio,
        "output_max_diff": float(output_diff) if status == "success" else None,
        "output_mse": float(output_mse) if status == "success" else None,
        "quantization_time_s": quantization_time,
        "latency_ms": {{
            "mean": mean_lat,
            "std": std_lat,
            "p50": p50_lat,
            "p95": p95_lat
        }},
        "aimet_version": aimet_common.__version__,
        "error": error_message
    }}
    print(json.dumps(result, indent=2))
    print("AIMET_RESULT_END")

    if status == "success":
        print(f"\\nAIMET_SUMMARY: Compression ratio: {{compression_ratio:.1f}}x")
        print(f"AIMET_SUMMARY: Output max diff: {{output_diff:.6f}}")
        print(f"AIMET_SUMMARY: Output MSE: {{output_mse:.6f}}")
        print(f"AIMET_SUMMARY: Mean latency: {{mean_lat:.3f}}ms")

        if output_diff < 0.1 and output_mse < 0.01:
            print("AIMET_STATUS: VERIFIED")
        elif output_diff < 0.5:
            print("AIMET_STATUS: PARTIALLY_VERIFIED")
        else:
            print("AIMET_STATUS: NOT_VERIFIED")
    else:
        print(f"AIMET_ERROR: {{error_message}}")


if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse AIMET output
pub fn parse_aimet_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("AIMET_ERROR:") || stderr.contains("AIMET_ERROR:") {
        let error_msg = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("AIMET_ERROR:"))
            .map(|l| l.replace("AIMET_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown AIMET error".to_string());
        return (VerificationStatus::Unknown { reason: error_msg }, None);
    }

    if let Some(json_str) = extract_json_result(stdout, "AIMET_RESULT_START", "AIMET_RESULT_END") {
        if let Ok(result) = serde_json::from_str::<serde_json::Value>(&json_str) {
            let status = result["status"].as_str().unwrap_or("error");
            let output_diff = result["output_max_diff"].as_f64().unwrap_or(1.0);
            let output_mse = result["output_mse"].as_f64().unwrap_or(1.0);

            if status == "error" {
                let error = result["error"]
                    .as_str()
                    .unwrap_or("Unknown error")
                    .to_string();
                return (VerificationStatus::Unknown { reason: error }, None);
            }

            let counterexample =
                crate::counterexample::build_quantization_counterexample(&result, "AIMET");

            if output_diff < 0.1 && output_mse < 0.01 {
                return (VerificationStatus::Proven, None);
            } else if output_diff < 0.5 {
                return (
                    VerificationStatus::Partial {
                        verified_percentage: 90.0,
                    },
                    counterexample,
                );
            } else {
                return (VerificationStatus::Disproven, counterexample);
            }
        }
    }

    if stdout.contains("AIMET_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("AIMET_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 90.0,
            },
            None,
        )
    } else if stdout.contains("AIMET_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse AIMET output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after_start = &output[start + start_marker.len()..];
        if let Some(end) = after_start.find(end_marker) {
            return Some(after_start[..end].trim().to_string());
        }
    }
    None
}
