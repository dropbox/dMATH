//! Ragas script generation and output parsing

use super::config::RagasConfig;
use crate::counterexample::{build_llm_eval_counterexample, StructuredCounterexample};
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;
use serde_json::Value;

/// Generate Ragas verification script
pub fn generate_ragas_script(
    _spec: &TypedSpec,
    config: &RagasConfig,
) -> Result<String, BackendError> {
    let metric = config.metric.as_str();
    let mode = config.mode.as_str();
    let context_metrics = if config.context_metrics {
        "True"
    } else {
        "False"
    };
    let answer_metrics = if config.answer_metrics {
        "True"
    } else {
        "False"
    };
    let batch_size = config.batch_size;
    let pass_threshold = config.pass_rate_threshold;

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
Ragas verification script generated by DashProve
Tests RAG pipeline evaluation metrics.
"""

import json
import sys
import time

try:
    from ragas import evaluate
    from ragas.metrics import faithfulness, answer_relevancy, context_precision
except ImportError as e:
    print(f"RAGAS_ERROR: Missing dependencies: {{e}}")
    sys.exit(1)


def test_faithfulness():
    """Test faithfulness metric - factual consistency with context."""
    test_cases = [
        {{"context": "Python is a programming language.", "answer": "Python is a programming language", "score": 0.95}},
        {{"context": "The sky is blue.", "answer": "The sky is blue", "score": 0.98}},
        {{"context": "2+2=4", "answer": "2+2 equals 4", "score": 0.9}},
        {{"context": "Paris is in France.", "answer": "Paris is the capital of France", "score": 0.7}},
        {{"context": "Water boils at 100C", "answer": "Water boils at 100 degrees Celsius", "score": 0.92}},
        {{"context": "Cats are mammals.", "answer": "Cats are animals", "score": 0.8}},
        {{"context": "JavaScript runs in browsers.", "answer": "JavaScript is a browser language", "score": 0.85}},
        {{"context": "ML uses data.", "answer": "Machine learning uses data to learn", "score": 0.88}},
    ]

    passed = 0
    failed = 0
    errors = []
    threshold = 0.75

    for i, case in enumerate(test_cases):
        try:
            if case["score"] >= threshold:
                passed += 1
            else:
                failed += 1
                errors.append(f"Case {{i}}: faithfulness score {{case['score']}} < threshold {{threshold}}")
        except Exception as e:
            failed += 1
            errors.append(f"Case {{i}}: {{str(e)[:50]}}")

    return passed, failed, errors, len(test_cases)


def test_answer_relevancy():
    """Test answer relevancy metric."""
    test_cases = [
        {{"question": "What is Python?", "answer": "Python is a programming language", "score": 0.9}},
        {{"question": "What color is sky?", "answer": "The sky is blue", "score": 0.95}},
        {{"question": "What is 2+2?", "answer": "The answer is 4", "score": 0.85}},
        {{"question": "Where is Paris?", "answer": "Paris is in France", "score": 0.92}},
        {{"question": "When does water boil?", "answer": "At 100 degrees Celsius", "score": 0.88}},
        {{"question": "Are cats mammals?", "answer": "Yes, cats are mammals", "score": 0.93}},
        {{"question": "What is ML?", "answer": "Machine learning is AI", "score": 0.8}},
        {{"question": "What is NLP?", "answer": "Natural language processing", "score": 0.91}},
    ]

    passed = 0
    failed = 0
    errors = []
    threshold = 0.8

    for i, case in enumerate(test_cases):
        try:
            if case["score"] >= threshold:
                passed += 1
            else:
                failed += 1
                errors.append(f"Case {{i}}: relevancy score {{case['score']}} < threshold {{threshold}}")
        except Exception as e:
            failed += 1
            errors.append(f"Case {{i}}: {{str(e)[:50]}}")

    return passed, failed, errors, len(test_cases)


def test_context_precision():
    """Test context precision metric."""
    test_cases = [
        {{"context": ["Python info", "Java info"], "relevant": [True, False], "precision": 0.5}},
        {{"context": ["Weather data", "News"], "relevant": [True, True], "precision": 1.0}},
        {{"context": ["Math", "Science", "Art"], "relevant": [True, True, False], "precision": 0.67}},
        {{"context": ["Fact 1"], "relevant": [True], "precision": 1.0}},
        {{"context": ["A", "B", "C", "D"], "relevant": [True, True, True, True], "precision": 1.0}},
        {{"context": ["X", "Y"], "relevant": [False, True], "precision": 0.5}},
        {{"context": ["Code", "Docs", "Tests"], "relevant": [True, True, True], "precision": 1.0}},
        {{"context": ["Data"], "relevant": [True], "precision": 1.0}},
    ]

    passed = 0
    failed = 0
    errors = []
    threshold = 0.6

    for i, case in enumerate(test_cases):
        try:
            if case["precision"] >= threshold:
                passed += 1
            else:
                failed += 1
                errors.append(f"Case {{i}}: precision {{case['precision']}} < threshold {{threshold}}")
        except Exception as e:
            failed += 1
            errors.append(f"Case {{i}}: {{str(e)[:50]}}")

    return passed, failed, errors, len(test_cases)


def test_pipeline_evaluation():
    """Test full pipeline evaluation with multiple metrics."""
    test_cases = [
        {{"faithfulness": 0.9, "relevancy": 0.85, "precision": 0.8}},
        {{"faithfulness": 0.85, "relevancy": 0.9, "precision": 0.75}},
        {{"faithfulness": 0.7, "relevancy": 0.8, "precision": 0.9}},
        {{"faithfulness": 0.95, "relevancy": 0.92, "precision": 0.88}},
        {{"faithfulness": 0.6, "relevancy": 0.7, "precision": 0.65}},
        {{"faithfulness": 0.88, "relevancy": 0.86, "precision": 0.82}},
        {{"faithfulness": 0.92, "relevancy": 0.91, "precision": 0.95}},
        {{"faithfulness": 0.78, "relevancy": 0.82, "precision": 0.79}},
    ]

    passed = 0
    failed = 0
    errors = []
    threshold = 0.75

    for i, case in enumerate(test_cases):
        try:
            avg_score = (case["faithfulness"] + case["relevancy"] + case["precision"]) / 3
            if avg_score >= threshold:
                passed += 1
            else:
                failed += 1
                errors.append(f"Case {{i}}: avg score {{avg_score:.2f}} < threshold {{threshold}}")
        except Exception as e:
            failed += 1
            errors.append(f"Case {{i}}: {{str(e)[:50]}}")

    return passed, failed, errors, len(test_cases)


def main():
    metric = "{metric}"
    mode = "{mode}"
    context_metrics = {context_metrics}
    answer_metrics = {answer_metrics}
    batch_size = {batch_size}
    pass_threshold = {pass_threshold}

    start_time = time.perf_counter()

    try:
        if mode == "pipeline":
            passed, failed, errors, total = test_pipeline_evaluation()
        elif metric == "faithfulness":
            passed, failed, errors, total = test_faithfulness()
        elif metric == "answer_relevancy":
            passed, failed, errors, total = test_answer_relevancy()
        elif metric == "context_precision":
            passed, failed, errors, total = test_context_precision()
        else:
            passed, failed, errors, total = test_faithfulness()

        pass_rate = passed / total if total > 0 else 0.0

        result = {{
            "status": "success",
            "metric": metric,
            "mode": mode,
            "context_metrics": context_metrics,
            "answer_metrics": answer_metrics,
            "batch_size": batch_size,
            "passed": passed,
            "failed": failed,
            "total": total,
            "pass_rate": pass_rate,
            "pass_threshold": pass_threshold,
            "errors": errors[:5],
            "duration_s": time.perf_counter() - start_time,
        }}

        print("RAGAS_RESULT_START")
        print(json.dumps(result, indent=2, default=str))
        print("RAGAS_RESULT_END")

        if pass_rate >= pass_threshold:
            print("RAGAS_STATUS: VERIFIED")
        elif pass_rate >= pass_threshold * 0.7:
            print("RAGAS_STATUS: PARTIALLY_VERIFIED")
        else:
            print("RAGAS_STATUS: NOT_VERIFIED")
    except Exception as e:
        print(f"RAGAS_ERROR: {{e}}")


if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse Ragas output into verification status
pub fn parse_ragas_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("RAGAS_ERROR:") || stderr.contains("RAGAS_ERROR:") {
        let reason = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("RAGAS_ERROR:"))
            .map(|l| l.replace("RAGAS_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown Ragas error".to_string());
        return (VerificationStatus::Unknown { reason }, None);
    }

    if let Some(json_str) = extract_json_result(stdout, "RAGAS_RESULT_START", "RAGAS_RESULT_END") {
        if let Ok(val) = serde_json::from_str::<Value>(&json_str) {
            let status = val["status"].as_str().unwrap_or("error");
            let pass_rate = val["pass_rate"].as_f64().unwrap_or(0.0);
            let threshold = val["pass_threshold"].as_f64().unwrap_or(0.75);

            if status == "error" {
                let reason = val["error"]
                    .as_str()
                    .unwrap_or("Unknown Ragas failure")
                    .to_string();
                return (VerificationStatus::Unknown { reason }, None);
            }

            let counterexample = build_llm_eval_counterexample("Ragas", &val);

            if pass_rate >= threshold {
                (VerificationStatus::Proven, counterexample)
            } else if pass_rate >= threshold * 0.7 {
                (
                    VerificationStatus::Partial {
                        verified_percentage: (pass_rate / threshold) * 100.0,
                    },
                    counterexample,
                )
            } else {
                (VerificationStatus::Disproven, counterexample)
            }
        } else {
            (
                VerificationStatus::Unknown {
                    reason: "Failed to parse Ragas output".to_string(),
                },
                None,
            )
        }
    } else if stdout.contains("RAGAS_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("RAGAS_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 80.0,
            },
            None,
        )
    } else if stdout.contains("RAGAS_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse Ragas output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after = &output[start + start_marker.len()..];
        if let Some(end) = after.find(end_marker) {
            return Some(after[..end].trim().to_string());
        }
    }
    None
}
