//! Evidently script generation and output parsing

use super::config::EvidentlyConfig;
use crate::counterexample::StructuredCounterexample;
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;

/// Generate an Evidently validation script
pub fn generate_evidently_script(
    _spec: &TypedSpec,
    config: &EvidentlyConfig,
) -> Result<String, BackendError> {
    let report_type = config.report_type.as_str();
    let stat_test_method = config.stat_test_method.as_str();
    let drift_threshold = config.drift_threshold;
    let n_samples = config.n_samples;
    let stattest_threshold = config.stattest_threshold;

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
Evidently ML monitoring script
Generated by DashProve

Generates drift and quality reports using Evidently.
"""

import sys
import json
import time
import numpy as np

try:
    import evidently
    from evidently.report import Report
    from evidently.metric_preset import (
        DataDriftPreset,
        DataQualityPreset,
        RegressionPreset,
        ClassificationPreset,
        TargetDriftPreset,
    )
except ImportError as e:
    print(f"EVIDENTLY_ERROR: Missing Evidently: {{e}}")
    print("EVIDENTLY_ERROR: Install with: pip install evidently")
    sys.exit(1)

try:
    import pandas as pd
except ImportError as e:
    print(f"EVIDENTLY_ERROR: Pandas required: {{e}}")
    sys.exit(1)


def create_test_data(n_samples, add_drift=False):
    """Create reference and current data for drift detection."""
    np.random.seed(42)

    # Reference data
    ref_data = pd.DataFrame({{
        "feature_1": np.random.normal(0, 1, n_samples),
        "feature_2": np.random.uniform(0, 10, n_samples),
        "feature_3": np.random.choice(["A", "B", "C"], n_samples),
        "feature_4": np.random.exponential(1, n_samples),
        "target": np.random.randint(0, 2, n_samples),
        "prediction": np.random.randint(0, 2, n_samples),
    }})

    # Current data (with optional drift)
    np.random.seed(123)
    if add_drift:
        # Add drift to simulate real scenario
        curr_data = pd.DataFrame({{
            "feature_1": np.random.normal(0.5, 1.2, n_samples),  # Shifted mean and std
            "feature_2": np.random.uniform(1, 12, n_samples),  # Shifted range
            "feature_3": np.random.choice(["A", "B", "C", "D"], n_samples),  # New category
            "feature_4": np.random.exponential(1.5, n_samples),  # Different rate
            "target": np.random.randint(0, 2, n_samples),
            "prediction": np.random.randint(0, 2, n_samples),
        }})
    else:
        curr_data = pd.DataFrame({{
            "feature_1": np.random.normal(0, 1, n_samples),
            "feature_2": np.random.uniform(0, 10, n_samples),
            "feature_3": np.random.choice(["A", "B", "C"], n_samples),
            "feature_4": np.random.exponential(1, n_samples),
            "target": np.random.randint(0, 2, n_samples),
            "prediction": np.random.randint(0, 2, n_samples),
        }})

    return ref_data, curr_data


def main():
    report_type = "{report_type}"
    stat_test_method = "{stat_test_method}"
    drift_threshold = {drift_threshold}
    n_samples = {n_samples}
    stattest_threshold = {stattest_threshold}

    # Create test data (with no drift for passing tests)
    ref_data, curr_data = create_test_data(n_samples, add_drift=False)

    start_time = time.perf_counter()

    try:
        # Select report preset
        if report_type == "data_drift":
            preset = DataDriftPreset(stattest_threshold=stattest_threshold)
        elif report_type == "data_quality":
            preset = DataQualityPreset()
        elif report_type == "regression_performance":
            preset = RegressionPreset()
        elif report_type == "classification_performance":
            preset = ClassificationPreset()
        elif report_type == "target_drift":
            preset = TargetDriftPreset()
        else:
            preset = DataDriftPreset()

        # Create and run report
        report = Report(metrics=[preset])
        report.run(reference_data=ref_data, current_data=curr_data)

        # Get results
        result_dict = report.as_dict()
        report_time = time.perf_counter() - start_time

        # Extract key metrics
        metrics = result_dict.get("metrics", [])
        drift_detected = False
        drifted_features = []
        total_features = 0
        drifted_count = 0

        for metric in metrics:
            metric_result = metric.get("result", {{}})

            # Check for drift in different report types
            if "drift_by_columns" in metric_result:
                for col, col_result in metric_result["drift_by_columns"].items():
                    total_features += 1
                    if col_result.get("drift_detected", False):
                        drift_detected = True
                        drifted_count += 1
                        drifted_features.append(col)

            if "dataset_drift" in metric_result:
                drift_detected = metric_result.get("dataset_drift", False)

        # Calculate drift score
        drift_score = drifted_count / total_features if total_features > 0 else 0.0

        status = "success"
        error_message = None

    except Exception as e:
        status = "error"
        error_message = str(e)
        report_time = time.perf_counter() - start_time
        drift_detected = False
        drift_score = 0.0
        drifted_features = []
        total_features = 0

    print("EVIDENTLY_RESULT_START")
    result = {{
        "status": status,
        "report_type": report_type,
        "stat_test_method": stat_test_method,
        "reference_samples": len(ref_data),
        "current_samples": len(curr_data),
        "drift_detected": drift_detected,
        "drift_score": drift_score,
        "total_features": total_features,
        "drifted_features": drifted_features,
        "drift_threshold": drift_threshold,
        "report_time_s": report_time,
        "evidently_version": evidently.__version__,
        "error": error_message
    }}
    print(json.dumps(result, indent=2, default=str))
    print("EVIDENTLY_RESULT_END")

    if status == "success":
        print(f"\nEVIDENTLY_SUMMARY: Report type: {{report_type}}")
        print(f"EVIDENTLY_SUMMARY: Drift detected: {{drift_detected}}")
        print(f"EVIDENTLY_SUMMARY: Drift score: {{drift_score:.2%}}")
        print(f"EVIDENTLY_SUMMARY: Drifted features: {{len(drifted_features)}}/{{total_features}}")

        if not drift_detected and drift_score <= drift_threshold:
            print("EVIDENTLY_STATUS: VERIFIED")
        elif drift_score <= drift_threshold * 2:
            print("EVIDENTLY_STATUS: PARTIALLY_VERIFIED")
        else:
            print("EVIDENTLY_STATUS: NOT_VERIFIED")
    else:
        print(f"EVIDENTLY_ERROR: {{error_message}}")


if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse Evidently output
pub fn parse_evidently_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("EVIDENTLY_ERROR:") || stderr.contains("EVIDENTLY_ERROR:") {
        let error_msg = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("EVIDENTLY_ERROR:"))
            .map(|l| l.replace("EVIDENTLY_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown Evidently error".to_string());
        return (VerificationStatus::Unknown { reason: error_msg }, None);
    }

    if let Some(json_str) =
        extract_json_result(stdout, "EVIDENTLY_RESULT_START", "EVIDENTLY_RESULT_END")
    {
        if let Ok(result) = serde_json::from_str::<serde_json::Value>(&json_str) {
            let status = result["status"].as_str().unwrap_or("error");
            let drift_detected = result["drift_detected"].as_bool().unwrap_or(false);
            let drift_score = result["drift_score"].as_f64().unwrap_or(0.0);
            let drift_threshold = result["drift_threshold"].as_f64().unwrap_or(0.1);

            if status == "error" {
                let error = result["error"]
                    .as_str()
                    .unwrap_or("Unknown error")
                    .to_string();
                return (VerificationStatus::Unknown { reason: error }, None);
            }

            let counterexample = crate::counterexample::build_evidently_counterexample(&result);

            if !drift_detected && drift_score <= drift_threshold {
                return (VerificationStatus::Proven, None);
            } else if drift_score <= drift_threshold * 2.0 {
                return (
                    VerificationStatus::Partial {
                        verified_percentage: (1.0 - drift_score) * 100.0,
                    },
                    counterexample,
                );
            } else {
                return (VerificationStatus::Disproven, counterexample);
            }
        }
    }

    if stdout.contains("EVIDENTLY_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("EVIDENTLY_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 80.0,
            },
            None,
        )
    } else if stdout.contains("EVIDENTLY_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse Evidently output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after_start = &output[start + start_marker.len()..];
        if let Some(end) = after_start.find(end_marker) {
            return Some(after_start[..end].trim().to_string());
        }
    }
    None
}
