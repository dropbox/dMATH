//! WhyLogs script generation and output parsing

use super::config::WhyLogsConfig;
use crate::counterexample::StructuredCounterexample;
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;

/// Generate a WhyLogs profiling script
pub fn generate_whylogs_script(
    _spec: &TypedSpec,
    config: &WhyLogsConfig,
) -> Result<String, BackendError> {
    let profile_type = config.profile_type.as_str();
    let constraint_type = config.constraint_type.as_str();
    let n_samples = config.n_samples;
    let track_histograms = if config.track_histograms {
        "True"
    } else {
        "False"
    };
    let track_frequent_items = if config.track_frequent_items {
        "True"
    } else {
        "False"
    };
    let track_cardinality = if config.track_cardinality {
        "True"
    } else {
        "False"
    };

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
WhyLogs data profiling script
Generated by DashProve

Profiles data and validates constraints using WhyLogs.
"""

import sys
import json
import time
import numpy as np

try:
    import whylogs as why
    from whylogs.core.constraints import Constraints, ConstraintsBuilder
    from whylogs.core.constraints.factories import (
        greater_than_number,
        smaller_than_number,
        is_in_range,
        no_missing_values,
        column_is_nullable_integral,
    )
except ImportError as e:
    print(f"WHYLOGS_ERROR: Missing WhyLogs: {{e}}")
    print("WHYLOGS_ERROR: Install with: pip install whylogs")
    sys.exit(1)

try:
    import pandas as pd
except ImportError as e:
    print(f"WHYLOGS_ERROR: Pandas required: {{e}}")
    sys.exit(1)


def create_test_dataframe(n_samples):
    """Create a test DataFrame for profiling."""
    np.random.seed(42)

    return pd.DataFrame({{
        "id": range(1, n_samples + 1),
        "name": [f"item_{{i}}" for i in range(n_samples)],
        "value": np.random.uniform(0, 100, n_samples),
        "count": np.random.randint(0, 50, n_samples),
        "category": np.random.choice(["A", "B", "C", "D"], n_samples),
        "score": np.random.normal(50, 10, n_samples),
        "is_active": np.random.choice([True, False], n_samples),
    }})


def build_constraints(constraint_type):
    """Build constraints based on type."""
    builder = ConstraintsBuilder()

    if constraint_type in ["schema", "all"]:
        # Schema constraints
        builder.add_constraint(no_missing_values(column_name="id"))

    if constraint_type in ["value", "all"]:
        # Value range constraints
        builder.add_constraint(greater_than_number(column_name="value", number=0))
        builder.add_constraint(smaller_than_number(column_name="value", number=100))
        builder.add_constraint(is_in_range(column_name="count", lower=0, upper=100))

    return builder.build()


def main():
    profile_type = "{profile_type}"
    constraint_type = "{constraint_type}"
    n_samples = {n_samples}
    track_histograms = {track_histograms}
    track_frequent_items = {track_frequent_items}
    track_cardinality = {track_cardinality}

    # Create test data
    df = create_test_dataframe(n_samples)

    start_time = time.perf_counter()

    try:
        # Create profile
        profile_result = why.log(df)
        profile = profile_result.profile()
        view = profile.view()

        # Get column summaries
        column_summaries = {{}}
        columns = view.get_columns()

        for col_name in df.columns:
            col_view = columns.get(col_name)
            if col_view:
                summary = col_view.to_summary_dict()
                column_summaries[col_name] = {{
                    "count": summary.get("counts/n", 0),
                    "null_count": summary.get("counts/null", 0),
                    "dtype": str(df[col_name].dtype),
                }}

        # Build and check constraints
        constraints = build_constraints(constraint_type)
        report = constraints.generate_constraints_report(view)

        # Parse constraint results
        constraint_results = []
        passed = 0
        failed = 0

        for feature_name, feature_report in report.items():
            for constraint_name, result in feature_report.items():
                constraint_passed = result == 1  # 1 = passed, 0 = failed
                constraint_results.append({{
                    "feature": feature_name,
                    "constraint": constraint_name,
                    "passed": constraint_passed,
                }})
                if constraint_passed:
                    passed += 1
                else:
                    failed += 1

        profile_time = time.perf_counter() - start_time

        total = passed + failed
        success_rate = passed / total if total > 0 else 1.0

        status = "success"
        error_message = None

    except Exception as e:
        status = "error"
        error_message = str(e)
        profile_time = time.perf_counter() - start_time
        passed = 0
        failed = 0
        success_rate = 0.0
        constraint_results = []
        column_summaries = {{}}

    print("WHYLOGS_RESULT_START")
    result = {{
        "status": status,
        "profile_type": profile_type,
        "constraint_type": constraint_type,
        "n_samples": n_samples,
        "n_columns": len(df.columns) if status == "success" else 0,
        "column_summaries": column_summaries,
        "constraints_evaluated": passed + failed,
        "constraints_passed": passed,
        "constraints_failed": failed,
        "success_rate": success_rate,
        "constraint_results": constraint_results[:20],  # Limit output
        "profile_time_s": profile_time,
        "whylogs_version": why.__version__,
        "error": error_message
    }}
    print(json.dumps(result, indent=2, default=str))
    print("WHYLOGS_RESULT_END")

    if status == "success":
        print(f"\nWHYLOGS_SUMMARY: Profile type: {{profile_type}}")
        print(f"WHYLOGS_SUMMARY: Columns profiled: {{len(column_summaries)}}")
        print(f"WHYLOGS_SUMMARY: Constraints: {{passed + failed}} evaluated")
        print(f"WHYLOGS_SUMMARY: Passed: {{passed}}, Failed: {{failed}}")
        print(f"WHYLOGS_SUMMARY: Success rate: {{success_rate:.1%}}")

        if success_rate >= 1.0:
            print("WHYLOGS_STATUS: VERIFIED")
        elif success_rate >= 0.8:
            print("WHYLOGS_STATUS: PARTIALLY_VERIFIED")
        else:
            print("WHYLOGS_STATUS: NOT_VERIFIED")
    else:
        print(f"WHYLOGS_ERROR: {{error_message}}")


if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse WhyLogs output
pub fn parse_whylogs_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("WHYLOGS_ERROR:") || stderr.contains("WHYLOGS_ERROR:") {
        let error_msg = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("WHYLOGS_ERROR:"))
            .map(|l| l.replace("WHYLOGS_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown WhyLogs error".to_string());
        return (VerificationStatus::Unknown { reason: error_msg }, None);
    }

    if let Some(json_str) =
        extract_json_result(stdout, "WHYLOGS_RESULT_START", "WHYLOGS_RESULT_END")
    {
        if let Ok(result) = serde_json::from_str::<serde_json::Value>(&json_str) {
            let status = result["status"].as_str().unwrap_or("error");
            let success_rate = result["success_rate"].as_f64().unwrap_or(0.0);

            if status == "error" {
                let error = result["error"]
                    .as_str()
                    .unwrap_or("Unknown error")
                    .to_string();
                return (VerificationStatus::Unknown { reason: error }, None);
            }

            let counterexample = crate::counterexample::build_whylogs_counterexample(&result);

            if success_rate >= 1.0 {
                return (VerificationStatus::Proven, None);
            } else if success_rate >= 0.8 {
                return (
                    VerificationStatus::Partial {
                        verified_percentage: success_rate * 100.0,
                    },
                    counterexample,
                );
            } else {
                return (VerificationStatus::Disproven, counterexample);
            }
        }
    }

    if stdout.contains("WHYLOGS_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("WHYLOGS_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 80.0,
            },
            None,
        )
    } else if stdout.contains("WHYLOGS_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse WhyLogs output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after_start = &output[start + start_marker.len()..];
        if let Some(end) = after_start.find(end_marker) {
            return Some(after_start[..end].trim().to_string());
        }
    }
    None
}
