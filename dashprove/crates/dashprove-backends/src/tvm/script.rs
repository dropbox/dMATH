//! Apache TVM script generation and output parsing

use super::config::TVMConfig;
use crate::counterexample::StructuredCounterexample;
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;

/// Generate a TVM compilation verification script
pub fn generate_tvm_script(_spec: &TypedSpec, config: &TVMConfig) -> Result<String, BackendError> {
    let model_path = config
        .model_path
        .as_ref()
        .map(|p| p.display().to_string())
        .unwrap_or_else(|| "model.onnx".to_string());
    let target = config.target.as_str();
    let opt_level = config.opt_level.as_int();
    let tuning_mode = config.tuning_mode.as_str();
    let enable_relay_opt = if config.enable_relay_opt {
        "True"
    } else {
        "False"
    };
    let warmup_iters = config.warmup_iterations;
    let bench_iters = config.benchmark_iterations;

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
Apache TVM model compilation verification script
Generated by DashProve

Validates TVM compilation and inference correctness.
"""

import sys
import json
import time
import numpy as np

try:
    import tvm
    from tvm import relay
    from tvm.contrib import graph_executor
except ImportError as e:
    print(f"TVM_ERROR: Missing TVM: {{e}}")
    print("TVM_ERROR: Install with: pip install apache-tvm")
    sys.exit(1)

def create_test_model():
    """Create a simple Relay model for testing."""
    # Simple MLP: input -> dense -> relu -> dense -> output
    input_shape = (1, 10)
    x = relay.var("input", shape=input_shape, dtype="float32")

    w1 = relay.var("w1", shape=(10, 20), dtype="float32")
    b1 = relay.var("b1", shape=(20,), dtype="float32")
    dense1 = relay.nn.dense(x, w1)
    bias1 = relay.nn.bias_add(dense1, b1)
    relu1 = relay.nn.relu(bias1)

    w2 = relay.var("w2", shape=(20, 5), dtype="float32")
    b2 = relay.var("b2", shape=(5,), dtype="float32")
    dense2 = relay.nn.dense(relu1, w2)
    output = relay.nn.bias_add(dense2, b2)

    func = relay.Function([x, w1, b1, w2, b2], output)
    mod = tvm.IRModule.from_expr(func)

    params = {{
        "w1": tvm.nd.array(np.random.randn(10, 20).astype("float32")),
        "b1": tvm.nd.array(np.zeros(20).astype("float32")),
        "w2": tvm.nd.array(np.random.randn(20, 5).astype("float32")),
        "b2": tvm.nd.array(np.zeros(5).astype("float32")),
    }}

    return mod, params, input_shape

def load_onnx_model(model_path: str):
    """Load ONNX model and convert to Relay."""
    try:
        import onnx
        onnx_model = onnx.load(model_path)

        # Get input shape from ONNX model
        input_info = onnx_model.graph.input[0]
        input_shape = []
        for dim in input_info.type.tensor_type.shape.dim:
            if dim.dim_value > 0:
                input_shape.append(dim.dim_value)
            else:
                input_shape.append(1)  # Dynamic dim default

        shape_dict = {{input_info.name: input_shape}}
        mod, params = relay.frontend.from_onnx(onnx_model, shape_dict)
        return mod, params, tuple(input_shape)
    except Exception as e:
        print(f"TVM_INFO: Failed to load ONNX: {{e}}")
        return None, None, None

def main():
    model_path = "{model_path}"
    target_str = "{target}"
    opt_level = {opt_level}
    tuning_mode = "{tuning_mode}"
    enable_relay_opt = {enable_relay_opt}
    warmup_iters = {warmup_iters}
    bench_iters = {bench_iters}

    # Try loading model
    mod, params, input_shape = load_onnx_model(model_path)
    if mod is None:
        print("TVM_INFO: Using synthetic test model")
        mod, params, input_shape = create_test_model()

    # Configure target
    try:
        target = tvm.target.Target(target_str)
    except Exception as e:
        print(f"TVM_INFO: Target '{{target_str}}' not available, using LLVM")
        target = tvm.target.Target("llvm")
        target_str = "llvm"

    # Apply Relay optimizations
    if enable_relay_opt:
        with tvm.transform.PassContext(opt_level=opt_level):
            mod = relay.transform.InferType()(mod)
            mod = relay.transform.FoldConstant()(mod)
            mod = relay.transform.FuseOps()(mod)

    # Compile
    try:
        with tvm.transform.PassContext(opt_level=opt_level):
            lib = relay.build(mod, target=target, params=params)
    except Exception as e:
        print(f"TVM_ERROR: Compilation failed: {{e}}")
        sys.exit(1)

    # Create runtime
    try:
        dev = tvm.device(str(target), 0)
        module = graph_executor.GraphModule(lib["default"](dev))
    except Exception as e:
        print(f"TVM_INFO: Device not available, using CPU")
        dev = tvm.cpu(0)
        target_str = "llvm"
        with tvm.transform.PassContext(opt_level=opt_level):
            lib = relay.build(mod, target="llvm", params=params)
        module = graph_executor.GraphModule(lib["default"](dev))

    # Generate test input
    np.random.seed(42)
    test_input = np.random.randn(*input_shape).astype("float32")
    module.set_input("input", test_input)

    # Warmup
    for _ in range(warmup_iters):
        module.run()

    # Benchmark
    latencies = []
    outputs = []
    for _ in range(bench_iters):
        start = time.perf_counter()
        module.run()
        latencies.append((time.perf_counter() - start) * 1000)
        outputs.append(module.get_output(0).numpy().copy())

    # Check consistency
    reference = outputs[0]
    consistent = True
    max_diff = 0.0
    for out in outputs[1:]:
        diff = np.max(np.abs(out - reference))
        max_diff = max(max_diff, diff)
        if diff > 1e-5:
            consistent = False

    # Statistics
    latencies = np.array(latencies)
    mean_lat = float(np.mean(latencies))
    std_lat = float(np.std(latencies))
    p50_lat = float(np.percentile(latencies, 50))
    p95_lat = float(np.percentile(latencies, 95))
    p99_lat = float(np.percentile(latencies, 99))
    min_lat = float(np.min(latencies))
    max_lat = float(np.max(latencies))
    throughput = 1000.0 / mean_lat

    output_shape = list(reference.shape)

    print("TVM_RESULT_START")
    result = {{
        "model_path": model_path,
        "target_requested": "{target}",
        "target_used": target_str,
        "opt_level": opt_level,
        "tuning_mode": tuning_mode,
        "tvm_version": tvm.__version__,
        "input_shape": list(input_shape),
        "output_shape": output_shape,
        "warmup_iterations": warmup_iters,
        "benchmark_iterations": bench_iters,
        "consistent_outputs": consistent,
        "max_output_diff": float(max_diff),
        "latency_ms": {{
            "mean": mean_lat,
            "std": std_lat,
            "min": min_lat,
            "max": max_lat,
            "p50": p50_lat,
            "p95": p95_lat,
            "p99": p99_lat
        }},
        "throughput_ips": throughput
    }}
    print(json.dumps(result, indent=2))
    print("TVM_RESULT_END")

    print(f"\\nTVM_SUMMARY: Mean latency: {{mean_lat:.3f}}ms (std: {{std_lat:.3f}}ms)")
    print(f"TVM_SUMMARY: P95: {{p95_lat:.3f}}ms, P99: {{p99_lat:.3f}}ms")
    print(f"TVM_SUMMARY: Throughput: {{throughput:.1f}} inf/sec")
    print(f"TVM_SUMMARY: Output consistency: {{'PASS' if consistent else 'FAIL'}}")

    if consistent and max_diff < 1e-5:
        print("TVM_STATUS: VERIFIED")
    elif consistent:
        print("TVM_STATUS: PARTIALLY_VERIFIED")
    else:
        print("TVM_STATUS: NOT_VERIFIED")

if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse TVM output
pub fn parse_tvm_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("TVM_ERROR:") || stderr.contains("TVM_ERROR:") {
        let error_msg = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("TVM_ERROR:"))
            .map(|l| l.replace("TVM_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown TVM error".to_string());
        return (VerificationStatus::Unknown { reason: error_msg }, None);
    }

    if let Some(json_str) = extract_json_result(stdout, "TVM_RESULT_START", "TVM_RESULT_END") {
        if let Ok(result) = serde_json::from_str::<serde_json::Value>(&json_str) {
            let consistent = result["consistent_outputs"].as_bool().unwrap_or(false);
            let max_diff = result["max_output_diff"].as_f64().unwrap_or(1.0);

            let counterexample =
                crate::counterexample::build_compiler_counterexample(&result, "TVM");

            if consistent && max_diff < 1e-5 {
                return (VerificationStatus::Proven, None);
            } else if consistent {
                return (
                    VerificationStatus::Partial {
                        verified_percentage: 95.0,
                    },
                    counterexample,
                );
            } else {
                return (VerificationStatus::Disproven, counterexample);
            }
        }
    }

    if stdout.contains("TVM_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("TVM_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 95.0,
            },
            None,
        )
    } else if stdout.contains("TVM_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse TVM output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after_start = &output[start + start_marker.len()..];
        if let Some(end) = after_start.find(end_marker) {
            return Some(after_start[..end].trim().to_string());
        }
    }
    None
}
