//! NNCF script generation and output parsing

use super::config::NNCFConfig;
use crate::counterexample::StructuredCounterexample;
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;

/// Generate an NNCF compression verification script
pub fn generate_nncf_script(
    _spec: &TypedSpec,
    config: &NNCFConfig,
) -> Result<String, BackendError> {
    let compression_mode = config.compression_mode.as_str();
    let quantization_mode = config.quantization_mode.as_str();
    let bit_width = config.bit_width.as_str();
    let target_sparsity = config.target_sparsity;
    let pruning_schedule = config.pruning_schedule.as_str();
    let calibration_samples = config.calibration_samples;

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
NNCF compression verification script
Generated by DashProve

Validates model compression correctness and compression ratio.
"""

import sys
import json
import time
import numpy as np

try:
    import nncf
    from nncf import NNCFConfig as NNCFConfigClass
    from nncf.torch import create_compressed_model
except ImportError as e:
    print(f"NNCF_ERROR: Missing NNCF: {{e}}")
    print("NNCF_ERROR: Install with: pip install nncf")
    sys.exit(1)

# Require PyTorch
try:
    import torch
    import torch.nn as nn
except ImportError as e:
    print(f"NNCF_ERROR: PyTorch required: {{e}}")
    sys.exit(1)


class SimpleModel(nn.Module):
    """Simple test model for compression."""
    def __init__(self):
        super().__init__()
        self.conv1 = nn.Conv2d(3, 16, 3, padding=1)
        self.bn1 = nn.BatchNorm2d(16)
        self.relu1 = nn.ReLU()
        self.conv2 = nn.Conv2d(16, 32, 3, padding=1)
        self.bn2 = nn.BatchNorm2d(32)
        self.relu2 = nn.ReLU()
        self.pool = nn.AdaptiveAvgPool2d(1)
        self.fc = nn.Linear(32, 10)

    def forward(self, x):
        x = self.relu1(self.bn1(self.conv1(x)))
        x = self.relu2(self.bn2(self.conv2(x)))
        x = self.pool(x)
        x = x.view(x.size(0), -1)
        x = self.fc(x)
        return x


class CalibrationDataset:
    """Simple calibration dataset."""
    def __init__(self, num_samples):
        np.random.seed(42)
        self.data = [torch.randn(1, 3, 32, 32) for _ in range(num_samples)]

    def __len__(self):
        return len(self.data)

    def __getitem__(self, idx):
        return self.data[idx], 0


def count_parameters(model):
    """Count total and non-zero parameters."""
    total = 0
    nonzero = 0
    for p in model.parameters():
        total += p.numel()
        nonzero += torch.count_nonzero(p).item()
    return total, nonzero


def main():
    compression_mode = "{compression_mode}"
    quantization_mode = "{quantization_mode}"
    bit_width = "{bit_width}"
    target_sparsity = {target_sparsity}
    pruning_schedule = "{pruning_schedule}"
    calibration_samples = {calibration_samples}

    # Create test model
    model = SimpleModel()
    model.eval()

    # Get original model stats
    total_params, original_nonzero = count_parameters(model)
    original_size = sum(p.numel() * p.element_size() for p in model.parameters())

    # Generate test input
    np.random.seed(42)
    test_input = torch.randn(1, 3, 32, 32)

    # Get original output
    with torch.no_grad():
        original_output = model(test_input).numpy()

    # Configure NNCF
    try:
        if compression_mode == "quantization":
            nncf_config_dict = {{
                "input_info": {{"sample_size": [1, 3, 32, 32]}},
                "compression": {{
                    "algorithm": "quantization",
                    "preset": "performance",
                    "initializer": {{
                        "range": {{
                            "num_init_samples": calibration_samples
                        }}
                    }}
                }}
            }}
        elif compression_mode in ["pruning", "sparsity"]:
            nncf_config_dict = {{
                "input_info": {{"sample_size": [1, 3, 32, 32]}},
                "compression": {{
                    "algorithm": "magnitude_sparsity",
                    "sparsity_init": target_sparsity,
                    "params": {{
                        "schedule": pruning_schedule,
                        "sparsity_target": target_sparsity
                    }}
                }}
            }}
        elif compression_mode == "filter_pruning":
            nncf_config_dict = {{
                "input_info": {{"sample_size": [1, 3, 32, 32]}},
                "compression": {{
                    "algorithm": "filter_pruning",
                    "pruning_init": target_sparsity,
                    "params": {{
                        "schedule": pruning_schedule,
                        "pruning_target": target_sparsity
                    }}
                }}
            }}
        else:  # quantization_pruning
            nncf_config_dict = {{
                "input_info": {{"sample_size": [1, 3, 32, 32]}},
                "compression": [
                    {{
                        "algorithm": "quantization",
                        "preset": "performance"
                    }},
                    {{
                        "algorithm": "magnitude_sparsity",
                        "sparsity_init": target_sparsity
                    }}
                ]
            }}

        nncf_config = NNCFConfigClass.from_dict(nncf_config_dict)

        # Create calibration dataset
        calib_dataset = CalibrationDataset(calibration_samples)
        calib_loader = torch.utils.data.DataLoader(calib_dataset, batch_size=1)

        # Compress model
        start_time = time.perf_counter()
        compression_ctrl, compressed_model = create_compressed_model(
            model,
            nncf_config,
            dump_graphs=False
        )
        compression_time = time.perf_counter() - start_time

        # Get compressed output
        compressed_model.eval()
        with torch.no_grad():
            compressed_output = compressed_model(test_input).numpy()

        # Calculate metrics
        output_diff = np.max(np.abs(original_output - compressed_output))
        output_mse = np.mean((original_output - compressed_output) ** 2)

        # Get compression stats
        compressed_params, compressed_nonzero = count_parameters(compressed_model)
        sparsity = 1.0 - (compressed_nonzero / total_params) if total_params > 0 else 0.0

        # Estimate compression ratio
        if compression_mode == "quantization":
            if bit_width == "8":
                compression_ratio = 4.0
            elif bit_width == "4":
                compression_ratio = 8.0
            else:
                compression_ratio = 2.0
        else:
            compression_ratio = 1.0 / (1.0 - sparsity) if sparsity < 1.0 else 1.0

        status = "success"
        error_message = None

    except Exception as e:
        status = "error"
        error_message = str(e)
        compression_time = 0.0
        output_diff = float('inf')
        output_mse = float('inf')
        compression_ratio = 1.0
        sparsity = 0.0
        compressed_model = None

    # Benchmark if successful
    if status == "success" and compressed_model is not None:
        latencies = []
        for _ in range(100):
            start = time.perf_counter()
            with torch.no_grad():
                _ = compressed_model(test_input)
            latencies.append((time.perf_counter() - start) * 1000)

        latencies = np.array(latencies)
        mean_lat = float(np.mean(latencies))
        std_lat = float(np.std(latencies))
        p50_lat = float(np.percentile(latencies, 50))
        p95_lat = float(np.percentile(latencies, 95))
    else:
        mean_lat = std_lat = p50_lat = p95_lat = 0.0

    print("NNCF_RESULT_START")
    result = {{
        "status": status,
        "compression_mode": compression_mode,
        "quantization_mode": quantization_mode,
        "bit_width": bit_width,
        "target_sparsity": target_sparsity,
        "actual_sparsity": float(sparsity) if status == "success" else None,
        "original_size_bytes": original_size,
        "total_params": total_params,
        "compression_ratio": compression_ratio,
        "output_max_diff": float(output_diff) if status == "success" else None,
        "output_mse": float(output_mse) if status == "success" else None,
        "compression_time_s": compression_time,
        "latency_ms": {{
            "mean": mean_lat,
            "std": std_lat,
            "p50": p50_lat,
            "p95": p95_lat
        }},
        "nncf_version": nncf.__version__,
        "error": error_message
    }}
    print(json.dumps(result, indent=2))
    print("NNCF_RESULT_END")

    if status == "success":
        print(f"\\nNNCF_SUMMARY: Compression ratio: {{compression_ratio:.1f}}x")
        print(f"NNCF_SUMMARY: Actual sparsity: {{sparsity:.2%}}")
        print(f"NNCF_SUMMARY: Output max diff: {{output_diff:.6f}}")
        print(f"NNCF_SUMMARY: Mean latency: {{mean_lat:.3f}}ms")

        if output_diff < 0.1 and output_mse < 0.01:
            print("NNCF_STATUS: VERIFIED")
        elif output_diff < 0.5:
            print("NNCF_STATUS: PARTIALLY_VERIFIED")
        else:
            print("NNCF_STATUS: NOT_VERIFIED")
    else:
        print(f"NNCF_ERROR: {{error_message}}")


if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse NNCF output
pub fn parse_nncf_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("NNCF_ERROR:") || stderr.contains("NNCF_ERROR:") {
        let error_msg = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("NNCF_ERROR:"))
            .map(|l| l.replace("NNCF_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown NNCF error".to_string());
        return (VerificationStatus::Unknown { reason: error_msg }, None);
    }

    if let Some(json_str) = extract_json_result(stdout, "NNCF_RESULT_START", "NNCF_RESULT_END") {
        if let Ok(result) = serde_json::from_str::<serde_json::Value>(&json_str) {
            let status = result["status"].as_str().unwrap_or("error");
            let output_diff = result["output_max_diff"].as_f64().unwrap_or(1.0);
            let output_mse = result["output_mse"].as_f64().unwrap_or(1.0);

            if status == "error" {
                let error = result["error"]
                    .as_str()
                    .unwrap_or("Unknown error")
                    .to_string();
                return (VerificationStatus::Unknown { reason: error }, None);
            }

            // Use the model_optimization helper to build a structured counterexample
            let counterexample =
                crate::counterexample::build_quantization_counterexample(&result, "NNCF");

            if output_diff < 0.1 && output_mse < 0.01 {
                return (VerificationStatus::Proven, None);
            } else if output_diff < 0.5 {
                return (
                    VerificationStatus::Partial {
                        verified_percentage: 90.0,
                    },
                    counterexample,
                );
            } else {
                return (VerificationStatus::Disproven, counterexample);
            }
        }
    }

    if stdout.contains("NNCF_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("NNCF_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 90.0,
            },
            None,
        )
    } else if stdout.contains("NNCF_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse NNCF output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after_start = &output[start + start_marker.len()..];
        if let Some(end) = after_start.find(end_marker) {
            return Some(after_start[..end].trim().to_string());
        }
    }
    None
}
