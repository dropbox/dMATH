//! Great Expectations script generation and output parsing

use super::config::GreatExpectationsConfig;
use crate::counterexample::StructuredCounterexample;
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;

/// Generate a Great Expectations validation script
pub fn generate_great_expectations_script(
    _spec: &TypedSpec,
    config: &GreatExpectationsConfig,
) -> Result<String, BackendError> {
    let validation_level = config.validation_level.as_str();
    let data_source_type = config.data_source_type.as_str();
    let result_format = config.result_format.as_str();
    let catch_exceptions = if config.catch_exceptions {
        "True"
    } else {
        "False"
    };
    let include_unexpected_rows = if config.include_unexpected_rows {
        "True"
    } else {
        "False"
    };
    let max_unexpected_values = config.max_unexpected_values;

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
Great Expectations data validation script
Generated by DashProve

Validates data quality using expectation suites.
"""

import sys
import json
import time
import numpy as np

try:
    import great_expectations as gx
    from great_expectations.core import ExpectationSuite
    from great_expectations.core.expectation_validation_result import ExpectationValidationResult
except ImportError as e:
    print(f"GX_ERROR: Missing Great Expectations: {{e}}")
    print("GX_ERROR: Install with: pip install great_expectations")
    sys.exit(1)

try:
    import pandas as pd
except ImportError as e:
    print(f"GX_ERROR: Pandas required: {{e}}")
    sys.exit(1)


def create_test_dataframe():
    """Create a test DataFrame for validation."""
    np.random.seed(42)
    n_rows = 1000

    return pd.DataFrame({{
        "id": range(1, n_rows + 1),
        "name": [f"item_{{i}}" for i in range(n_rows)],
        "value": np.random.uniform(0, 100, n_rows),
        "count": np.random.randint(0, 50, n_rows),
        "category": np.random.choice(["A", "B", "C", "D"], n_rows),
        "timestamp": pd.date_range("2024-01-01", periods=n_rows, freq="h"),
        "is_active": np.random.choice([True, False], n_rows),
        "score": np.random.normal(50, 10, n_rows),
    }})


def build_expectation_suite(validation_level):
    """Build an expectation suite based on validation level."""
    expectations = []

    # Basic expectations (always included)
    expectations.extend([
        {{"expectation_type": "expect_table_row_count_to_be_between", "kwargs": {{"min_value": 100, "max_value": 10000}}}},
        {{"expectation_type": "expect_column_to_exist", "kwargs": {{"column": "id"}}}},
        {{"expectation_type": "expect_column_to_exist", "kwargs": {{"column": "value"}}}},
        {{"expectation_type": "expect_column_values_to_not_be_null", "kwargs": {{"column": "id"}}}},
        {{"expectation_type": "expect_column_values_to_be_unique", "kwargs": {{"column": "id"}}}},
    ])

    if validation_level in ["standard", "comprehensive"]:
        # Standard expectations
        expectations.extend([
            {{"expectation_type": "expect_column_values_to_be_between", "kwargs": {{"column": "value", "min_value": 0, "max_value": 100}}}},
            {{"expectation_type": "expect_column_values_to_be_in_set", "kwargs": {{"column": "category", "value_set": ["A", "B", "C", "D"]}}}},
            {{"expectation_type": "expect_column_values_to_be_of_type", "kwargs": {{"column": "is_active", "type_": "bool"}}}},
            {{"expectation_type": "expect_column_mean_to_be_between", "kwargs": {{"column": "score", "min_value": 40, "max_value": 60}}}},
        ])

    if validation_level == "comprehensive":
        # Comprehensive expectations
        expectations.extend([
            {{"expectation_type": "expect_column_stdev_to_be_between", "kwargs": {{"column": "score", "min_value": 5, "max_value": 15}}}},
            {{"expectation_type": "expect_column_proportion_of_unique_values_to_be_between", "kwargs": {{"column": "name", "min_value": 0.9, "max_value": 1.0}}}},
            {{"expectation_type": "expect_column_values_to_be_between", "kwargs": {{"column": "count", "min_value": 0, "max_value": 100}}}},
        ])

    return expectations


def main():
    validation_level = "{validation_level}"
    data_source_type = "{data_source_type}"
    result_format = "{result_format}"
    catch_exceptions = {catch_exceptions}
    include_unexpected_rows = {include_unexpected_rows}
    max_unexpected_values = {max_unexpected_values}

    # Create test data
    df = create_test_dataframe()

    start_time = time.perf_counter()

    try:
        # Create context and data source
        context = gx.get_context()

        # Create expectation suite
        suite_name = "dashprove_validation_suite"
        expectations = build_expectation_suite(validation_level)

        # Build validator
        validator = context.sources.pandas_default.read_dataframe(df)

        # Run expectations manually
        validation_results = []
        successful = 0
        failed = 0

        for exp in expectations:
            try:
                exp_type = exp["expectation_type"]
                kwargs = exp.get("kwargs", {{}})

                # Get the expectation method from validator
                method = getattr(validator, exp_type, None)
                if method:
                    result = method(**kwargs)
                    passed = result.success if hasattr(result, 'success') else True
                    validation_results.append({{
                        "expectation": exp_type,
                        "success": passed,
                        "kwargs": kwargs,
                    }})
                    if passed:
                        successful += 1
                    else:
                        failed += 1
                else:
                    validation_results.append({{
                        "expectation": exp_type,
                        "success": None,
                        "error": "Expectation not found",
                    }})
            except Exception as e:
                if catch_exceptions:
                    validation_results.append({{
                        "expectation": exp.get("expectation_type", "unknown"),
                        "success": False,
                        "error": str(e),
                    }})
                    failed += 1
                else:
                    raise

        validation_time = time.perf_counter() - start_time

        total = successful + failed
        success_rate = successful / total if total > 0 else 0.0

        status = "success"
        error_message = None

    except Exception as e:
        status = "error"
        error_message = str(e)
        validation_time = time.perf_counter() - start_time
        successful = 0
        failed = 0
        success_rate = 0.0
        validation_results = []

    print("GX_RESULT_START")
    result = {{
        "status": status,
        "validation_level": validation_level,
        "data_source_type": data_source_type,
        "row_count": len(df) if status == "success" else 0,
        "column_count": len(df.columns) if status == "success" else 0,
        "expectations_evaluated": successful + failed,
        "expectations_passed": successful,
        "expectations_failed": failed,
        "success_rate": success_rate,
        "validation_time_s": validation_time,
        "validation_results": validation_results[:20],  # Limit for output
        "gx_version": gx.__version__,
        "error": error_message
    }}
    print(json.dumps(result, indent=2, default=str))
    print("GX_RESULT_END")

    if status == "success":
        print(f"\nGX_SUMMARY: Expectations: {{successful + failed}} evaluated")
        print(f"GX_SUMMARY: Passed: {{successful}}, Failed: {{failed}}")
        print(f"GX_SUMMARY: Success rate: {{success_rate:.1%}}")
        print(f"GX_SUMMARY: Validation time: {{validation_time:.3f}}s")

        if success_rate >= 1.0:
            print("GX_STATUS: VERIFIED")
        elif success_rate >= 0.9:
            print("GX_STATUS: PARTIALLY_VERIFIED")
        else:
            print("GX_STATUS: NOT_VERIFIED")
    else:
        print(f"GX_ERROR: {{error_message}}")


if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse Great Expectations output
pub fn parse_great_expectations_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("GX_ERROR:") || stderr.contains("GX_ERROR:") {
        let error_msg = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("GX_ERROR:"))
            .map(|l| l.replace("GX_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown Great Expectations error".to_string());
        return (VerificationStatus::Unknown { reason: error_msg }, None);
    }

    if let Some(json_str) = extract_json_result(stdout, "GX_RESULT_START", "GX_RESULT_END") {
        if let Ok(result) = serde_json::from_str::<serde_json::Value>(&json_str) {
            let status = result["status"].as_str().unwrap_or("error");
            let success_rate = result["success_rate"].as_f64().unwrap_or(0.0);

            if status == "error" {
                let error = result["error"]
                    .as_str()
                    .unwrap_or("Unknown error")
                    .to_string();
                return (VerificationStatus::Unknown { reason: error }, None);
            }

            let counterexample =
                crate::counterexample::build_great_expectations_counterexample(&result);

            if success_rate >= 1.0 {
                return (VerificationStatus::Proven, None);
            } else if success_rate >= 0.9 {
                return (
                    VerificationStatus::Partial {
                        verified_percentage: success_rate * 100.0,
                    },
                    counterexample,
                );
            } else {
                return (VerificationStatus::Disproven, counterexample);
            }
        }
    }

    if stdout.contains("GX_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("GX_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 90.0,
            },
            None,
        )
    } else if stdout.contains("GX_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse Great Expectations output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after_start = &output[start + start_marker.len()..];
        if let Some(end) = after_start.find(end_marker) {
            return Some(after_start[..end].trim().to_string());
        }
    }
    None
}
