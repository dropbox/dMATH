//! Promptfoo script generation and output parsing

use super::config::PromptfooConfig;
use crate::counterexample::{build_llm_eval_counterexample, StructuredCounterexample};
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;
use serde_json::Value;

/// Generate Promptfoo verification script (Node.js)
pub fn generate_promptfoo_script(
    _spec: &TypedSpec,
    config: &PromptfooConfig,
) -> Result<String, BackendError> {
    let assertion_type = config.assertion_type.as_str();
    let output_format = config.output_format.as_str();
    let iterations = config.iterations;
    let max_concurrency = config.max_concurrency;
    let pass_threshold = config.pass_rate_threshold;

    Ok(format!(
        r#"#!/usr/bin/env node
/**
 * Promptfoo prompt evaluation script generated by DashProve
 * Tests prompt quality and assertion pass rates.
 */

const assertionType = "{assertion_type}";
const outputFormat = "{output_format}";
const iterations = {iterations};
const maxConcurrency = {max_concurrency};
const passThreshold = {pass_threshold};

// Simulated test cases for prompt evaluation
const testCases = [
  {{
    prompt: "What is 2+2?",
    expected: "4",
    response: "The answer is 4.",
    assertionType: "contains"
  }},
  {{
    prompt: "Say hello",
    expected: "hello",
    response: "Hello! How can I help you?",
    assertionType: "contains"
  }},
  {{
    prompt: "Return JSON with name",
    expected: '{{"name":',
    response: '{{"name": "Alice", "age": 30}}',
    assertionType: "is-json"
  }},
  {{
    prompt: "Explain AI briefly",
    expected: "artificial intelligence",
    response: "AI stands for artificial intelligence, the simulation of human intelligence.",
    assertionType: "contains"
  }},
  {{
    prompt: "What color is the sky?",
    expected: "blue",
    response: "The sky appears blue due to Rayleigh scattering.",
    assertionType: "contains"
  }},
  {{
    prompt: "Respond with only YES or NO",
    expected: "^(YES|NO)$",
    response: "MAYBE",
    assertionType: "regex"
  }},
  {{
    prompt: "List three fruits",
    expected: "apple",
    response: "Here are three fruits: apple, banana, and orange.",
    assertionType: "contains"
  }},
  {{
    prompt: "Exact match test",
    expected: "exact",
    response: "not exact",
    assertionType: "equals"
  }}
];

function runContainsAssertion(response, expected) {{
  return response.toLowerCase().includes(expected.toLowerCase());
}}

function runEqualsAssertion(response, expected) {{
  return response.trim() === expected.trim();
}}

function runRegexAssertion(response, pattern) {{
  try {{
    const regex = new RegExp(pattern, 'i');
    return regex.test(response);
  }} catch (e) {{
    return false;
  }}
}}

function runJsonAssertion(response) {{
  try {{
    JSON.parse(response);
    return true;
  }} catch (e) {{
    return false;
  }}
}}

function runAssertion(testCase, targetAssertionType) {{
  const type = targetAssertionType || testCase.assertionType;
  switch (type) {{
    case 'contains':
      return runContainsAssertion(testCase.response, testCase.expected);
    case 'equals':
      return runEqualsAssertion(testCase.response, testCase.expected);
    case 'regex':
      return runRegexAssertion(testCase.response, testCase.expected);
    case 'is-json':
      return runJsonAssertion(testCase.response);
    default:
      return runContainsAssertion(testCase.response, testCase.expected);
  }}
}}

async function main() {{
  const startTime = Date.now();

  let passed = 0;
  let failed = 0;
  const errors = [];

  for (let iter = 0; iter < iterations; iter++) {{
    for (let i = 0; i < testCases.length; i++) {{
      const testCase = testCases[i];
      const success = runAssertion(testCase, assertionType);

      if (success) {{
        passed++;
      }} else {{
        failed++;
        if (errors.length < 5) {{
          errors.push(`Case ${{i}} (iter ${{iter}}): assertion failed for prompt "${{testCase.prompt.substring(0, 30)}}..."`);
        }}
      }}
    }}
  }}

  const total = passed + failed;
  const passRate = total > 0 ? passed / total : 0;
  const durationS = (Date.now() - startTime) / 1000;

  const result = {{
    status: "success",
    assertion_type: assertionType,
    output_format: outputFormat,
    iterations: iterations,
    max_concurrency: maxConcurrency,
    passed: passed,
    failed: failed,
    total: total,
    pass_rate: passRate,
    pass_threshold: passThreshold,
    errors: errors,
    duration_s: durationS
  }};

  console.log("PROMPTFOO_RESULT_START");
  console.log(JSON.stringify(result, null, 2));
  console.log("PROMPTFOO_RESULT_END");

  if (passRate >= passThreshold) {{
    console.log("PROMPTFOO_STATUS: VERIFIED");
  }} else if (passRate >= passThreshold * 0.7) {{
    console.log("PROMPTFOO_STATUS: PARTIALLY_VERIFIED");
  }} else {{
    console.log("PROMPTFOO_STATUS: NOT_VERIFIED");
  }}
}}

main().catch(e => {{
  console.log(`PROMPTFOO_ERROR: ${{e.message}}`);
  process.exit(1);
}});
"#
    ))
}

/// Parse Promptfoo output into verification status
pub fn parse_promptfoo_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("PROMPTFOO_ERROR:") || stderr.contains("PROMPTFOO_ERROR:") {
        let reason = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("PROMPTFOO_ERROR:"))
            .map(|l| l.replace("PROMPTFOO_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown Promptfoo error".to_string());
        return (VerificationStatus::Unknown { reason }, None);
    }

    if let Some(json_str) =
        extract_json_result(stdout, "PROMPTFOO_RESULT_START", "PROMPTFOO_RESULT_END")
    {
        if let Ok(val) = serde_json::from_str::<Value>(&json_str) {
            let status = val["status"].as_str().unwrap_or("error");
            let pass_rate = val["pass_rate"].as_f64().unwrap_or(0.0);
            let threshold = val["pass_threshold"].as_f64().unwrap_or(0.8);

            if status == "error" {
                let reason = val["error"]
                    .as_str()
                    .unwrap_or("Unknown Promptfoo failure")
                    .to_string();
                return (VerificationStatus::Unknown { reason }, None);
            }

            let counterexample = build_llm_eval_counterexample("PromptFoo", &val);

            if pass_rate >= threshold {
                (VerificationStatus::Proven, counterexample)
            } else if pass_rate >= threshold * 0.7 {
                (
                    VerificationStatus::Partial {
                        verified_percentage: (pass_rate / threshold) * 100.0,
                    },
                    counterexample,
                )
            } else {
                (VerificationStatus::Disproven, counterexample)
            }
        } else {
            (
                VerificationStatus::Unknown {
                    reason: "Failed to parse Promptfoo output".to_string(),
                },
                None,
            )
        }
    } else if stdout.contains("PROMPTFOO_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("PROMPTFOO_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 80.0,
            },
            None,
        )
    } else if stdout.contains("PROMPTFOO_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse Promptfoo output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after = &output[start + start_marker.len()..];
        if let Some(end) = after.find(end_marker) {
            return Some(after[..end].trim().to_string());
        }
    }
    None
}
