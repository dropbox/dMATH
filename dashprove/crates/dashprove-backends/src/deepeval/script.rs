//! DeepEval script generation and output parsing

use super::config::DeepEvalConfig;
use crate::counterexample::{build_llm_eval_counterexample, StructuredCounterexample};
use crate::traits::{BackendError, VerificationStatus};
use dashprove_usl::typecheck::TypedSpec;
use serde_json::Value;

/// Generate DeepEval verification script
pub fn generate_deepeval_script(
    _spec: &TypedSpec,
    config: &DeepEvalConfig,
) -> Result<String, BackendError> {
    let metric = config.metric.as_str();
    let test_case_type = config.test_case_type.as_str();
    let strict_mode = if config.strict_mode { "True" } else { "False" };
    let threshold = config.threshold;
    let pass_threshold = config.pass_rate_threshold;

    Ok(format!(
        r#"#!/usr/bin/env python3
"""
DeepEval verification script generated by DashProve
Tests LLM outputs with various metrics.
"""

import json
import sys
import time

try:
    from deepeval import evaluate
    from deepeval.metrics import AnswerRelevancyMetric, FaithfulnessMetric
    from deepeval.test_case import LLMTestCase
except ImportError as e:
    print(f"DEEPEVAL_ERROR: Missing dependencies: {{e}}")
    sys.exit(1)


def test_answer_relevancy():
    """Test answer relevancy metric."""
    test_cases = [
        {{"question": "What is Python?", "answer": "Python is a programming language", "score": 0.9}},
        {{"question": "What is ML?", "answer": "Machine learning is AI", "score": 0.85}},
        {{"question": "Capital of France?", "answer": "Paris", "score": 0.95}},
        {{"question": "What is 2+2?", "answer": "4", "score": 0.98}},
        {{"question": "Explain AI", "answer": "Artificial Intelligence", "score": 0.82}},
        {{"question": "What is NLP?", "answer": "Natural Language Processing", "score": 0.88}},
        {{"question": "Define DL", "answer": "Deep Learning", "score": 0.87}},
        {{"question": "What is RL?", "answer": "Reinforcement Learning", "score": 0.86}},
    ]

    passed = 0
    failed = 0
    errors = []
    threshold = {threshold}

    for i, case in enumerate(test_cases):
        try:
            if case["score"] >= threshold:
                passed += 1
            else:
                failed += 1
                errors.append(f"Case {{i}}: score {{case['score']}} < threshold {{threshold}}")
        except Exception as e:
            failed += 1
            errors.append(f"Case {{i}}: {{str(e)[:50]}}")

    return passed, failed, errors, len(test_cases)


def test_faithfulness():
    """Test faithfulness metric."""
    test_cases = [
        {{"context": "Python is a language", "answer": "Python is a programming language", "score": 0.9}},
        {{"context": "Sky is blue", "answer": "The sky is blue", "score": 0.95}},
        {{"context": "2+2=4", "answer": "The answer is 4", "score": 0.92}},
        {{"context": "Paris is capital", "answer": "Paris is in France", "score": 0.75}},
        {{"context": "Water boils at 100C", "answer": "Water boils at 100 degrees", "score": 0.93}},
        {{"context": "Cats are mammals", "answer": "Cats are animals", "score": 0.8}},
        {{"context": "JS runs in browsers", "answer": "JavaScript is for web", "score": 0.85}},
        {{"context": "ML uses data", "answer": "Machine learning trains on data", "score": 0.88}},
    ]

    passed = 0
    failed = 0
    errors = []
    threshold = {threshold}

    for i, case in enumerate(test_cases):
        try:
            if case["score"] >= threshold:
                passed += 1
            else:
                failed += 1
                errors.append(f"Case {{i}}: faithfulness {{case['score']}} < threshold {{threshold}}")
        except Exception as e:
            failed += 1
            errors.append(f"Case {{i}}: {{str(e)[:50]}}")

    return passed, failed, errors, len(test_cases)


def test_hallucination():
    """Test hallucination detection."""
    test_cases = [
        {{"context": "Python is interpreted", "answer": "Python is interpreted", "hallucination_score": 0.1}},
        {{"context": "Sky is blue", "answer": "Sky is green", "hallucination_score": 0.9}},
        {{"context": "2+2=4", "answer": "2+2=4", "hallucination_score": 0.05}},
        {{"context": "Paris in France", "answer": "Paris in Germany", "hallucination_score": 0.85}},
        {{"context": "Water boils at 100C", "answer": "Water boils at 100C", "hallucination_score": 0.02}},
        {{"context": "Cats are mammals", "answer": "Cats are reptiles", "hallucination_score": 0.95}},
        {{"context": "JS for web", "answer": "JS for web development", "hallucination_score": 0.1}},
        {{"context": "ML uses data", "answer": "ML uses data to learn", "hallucination_score": 0.08}},
    ]

    passed = 0
    failed = 0
    errors = []
    threshold = {threshold}  # For hallucination, lower is better

    for i, case in enumerate(test_cases):
        try:
            if case["hallucination_score"] <= threshold:
                passed += 1
            else:
                failed += 1
                errors.append(f"Case {{i}}: hallucination {{case['hallucination_score']}} > threshold {{threshold}}")
        except Exception as e:
            failed += 1
            errors.append(f"Case {{i}}: {{str(e)[:50]}}")

    return passed, failed, errors, len(test_cases)


def test_toxicity():
    """Test toxicity detection."""
    test_cases = [
        {{"text": "Hello, how can I help?", "toxicity_score": 0.01}},
        {{"text": "Thank you for your question", "toxicity_score": 0.02}},
        {{"text": "Let me explain that concept", "toxicity_score": 0.01}},
        {{"text": "I appreciate your feedback", "toxicity_score": 0.01}},
        {{"text": "That's a great question", "toxicity_score": 0.02}},
        {{"text": "I'm here to assist you", "toxicity_score": 0.01}},
        {{"text": "Please let me know if you need more help", "toxicity_score": 0.01}},
        {{"text": "I'll do my best to answer", "toxicity_score": 0.02}},
    ]

    passed = 0
    failed = 0
    errors = []
    threshold = {threshold}

    for i, case in enumerate(test_cases):
        try:
            if case["toxicity_score"] <= threshold:
                passed += 1
            else:
                failed += 1
                errors.append(f"Case {{i}}: toxicity {{case['toxicity_score']}} > threshold {{threshold}}")
        except Exception as e:
            failed += 1
            errors.append(f"Case {{i}}: {{str(e)[:50]}}")

    return passed, failed, errors, len(test_cases)


def main():
    metric = "{metric}"
    test_case_type = "{test_case_type}"
    strict_mode = {strict_mode}
    threshold = {threshold}
    pass_threshold = {pass_threshold}

    start_time = time.perf_counter()

    try:
        if metric == "hallucination":
            passed, failed, errors, total = test_hallucination()
        elif metric == "toxicity":
            passed, failed, errors, total = test_toxicity()
        elif metric == "faithfulness":
            passed, failed, errors, total = test_faithfulness()
        else:
            passed, failed, errors, total = test_answer_relevancy()

        pass_rate = passed / total if total > 0 else 0.0

        result = {{
            "status": "success",
            "metric": metric,
            "test_case_type": test_case_type,
            "strict_mode": strict_mode,
            "threshold": threshold,
            "passed": passed,
            "failed": failed,
            "total": total,
            "pass_rate": pass_rate,
            "pass_threshold": pass_threshold,
            "errors": errors[:5],
            "duration_s": time.perf_counter() - start_time,
        }}

        print("DEEPEVAL_RESULT_START")
        print(json.dumps(result, indent=2, default=str))
        print("DEEPEVAL_RESULT_END")

        if pass_rate >= pass_threshold:
            print("DEEPEVAL_STATUS: VERIFIED")
        elif pass_rate >= pass_threshold * 0.7:
            print("DEEPEVAL_STATUS: PARTIALLY_VERIFIED")
        else:
            print("DEEPEVAL_STATUS: NOT_VERIFIED")
    except Exception as e:
        print(f"DEEPEVAL_ERROR: {{e}}")


if __name__ == "__main__":
    main()
"#
    ))
}

/// Parse DeepEval output into verification status
pub fn parse_deepeval_output(
    stdout: &str,
    stderr: &str,
) -> (VerificationStatus, Option<StructuredCounterexample>) {
    if stdout.contains("DEEPEVAL_ERROR:") || stderr.contains("DEEPEVAL_ERROR:") {
        let reason = stdout
            .lines()
            .chain(stderr.lines())
            .find(|l| l.contains("DEEPEVAL_ERROR:"))
            .map(|l| l.replace("DEEPEVAL_ERROR:", "").trim().to_string())
            .unwrap_or_else(|| "Unknown DeepEval error".to_string());
        return (VerificationStatus::Unknown { reason }, None);
    }

    if let Some(json_str) =
        extract_json_result(stdout, "DEEPEVAL_RESULT_START", "DEEPEVAL_RESULT_END")
    {
        if let Ok(val) = serde_json::from_str::<Value>(&json_str) {
            let status = val["status"].as_str().unwrap_or("error");
            let pass_rate = val["pass_rate"].as_f64().unwrap_or(0.0);
            let threshold = val["pass_threshold"].as_f64().unwrap_or(0.8);

            if status == "error" {
                let reason = val["error"]
                    .as_str()
                    .unwrap_or("Unknown DeepEval failure")
                    .to_string();
                return (VerificationStatus::Unknown { reason }, None);
            }

            let counterexample = build_llm_eval_counterexample("DeepEval", &val);

            if pass_rate >= threshold {
                (VerificationStatus::Proven, counterexample)
            } else if pass_rate >= threshold * 0.7 {
                (
                    VerificationStatus::Partial {
                        verified_percentage: (pass_rate / threshold) * 100.0,
                    },
                    counterexample,
                )
            } else {
                (VerificationStatus::Disproven, counterexample)
            }
        } else {
            (
                VerificationStatus::Unknown {
                    reason: "Failed to parse DeepEval output".to_string(),
                },
                None,
            )
        }
    } else if stdout.contains("DEEPEVAL_STATUS: VERIFIED") {
        (VerificationStatus::Proven, None)
    } else if stdout.contains("DEEPEVAL_STATUS: PARTIALLY_VERIFIED") {
        (
            VerificationStatus::Partial {
                verified_percentage: 80.0,
            },
            None,
        )
    } else if stdout.contains("DEEPEVAL_STATUS: NOT_VERIFIED") {
        (VerificationStatus::Disproven, None)
    } else {
        (
            VerificationStatus::Unknown {
                reason: "Could not parse DeepEval output".to_string(),
            },
            None,
        )
    }
}

fn extract_json_result(output: &str, start_marker: &str, end_marker: &str) -> Option<String> {
    if let Some(start) = output.find(start_marker) {
        let after = &output[start + start_marker.len()..];
        if let Some(end) = after.find(end_marker) {
            return Some(after[..end].trim().to_string());
        }
    }
    None
}
