{
  "id": "factscore",
  "name": "FActScore",
  "category": "llm",
  "subcategory": "evaluation",
  "description": "Fine-grained atomic factuality evaluation for LLM generations",
  "long_description": "FActScore (Factual Precision in Atomicity Score) is a metric for evaluating the factual accuracy of LLM-generated text. It breaks down generated text into atomic facts and verifies each against a knowledge source. Developed by researchers at UW and AI2.",
  "capabilities": [
    "atomic_fact_extraction",
    "fact_verification",
    "factuality_scoring",
    "knowledge_source_comparison",
    "fine_grained_evaluation",
    "biography_evaluation"
  ],
  "property_types": [
    "factuality",
    "hallucination_detection"
  ],
  "input_languages": [
    "python"
  ],
  "output_formats": [
    "factscore",
    "atomic_facts",
    "verification_results"
  ],
  "installation": {
    "methods": [
      {
        "type": "pip",
        "command": "pip install factscore"
      },
      {
        "type": "source",
        "url": "https://github.com/shmsw25/FActScore"
      }
    ],
    "dependencies": [
      "transformers",
      "torch",
      "spacy"
    ],
    "platforms": [
      "linux",
      "macos"
    ]
  },
  "documentation": {
    "official": "https://github.com/shmsw25/FActScore",
    "tutorial": "https://github.com/shmsw25/FActScore/blob/main/README.md",
    "api_reference": "https://github.com/shmsw25/FActScore",
    "examples": "https://github.com/shmsw25/FActScore/tree/main/examples"
  },
  "tactics": [
    {
      "name": "compute_score",
      "description": "Calculate FActScore for text",
      "syntax": "fs.get_score(topics, generations, knowledge_source)",
      "when_to_use": "Evaluating factual accuracy",
      "examples": [
        "score = fs.get_score(['Einstein'], [bio_text], gamma=10)"
      ]
    },
    {
      "name": "extract_facts",
      "description": "Extract atomic facts from text",
      "syntax": "fs.extract_atomic_facts(text)",
      "when_to_use": "Breaking down into verifiable units",
      "examples": [
        "facts = fs.extract_atomic_facts(generation)"
      ]
    },
    {
      "name": "verify_fact",
      "description": "Verify single fact",
      "syntax": "fs.verify_fact(fact, knowledge_source)",
      "when_to_use": "Checking individual claims",
      "examples": [
        "is_true = fs.verify_fact('Einstein was born in 1879', wiki)"
      ]
    }
  ],
  "error_patterns": [
    {
      "pattern": "Low FActScore",
      "meaning": "Text contains many false claims",
      "common_causes": [
        "Hallucination",
        "Outdated knowledge"
      ],
      "fixes": [
        "Improve retrieval",
        "Add citations"
      ]
    },
    {
      "pattern": "Extraction failed",
      "meaning": "Could not parse atomic facts",
      "common_causes": [
        "Unusual text format",
        "Very short text"
      ],
      "fixes": [
        "Preprocess text",
        "Check input format"
      ]
    }
  ],
  "integration": {
    "dashprove_backend": true,
    "usl_property_types": [
      "hallucination_detection"
    ],
    "cli_command": "dashprove verify --backend factscore"
  },
  "performance": {
    "typical_runtime": "Seconds per generation",
    "scalability": "Moderate (fact verification)",
    "memory_usage": "Moderate"
  },
  "comparisons": {
    "similar_tools": [
      "selfcheckgpt",
      "truthfulqa",
      "fever"
    ],
    "advantages": [
      "Fine-grained fact-level evaluation",
      "Research-backed methodology",
      "Clear atomic decomposition"
    ],
    "disadvantages": [
      "Requires knowledge source",
      "Slower than simpler metrics",
      "Research focus"
    ]
  },
  "metadata": {
    "version": "1.0",
    "last_updated": "2025-12-20",
    "maintainer": "UW NLP / AI2",
    "license": "Apache-2.0"
  }
}
