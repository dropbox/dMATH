{
  "id": "selfcheckgpt",
  "name": "SelfCheckGPT",
  "category": "llm_hallucination",
  "subcategory": "detection",

  "description": "Zero-resource black-box hallucination detection",
  "long_description": "SelfCheckGPT is a method for detecting LLM hallucinations without external knowledge bases. It works by sampling multiple responses from the LLM and checking consistency - facts that appear consistently across samples are likely true, while inconsistent claims are potential hallucinations.",

  "capabilities": [
    "hallucination_detection",
    "consistency_checking",
    "zero_resource",
    "black_box",
    "sentence_level",
    "passage_level"
  ],
  "property_types": ["consistency", "factuality"],
  "input_languages": ["python"],
  "output_formats": ["score", "analysis"],

  "installation": {
    "methods": [
      {"type": "pip", "command": "pip install selfcheckgpt"}
    ],
    "dependencies": ["transformers", "spacy"],
    "platforms": ["linux", "macos", "windows"]
  },

  "documentation": {
    "official": "https://github.com/potsawee/selfcheckgpt",
    "tutorial": "https://github.com/potsawee/selfcheckgpt#usage",
    "api_reference": "https://github.com/potsawee/selfcheckgpt",
    "examples": "https://github.com/potsawee/selfcheckgpt/tree/main/demo"
  },

  "tactics": [
    {
      "name": "BERTScore",
      "description": "BERT-based similarity check",
      "syntax": "selfcheck.predict(sentences, samples, method='bertscore')",
      "when_to_use": "For semantic similarity checking",
      "examples": ["scores = selfcheck_bertscore.predict(sentences, sampled_passages)"]
    },
    {
      "name": "NLI",
      "description": "Natural language inference",
      "syntax": "selfcheck.predict(sentences, samples, method='nli')",
      "when_to_use": "For entailment-based checking",
      "examples": ["scores = selfcheck_nli.predict(sentences, sampled_passages)"]
    },
    {
      "name": "LLMPrompt",
      "description": "LLM-as-judge checking",
      "syntax": "SelfCheckLLMPrompt()",
      "when_to_use": "For GPT-based consistency check",
      "examples": ["selfcheck_prompt = SelfCheckLLMPrompt()\nscores = selfcheck_prompt.predict(...)"]
    }
  ],

  "error_patterns": [
    {
      "pattern": "Low consistency",
      "meaning": "Potential hallucination",
      "common_causes": ["Factual error", "Uncertain knowledge"],
      "fixes": ["Verify with external source", "Use retrieval augmentation"]
    },
    {
      "pattern": "Model error",
      "meaning": "Underlying model failed",
      "common_causes": ["GPU OOM", "Invalid input"],
      "fixes": ["Reduce batch size", "Check input format"]
    }
  ],

  "integration": {
    "dashprove_backend": true,
    "usl_property_types": ["factuality", "consistency"],
    "cli_command": "dashprove verify --backend selfcheckgpt"
  },

  "performance": {
    "typical_runtime": "seconds per passage",
    "scalability": "Requires multiple LLM samples",
    "memory_usage": "Depends on NLI model"
  },

  "comparisons": {
    "similar_tools": ["factscore", "chainpoll", "trulens"],
    "advantages": [
      "No external knowledge needed",
      "Works with any LLM",
      "Multiple checking methods"
    ],
    "disadvantages": [
      "Requires multiple samples",
      "Can't catch consistent hallucinations"
    ]
  },

  "metadata": {
    "version": "0.1.0",
    "last_updated": "2025-12-20",
    "maintainer": "Cambridge NLP",
    "license": "MIT"
  }
}
