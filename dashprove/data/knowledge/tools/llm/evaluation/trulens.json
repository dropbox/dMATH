{
  "id": "trulens",
  "name": "TruLens",
  "category": "llm_evaluation",
  "subcategory": "quality_assessment",
  "description": "Evaluation and tracking for LLM applications",
  "long_description": "TruLens is a library for evaluating and tracking LLM applications. It provides feedback functions to measure qualities like groundedness, relevance, and harmfulness. TruLens integrates with LangChain and LlamaIndex, and includes a dashboard for tracking experiments.",
  "capabilities": [
    "groundedness_evaluation",
    "relevance_scoring",
    "harmfulness_detection",
    "rag_evaluation",
    "chain_tracing",
    "feedback_functions",
    "experiment_tracking",
    "dashboard"
  ],
  "property_types": [
    "llm_quality",
    "rag_quality"
  ],
  "input_languages": [
    "python",
    "langchain",
    "llamaindex"
  ],
  "output_formats": [
    "scores",
    "traces",
    "dashboard"
  ],
  "installation": {
    "methods": [
      {
        "type": "pip",
        "command": "pip install trulens-eval"
      },
      {
        "type": "source",
        "url": "https://github.com/truera/trulens"
      }
    ],
    "dependencies": [
      "python",
      "openai/anthropic",
      "langchain/llamaindex"
    ],
    "platforms": [
      "linux",
      "macos",
      "windows"
    ]
  },
  "documentation": {
    "official": "https://www.trulens.org/",
    "tutorial": "https://www.trulens.org/trulens_eval/getting_started/",
    "api_reference": "https://www.trulens.org/trulens_eval/api/",
    "examples": "https://github.com/truera/trulens/tree/main/trulens_eval/examples"
  },
  "tactics": [
    {
      "name": "groundedness",
      "description": "Check if response is grounded in context",
      "syntax": "Groundedness().groundedness_with_cot_reasons()",
      "when_to_use": "RAG applications to verify sourcing",
      "examples": [
        "f_groundedness = Groundedness().groundedness_with_cot_reasons()\nrecorder = TruChain(chain, feedbacks=[f_groundedness])"
      ]
    },
    {
      "name": "relevance",
      "description": "Check answer relevance to question",
      "syntax": "Relevance().relevance()",
      "when_to_use": "Verify response answers the question",
      "examples": [
        "f_relevance = Relevance().relevance()\nrecorder = TruChain(chain, feedbacks=[f_relevance])"
      ]
    },
    {
      "name": "context_relevance",
      "description": "Check retrieved context relevance",
      "syntax": "ContextRelevance().qs_relevance()",
      "when_to_use": "Verify retrieval quality in RAG",
      "examples": [
        "f_context = ContextRelevance().qs_relevance()"
      ]
    },
    {
      "name": "dashboard",
      "description": "Launch evaluation dashboard",
      "syntax": "Tru().run_dashboard()",
      "when_to_use": "Visualizing evaluation results",
      "examples": [
        "from trulens_eval import Tru\nTru().run_dashboard()"
      ]
    }
  ],
  "error_patterns": [
    {
      "pattern": "Low groundedness score",
      "meaning": "Response not well supported by context",
      "common_causes": [
        "Hallucination",
        "Missing context",
        "Poor retrieval"
      ],
      "fixes": [
        "Improve retrieval",
        "Add constraints to prompt"
      ]
    },
    {
      "pattern": "Low relevance score",
      "meaning": "Response doesn't address question",
      "common_causes": [
        "Off-topic response",
        "Misunderstood question"
      ],
      "fixes": [
        "Improve prompt",
        "Add examples"
      ]
    }
  ],
  "integration": {
    "dashprove_backend": true,
    "usl_property_types": [
      "llm_quality"
    ],
    "cli_command": "dashprove verify --backend trulens"
  },
  "performance": {
    "typical_runtime": "Depends on LLM calls for evaluation",
    "scalability": "Async evaluation supported",
    "memory_usage": "Moderate"
  },
  "comparisons": {
    "similar_tools": [
      "ragas",
      "langsmith",
      "deepeval"
    ],
    "advantages": [
      "Good RAG evaluation",
      "Dashboard included",
      "Chain tracing",
      "Multiple feedback types"
    ],
    "disadvantages": [
      "Requires LLM for evaluation",
      "Cost of evaluation calls"
    ]
  },
  "metadata": {
    "version": "0.26.0",
    "last_updated": "2025-12-20",
    "maintainer": "TruEra",
    "license": "MIT"
  }
}
