{
  "id": "langsmith",
  "name": "LangSmith",
  "category": "llm",
  "subcategory": "evaluation",
  "description": "Platform for LLM application development, debugging, and monitoring",
  "long_description": "LangSmith is LangChain's platform for debugging, testing, evaluating, and monitoring LLM applications. It provides tracing for LLM calls, dataset management for evaluation, and tools for comparing model outputs. Essential for production LLM application development.",
  "capabilities": [
    "llm_tracing",
    "run_evaluation",
    "dataset_management",
    "prompt_versioning",
    "feedback_collection",
    "a_b_testing",
    "collaboration",
    "production_monitoring"
  ],
  "property_types": [
    "llm_evaluation",
    "tracing",
    "quality"
  ],
  "input_languages": [
    "python",
    "typescript"
  ],
  "output_formats": [
    "traces",
    "evaluation_results",
    "dashboard"
  ],
  "installation": {
    "methods": [
      {
        "type": "pip",
        "command": "pip install langsmith"
      },
      {
        "type": "npm",
        "command": "npm install langsmith"
      }
    ],
    "dependencies": [
      "langchain"
    ],
    "platforms": [
      "linux",
      "macos",
      "windows"
    ]
  },
  "documentation": {
    "official": "https://docs.smith.langchain.com/",
    "tutorial": "https://docs.smith.langchain.com/tutorials/",
    "api_reference": "https://docs.smith.langchain.com/reference/",
    "examples": "https://github.com/langchain-ai/langsmith-cookbook"
  },
  "tactics": [
    {
      "name": "trace",
      "description": "Enable tracing for LLM calls",
      "syntax": "@traceable def my_func():",
      "when_to_use": "Debugging LLM applications",
      "examples": [
        "@traceable\\ndef call_llm(prompt): return llm.invoke(prompt)"
      ]
    },
    {
      "name": "evaluate",
      "description": "Run evaluation on dataset",
      "syntax": "evaluate(predict_fn, data=dataset, evaluators=[...])",
      "when_to_use": "Testing LLM performance",
      "examples": [
        "evaluate(chain.invoke, data='my_dataset', evaluators=[qa_evaluator])"
      ]
    },
    {
      "name": "create_dataset",
      "description": "Create evaluation dataset",
      "syntax": "client.create_dataset(name, description)",
      "when_to_use": "Setting up evaluation data",
      "examples": [
        "ds = client.create_dataset('test_set', 'QA examples')"
      ]
    },
    {
      "name": "feedback",
      "description": "Add feedback to run",
      "syntax": "client.create_feedback(run_id, key, score)",
      "when_to_use": "Collecting human feedback",
      "examples": [
        "client.create_feedback(run.id, 'correctness', score=1)"
      ]
    }
  ],
  "error_patterns": [
    {
      "pattern": "API key not found",
      "meaning": "LangSmith credentials missing",
      "common_causes": [
        "Missing env var",
        "Invalid key"
      ],
      "fixes": [
        "Set LANGCHAIN_API_KEY",
        "Check key validity"
      ]
    },
    {
      "pattern": "Dataset not found",
      "meaning": "Referenced dataset doesn't exist",
      "common_causes": [
        "Wrong name",
        "Not created"
      ],
      "fixes": [
        "Create dataset first",
        "Check name"
      ]
    }
  ],
  "integration": {
    "dashprove_backend": true,
    "usl_property_types": [
      "llm_evaluation"
    ],
    "cli_command": "dashprove verify --backend langsmith"
  },
  "performance": {
    "typical_runtime": "Seconds per trace",
    "scalability": "Good for production",
    "memory_usage": "Low (cloud-based)"
  },
  "comparisons": {
    "similar_tools": [
      "trulens",
      "promptfoo",
      "weights_biases"
    ],
    "advantages": [
      "Excellent LangChain integration",
      "Good UI/UX",
      "Production monitoring",
      "Active development"
    ],
    "disadvantages": [
      "LangChain-focused",
      "Requires cloud service"
    ]
  },
  "metadata": {
    "version": "0.1.0",
    "last_updated": "2025-12-20",
    "maintainer": "LangChain",
    "license": "MIT"
  }
}
