{
  "id": "iree",
  "name": "IREE",
  "category": "ai_ml",
  "subcategory": "inference_optimization",
  "description": "MLIR-based end-to-end compiler and runtime for ML models",
  "long_description": "IREE (Intermediate Representation Execution Environment) is an MLIR-based end-to-end compiler and runtime that lowers ML models to unified IR for execution on various hardware targets. Developed by Google, it focuses on efficient scheduling, memory planning, and supports CPUs, GPUs, and accelerators.",
  "capabilities": [
    "mlir_compilation",
    "unified_ir",
    "vulkan_backend",
    "cuda_backend",
    "cpu_backend",
    "dynamic_shapes",
    "async_execution",
    "model_streaming"
  ],
  "property_types": [
    "performance",
    "portability"
  ],
  "input_languages": [
    "tensorflow",
    "pytorch",
    "jax",
    "tflite"
  ],
  "output_formats": [
    "vmfb",
    "inference_result",
    "benchmark"
  ],
  "installation": {
    "methods": [
      {
        "type": "pip",
        "command": "pip install iree-compiler iree-runtime"
      },
      {
        "type": "source",
        "url": "https://github.com/openxla/iree"
      }
    ],
    "dependencies": [
      "cmake",
      "python"
    ],
    "platforms": [
      "linux",
      "macos",
      "windows"
    ]
  },
  "documentation": {
    "official": "https://iree.dev/",
    "tutorial": "https://iree.dev/guides/",
    "api_reference": "https://iree.dev/reference/",
    "examples": "https://github.com/openxla/iree/tree/main/samples"
  },
  "tactics": [
    {
      "name": "compile",
      "description": "Compile model to IREE module",
      "syntax": "iree-compile model.mlir -o model.vmfb",
      "when_to_use": "Preparing model for deployment",
      "examples": [
        "iree-compile model.mlir --iree-hal-target-backends=vulkan-spirv -o model.vmfb"
      ]
    },
    {
      "name": "run",
      "description": "Run compiled module",
      "syntax": "iree-run-module --module=model.vmfb",
      "when_to_use": "Executing inference",
      "examples": [
        "iree-run-module --module=model.vmfb --input='1x224x224x3xf32'"
      ]
    },
    {
      "name": "benchmark",
      "description": "Benchmark module performance",
      "syntax": "iree-benchmark-module --module=model.vmfb",
      "when_to_use": "Measuring latency",
      "examples": [
        "iree-benchmark-module --module=model.vmfb --benchmark_repetitions=100"
      ]
    }
  ],
  "error_patterns": [
    {
      "pattern": "Compilation failed",
      "meaning": "Could not compile model",
      "common_causes": [
        "Unsupported op",
        "Shape issues"
      ],
      "fixes": [
        "Check supported ops",
        "Simplify model"
      ]
    },
    {
      "pattern": "HAL error",
      "meaning": "Hardware abstraction layer issue",
      "common_causes": [
        "Missing driver",
        "Incompatible target"
      ],
      "fixes": [
        "Install drivers",
        "Check target compatibility"
      ]
    }
  ],
  "integration": {
    "dashprove_backend": true,
    "usl_property_types": [
      "model_optimization"
    ],
    "cli_command": "dashprove verify --backend iree"
  },
  "performance": {
    "typical_runtime": "Seconds for compile, milliseconds for inference",
    "scalability": "Good across hardware",
    "memory_usage": "Efficient"
  },
  "comparisons": {
    "similar_tools": [
      "tvm",
      "xla",
      "onnxruntime"
    ],
    "advantages": [
      "Modern MLIR-based design",
      "Good dynamic shape support",
      "Efficient scheduling",
      "Active Google development"
    ],
    "disadvantages": [
      "Newer project",
      "Smaller ecosystem",
      "Still maturing"
    ]
  },
  "metadata": {
    "version": "2.7.0",
    "last_updated": "2025-12-20",
    "maintainer": "Google (OpenXLA)",
    "license": "Apache-2.0"
  }
}
