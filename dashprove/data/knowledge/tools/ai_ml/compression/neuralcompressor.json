{
  "id": "neuralcompressor",
  "name": "Intel Neural Compressor",
  "category": "ai_ml",
  "subcategory": "model_compression",
  "description": "Intel toolkit for neural network quantization and compression",
  "long_description": "Intel Neural Compressor is an open-source Python library for model compression with minimal accuracy loss. It supports various compression techniques including quantization (static, dynamic, aware training), pruning, and distillation. Works with PyTorch, TensorFlow, and ONNX models.",
  "capabilities": [
    "post_training_quantization",
    "quantization_aware_training",
    "pruning",
    "knowledge_distillation",
    "mixed_precision",
    "accuracy_aware_tuning",
    "benchmark_integration"
  ],
  "property_types": [
    "accuracy",
    "compression_ratio",
    "performance"
  ],
  "input_languages": [
    "pytorch",
    "tensorflow",
    "onnx"
  ],
  "output_formats": [
    "quantized_model",
    "pruned_model",
    "benchmark"
  ],
  "installation": {
    "methods": [
      {
        "type": "pip",
        "command": "pip install neural-compressor"
      },
      {
        "type": "source",
        "url": "https://github.com/intel/neural-compressor"
      }
    ],
    "dependencies": [
      "pytorch",
      "tensorflow",
      "onnx"
    ],
    "platforms": [
      "linux",
      "macos",
      "windows"
    ]
  },
  "documentation": {
    "official": "https://raw.githubusercontent.com/intel/neural-compressor/master/README.md",
    "tutorial": "https://raw.githubusercontent.com/intel/neural-compressor/master/docs/source/get_started.md",
    "api_reference": "https://raw.githubusercontent.com/intel/neural-compressor/master/docs/source/metric.md",
    "examples": "https://github.com/intel/neural-compressor/tree/master/examples"
  },
  "tactics": [
    {
      "name": "quantize",
      "description": "Apply post-training quantization",
      "syntax": "quantize(model, dataloader, config)",
      "when_to_use": "Quick model compression",
      "examples": [
        "q_model = quantize(model, calib_dataloader, quant_config)"
      ]
    },
    {
      "name": "prune",
      "description": "Apply pruning",
      "syntax": "pruning.prepare(model, config)",
      "when_to_use": "Reducing model size via sparsity",
      "examples": [
        "pruned_model = prune(model, config, train_func)"
      ]
    },
    {
      "name": "distill",
      "description": "Apply knowledge distillation",
      "syntax": "distillation.prepare(student, teacher, config)",
      "when_to_use": "Training smaller model from larger",
      "examples": [
        "distiller = Distillation(model, teacher_model); distiller.fit()"
      ]
    },
    {
      "name": "autotune",
      "description": "Accuracy-aware auto-tuning",
      "syntax": "quantize(model, dataloader, config, accuracy_criterion)",
      "when_to_use": "Finding best compression with accuracy target",
      "examples": [
        "q_model = quantize(model, dl, config, acc_criterion=0.01)"
      ]
    }
  ],
  "error_patterns": [
    {
      "pattern": "Accuracy drop too large",
      "meaning": "Quantization degraded accuracy beyond threshold",
      "common_causes": [
        "Sensitive layers",
        "Poor calibration"
      ],
      "fixes": [
        "Adjust config",
        "Use mixed precision",
        "More calibration data"
      ]
    },
    {
      "pattern": "Op not supported",
      "meaning": "Operator cannot be quantized",
      "common_causes": [
        "Custom op",
        "Unsupported type"
      ],
      "fixes": [
        "Fallback to FP32",
        "Register custom quantization"
      ]
    }
  ],
  "integration": {
    "dashprove_backend": true,
    "usl_property_types": [
      "model_compression"
    ],
    "cli_command": "dashprove verify --backend neuralcompressor"
  },
  "performance": {
    "typical_runtime": "Minutes to hours for tuning",
    "scalability": "Good for various model sizes",
    "memory_usage": "Reduced after compression"
  },
  "comparisons": {
    "similar_tools": [
      "nncf",
      "aimet",
      "brevitas"
    ],
    "advantages": [
      "Intel hardware optimized",
      "Multiple compression techniques",
      "Accuracy-aware tuning",
      "Good framework support"
    ],
    "disadvantages": [
      "Best on Intel hardware",
      "Can be complex to configure"
    ]
  },
  "metadata": {
    "version": "2.5",
    "last_updated": "2025-12-20",
    "maintainer": "Intel",
    "license": "Apache-2.0"
  }
}
