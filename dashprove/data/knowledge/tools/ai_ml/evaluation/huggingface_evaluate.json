{
  "id": "huggingface_evaluate",
  "name": "Hugging Face Evaluate",
  "category": "ml_evaluation",
  "subcategory": "metrics",

  "description": "Unified evaluation library for ML models",
  "long_description": "Hugging Face Evaluate is a library for easily evaluating ML models and datasets. It provides access to 100+ evaluation metrics (accuracy, BLEU, ROUGE, BERTScore, etc.) with a unified API, making it easy to compare models across different tasks.",

  "capabilities": [
    "metric_computation",
    "comparison_evaluation",
    "distributed_evaluation",
    "custom_metrics",
    "dataset_metrics",
    "measurement"
  ],
  "property_types": ["accuracy", "f1", "bleu", "rouge", "bertscore"],
  "input_languages": ["python"],
  "output_formats": ["dict", "json"],

  "installation": {
    "methods": [
      {"type": "pip", "command": "pip install evaluate"},
      {"type": "pip_extras", "command": "pip install evaluate[evaluator]"}
    ],
    "dependencies": ["datasets", "numpy"],
    "platforms": ["linux", "macos", "windows"]
  },

  "documentation": {
    "official": "https://huggingface.co/docs/evaluate/",
    "tutorial": "https://huggingface.co/docs/evaluate/a_quick_tour",
    "api_reference": "https://huggingface.co/docs/evaluate/package_reference/main_classes",
    "examples": "https://huggingface.co/docs/evaluate/transformers_integrations"
  },

  "tactics": [
    {
      "name": "load_metric",
      "description": "Load evaluation metric",
      "syntax": "evaluate.load('metric_name')",
      "when_to_use": "To use standard metrics",
      "examples": ["import evaluate\n\naccuracy = evaluate.load('accuracy')\nresult = accuracy.compute(\n    predictions=[0, 1, 1],\n    references=[0, 1, 0]\n)"]
    },
    {
      "name": "combine_metrics",
      "description": "Use multiple metrics",
      "syntax": "evaluate.combine(['m1', 'm2'])",
      "when_to_use": "For comprehensive evaluation",
      "examples": ["clf_metrics = evaluate.combine(['accuracy', 'f1', 'precision', 'recall'])\nresult = clf_metrics.compute(\n    predictions=preds,\n    references=labels\n)"]
    },
    {
      "name": "evaluator",
      "description": "End-to-end evaluation",
      "syntax": "evaluator.compute(model, data)",
      "when_to_use": "For full pipeline evaluation",
      "examples": ["from evaluate import evaluator\ntask_evaluator = evaluator('text-classification')\nresult = task_evaluator.compute(\n    model_or_pipeline='model_name',\n    data='dataset_name'\n)"]
    },
    {
      "name": "text_generation",
      "description": "Evaluate text generation",
      "syntax": "evaluate.load('bleu/rouge/bertscore')",
      "when_to_use": "For NLG tasks",
      "examples": ["bleu = evaluate.load('bleu')\nresult = bleu.compute(\n    predictions=['the cat sat on mat'],\n    references=[['the cat sat on the mat']]\n)"]
    }
  ],

  "error_patterns": [
    {
      "pattern": "Metric.*not found",
      "meaning": "Invalid metric name",
      "common_causes": ["Typo", "Metric not installed"],
      "fixes": ["Check metric list: evaluate.list_evaluation_modules()"]
    },
    {
      "pattern": "Shape mismatch",
      "meaning": "Input arrays wrong shape",
      "common_causes": ["Predictions/references length mismatch"],
      "fixes": ["Ensure same length arrays"]
    },
    {
      "pattern": "requires.*package",
      "meaning": "Missing dependency",
      "common_causes": ["Optional dependency not installed"],
      "fixes": ["Install required package: pip install <package>"]
    }
  ],

  "integration": {
    "dashprove_backend": true,
    "usl_property_types": ["accuracy", "f1", "bleu"],
    "cli_command": "dashprove verify --backend hf-evaluate"
  },

  "performance": {
    "typical_runtime": "Milliseconds to seconds",
    "scalability": "Good with batching",
    "memory_usage": "Low to medium"
  },

  "comparisons": {
    "similar_tools": ["torchmetrics", "sklearn.metrics", "nltk"],
    "advantages": [
      "Unified API",
      "100+ metrics",
      "HF ecosystem integration",
      "Distributed support",
      "Measurement tracking"
    ],
    "disadvantages": [
      "Some metrics slow",
      "HF dependency",
      "Less granular control"
    ]
  },

  "metadata": {
    "version": "0.4.0",
    "last_updated": "2025-12-22",
    "maintainer": "Hugging Face",
    "license": "Apache-2.0"
  }
}
