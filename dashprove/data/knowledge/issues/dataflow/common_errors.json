{
  "tool": "dataflow",
  "category": "gcp_data",
  "description": "Google Cloud Dataflow - unified stream and batch data processing",
  "common_errors": [
    {
      "id": "permission_denied",
      "pattern": "PermissionDenied",
      "regex": "(?i)permission.*denied|403|not.*authorized|iam.*error",
      "cause": "IAM permissions insufficient for Dataflow operation",
      "solution": "Grant roles/dataflow.worker to service account, add storage.objectAdmin for temp locations",
      "severity": "high",
      "category": "permissions"
    },
    {
      "id": "worker_startup",
      "pattern": "Worker failed to start",
      "regex": "(?i)worker.*failed|vm.*startup.*error|boot.*disk|compute.*error",
      "cause": "Dataflow worker VM failed to start",
      "solution": "Check Compute Engine quotas, verify network configuration, review worker machine type availability",
      "severity": "high",
      "category": "workers"
    },
    {
      "id": "pipeline_failed",
      "pattern": "Pipeline failed",
      "regex": "(?i)pipeline.*failed|job.*failed|stage.*failed|transform.*error",
      "cause": "Pipeline execution failed at specific stage",
      "solution": "Check job logs in Cloud Console, review failed step output, fix code/data issues",
      "severity": "high",
      "category": "execution"
    },
    {
      "id": "staging_error",
      "pattern": "Staging",
      "regex": "(?i)staging.*error|temp.*location|failed.*to.*stage|gcs.*staging",
      "cause": "Failed to stage files to GCS temp location",
      "solution": "Verify GCS bucket permissions, ensure bucket exists and is accessible, check network",
      "severity": "high",
      "category": "staging"
    },
    {
      "id": "quota_exceeded",
      "pattern": "Quota exceeded",
      "regex": "(?i)quota.*exceeded|cpu.*quota|ip.*quota|resource.*limit",
      "cause": "GCP quota exceeded for Dataflow resources",
      "solution": "Request quota increase, reduce max workers, use different region with capacity",
      "severity": "medium",
      "category": "limits"
    },
    {
      "id": "serialization_error",
      "pattern": "Serialization",
      "regex": "(?i)serialization.*error|cannot.*serialize|coder.*error|not.*serializable",
      "cause": "Object serialization failed for distributed processing",
      "solution": "Ensure all classes are Serializable, use Coders for custom types, avoid non-serializable closures",
      "severity": "high",
      "category": "code"
    },
    {
      "id": "oom_error",
      "pattern": "OutOfMemory",
      "regex": "(?i)out.*of.*memory|oom|java.*heap|memory.*exceeded",
      "cause": "Worker ran out of memory",
      "solution": "Increase worker memory, optimize pipeline to reduce memory usage, use larger machine types",
      "severity": "high",
      "category": "resources"
    },
    {
      "id": "streaming_error",
      "pattern": "Streaming",
      "regex": "(?i)watermark|late.*data|window.*error|trigger.*error|streaming.*insert",
      "cause": "Streaming pipeline windowing or watermark issue",
      "solution": "Review windowing strategy, configure allowed lateness, check watermark progression",
      "severity": "medium",
      "category": "streaming"
    },
    {
      "id": "bigquery_error",
      "pattern": "BigQuery",
      "regex": "(?i)bigquery.*error|bq.*write|table.*not.*found|schema.*mismatch",
      "cause": "BigQuery sink operation failed",
      "solution": "Verify table schema matches, check BigQuery permissions, review write disposition",
      "severity": "medium",
      "category": "sinks"
    },
    {
      "id": "network_error",
      "pattern": "Network",
      "regex": "(?i)network.*error|vpc.*error|subnetwork|private.*google.*access",
      "cause": "VPC network configuration issue",
      "solution": "Enable Private Google Access, configure subnetwork, review firewall rules",
      "severity": "high",
      "category": "networking"
    }
  ],
  "related_tools": ["gcloud", "bigquery", "pubsub", "cloud_storage"],
  "documentation_url": "https://cloud.google.com/dataflow/docs"
}
