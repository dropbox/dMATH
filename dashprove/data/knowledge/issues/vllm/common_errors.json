{
  "tool": "vllm",
  "category": "ai_serving",
  "common_errors": [
    {
      "id": "vllm_install",
      "pattern": "pip install vllm|installation error|CUDA",
      "severity": "error",
      "causes": [
        "CUDA version mismatch",
        "GPU not supported",
        "Build requirements"
      ],
      "solutions": [
        "Install: pip install vllm",
        "Requires CUDA 11.8+ and compatible GPU",
        "Check: nvidia-smi for CUDA version"
      ]
    },
    {
      "id": "vllm_gpu_memory",
      "pattern": "out of memory|CUDA OOM|gpu_memory_utilization",
      "severity": "error",
      "causes": [
        "Model too large for GPU",
        "Memory fragmentation",
        "Batch size too large"
      ],
      "solutions": [
        "Set gpu_memory_utilization=0.9 (default)",
        "Lower: gpu_memory_utilization=0.7",
        "Use tensor_parallel_size for multi-GPU"
      ]
    },
    {
      "id": "vllm_model_load",
      "pattern": "model not found|load model|HuggingFace",
      "severity": "error",
      "causes": [
        "Model name incorrect",
        "Network issues",
        "Authentication needed"
      ],
      "solutions": [
        "Use HuggingFace model ID: 'meta-llama/Llama-2-7b-chat-hf'",
        "Set HF_TOKEN for gated models",
        "Or use local path: '/path/to/model'"
      ]
    },
    {
      "id": "vllm_quantization",
      "pattern": "quantization|AWQ|GPTQ|load_in_8bit",
      "severity": "info",
      "causes": [
        "Reduce memory usage",
        "Faster inference",
        "Quantized model support"
      ],
      "solutions": [
        "AWQ: LLM(model, quantization='awq')",
        "GPTQ: LLM(model, quantization='gptq')",
        "Use pre-quantized models"
      ]
    },
    {
      "id": "vllm_server",
      "pattern": "serve|server|OpenAI compatible",
      "severity": "info",
      "causes": [
        "Running inference server",
        "API compatibility",
        "Deployment"
      ],
      "solutions": [
        "CLI: python -m vllm.entrypoints.openai.api_server --model model_id",
        "Port: --port 8000",
        "OpenAI-compatible API at /v1/completions"
      ]
    },
    {
      "id": "vllm_sampling",
      "pattern": "sampling|temperature|top_p",
      "severity": "info",
      "causes": [
        "Output quality",
        "Generation parameters",
        "Deterministic output"
      ],
      "solutions": [
        "SamplingParams(temperature=0.8, top_p=0.95)",
        "temperature=0 for deterministic",
        "max_tokens for length control"
      ]
    },
    {
      "id": "vllm_batch",
      "pattern": "batch|throughput|concurrent",
      "severity": "info",
      "causes": [
        "High throughput inference",
        "Batching requests",
        "Parallelism"
      ],
      "solutions": [
        "llm.generate(prompts) for batch",
        "Continuous batching automatic",
        "max_num_seqs for concurrent limit"
      ]
    },
    {
      "id": "vllm_tensor_parallel",
      "pattern": "tensor_parallel|multi-GPU|distribute",
      "severity": "info",
      "causes": [
        "Large models",
        "Multi-GPU setup",
        "Memory distribution"
      ],
      "solutions": [
        "LLM(model, tensor_parallel_size=2)",
        "Must have multiple GPUs",
        "Model distributed across GPUs"
      ]
    }
  ]
}
