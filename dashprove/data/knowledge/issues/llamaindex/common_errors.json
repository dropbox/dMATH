{
  "tool": "llamaindex",
  "category": "ai_framework",
  "common_errors": [
    {
      "id": "llamaindex_install",
      "pattern": "install llamaindex|llama_index|import error",
      "severity": "error",
      "causes": [
        "Package name confusion",
        "Missing dependencies",
        "Version mismatch"
      ],
      "solutions": [
        "Install: pip install llama-index",
        "Import: from llama_index.core import ...",
        "Integrations: pip install llama-index-llms-openai"
      ]
    },
    {
      "id": "llamaindex_api_key",
      "pattern": "API key|OPENAI_API_KEY|llm not configured",
      "severity": "error",
      "causes": [
        "Missing API key",
        "LLM not configured",
        "Default LLM issues"
      ],
      "solutions": [
        "Set OPENAI_API_KEY for default LLM",
        "Or explicitly: Settings.llm = OpenAI(api_key='...')",
        "Use local models with Ollama integration"
      ]
    },
    {
      "id": "llamaindex_settings",
      "pattern": "Settings|ServiceContext|global config",
      "severity": "warning",
      "causes": [
        "Old ServiceContext API",
        "Configuration changes",
        "Global vs local settings"
      ],
      "solutions": [
        "Use: from llama_index.core import Settings",
        "Settings.llm = your_llm",
        "Settings.embed_model = your_embed",
        "ServiceContext deprecated"
      ]
    },
    {
      "id": "llamaindex_documents",
      "pattern": "Document|load documents|SimpleDirectoryReader",
      "severity": "info",
      "causes": [
        "Loading data",
        "Document format",
        "Metadata handling"
      ],
      "solutions": [
        "SimpleDirectoryReader('path').load_data()",
        "Document(text='...', metadata={...})",
        "Various readers for different formats"
      ]
    },
    {
      "id": "llamaindex_index",
      "pattern": "VectorStoreIndex|index|create index",
      "severity": "info",
      "causes": [
        "Creating index",
        "Index types",
        "Persistence"
      ],
      "solutions": [
        "index = VectorStoreIndex.from_documents(docs)",
        "Persist: index.storage_context.persist()",
        "Load: load_index_from_storage(storage_context)"
      ]
    },
    {
      "id": "llamaindex_query_engine",
      "pattern": "query_engine|query|response",
      "severity": "info",
      "causes": [
        "Querying index",
        "Response modes",
        "Customization"
      ],
      "solutions": [
        "query_engine = index.as_query_engine()",
        "response = query_engine.query('question')",
        "Configure similarity_top_k, response_mode"
      ]
    },
    {
      "id": "llamaindex_embedding",
      "pattern": "embedding|embed_model|dimension mismatch",
      "severity": "error",
      "causes": [
        "Embedding model mismatch",
        "Dimension issues",
        "Model not found"
      ],
      "solutions": [
        "Settings.embed_model = OpenAIEmbedding()",
        "Local: HuggingFaceEmbedding(model_name='...')",
        "Match embedding dimensions with vector store"
      ]
    },
    {
      "id": "llamaindex_streaming",
      "pattern": "streaming|stream|real-time",
      "severity": "info",
      "causes": [
        "Streaming responses",
        "Long queries",
        "UI integration"
      ],
      "solutions": [
        "query_engine = index.as_query_engine(streaming=True)",
        "for token in response.response_gen:",
        "Works with chat engines too"
      ]
    }
  ]
}
