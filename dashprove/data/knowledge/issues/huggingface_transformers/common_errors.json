{
  "tool": "huggingface_transformers",
  "category": "ai_ml",
  "common_errors": [
    {
      "id": "transformers_install",
      "pattern": "pip install transformers|import error|version",
      "severity": "error",
      "causes": [
        "Package not installed",
        "Missing optional deps",
        "Version conflict"
      ],
      "solutions": [
        "Install: pip install transformers",
        "With PyTorch: pip install transformers[torch]",
        "With TensorFlow: pip install transformers[tf]"
      ]
    },
    {
      "id": "transformers_model_load",
      "pattern": "from_pretrained|model not found|download",
      "severity": "error",
      "causes": [
        "Model name incorrect",
        "Network issues",
        "Authentication needed"
      ],
      "solutions": [
        "model = AutoModel.from_pretrained('model-name')",
        "Set HF_TOKEN for gated models",
        "Or: huggingface-cli login"
      ]
    },
    {
      "id": "transformers_tokenizer",
      "pattern": "tokenizer|AutoTokenizer|encode",
      "severity": "error",
      "causes": [
        "Tokenizer mismatch",
        "Wrong tokenizer class",
        "Special tokens"
      ],
      "solutions": [
        "tokenizer = AutoTokenizer.from_pretrained('model-name')",
        "Must match model",
        "Add special tokens: add_special_tokens=True"
      ]
    },
    {
      "id": "transformers_gpu",
      "pattern": "cuda|device|GPU|to\\('cuda'\\)",
      "severity": "warning",
      "causes": [
        "Model on wrong device",
        "No GPU available",
        "Device mismatch"
      ],
      "solutions": [
        "model.to('cuda') if torch.cuda.is_available()",
        "Or: device_map='auto' for automatic",
        "inputs = inputs.to(model.device)"
      ]
    },
    {
      "id": "transformers_pipeline",
      "pattern": "pipeline|task|inference",
      "severity": "info",
      "causes": [
        "Quick inference setup",
        "Task-specific pipeline",
        "Simple API"
      ],
      "solutions": [
        "pipe = pipeline('text-generation', model='...')",
        "Tasks: text-generation, sentiment-analysis, etc",
        "result = pipe('input text')"
      ]
    },
    {
      "id": "transformers_generation",
      "pattern": "generate|max_length|generation config",
      "severity": "info",
      "causes": [
        "Text generation",
        "Output length",
        "Sampling params"
      ],
      "solutions": [
        "outputs = model.generate(inputs, max_new_tokens=100)",
        "Use GenerationConfig for complex settings",
        "do_sample=True for variety"
      ]
    },
    {
      "id": "transformers_memory",
      "pattern": "out of memory|CUDA OOM|memory",
      "severity": "error",
      "causes": [
        "Model too large",
        "Batch size too big",
        "Gradient accumulation"
      ],
      "solutions": [
        "Use device_map='auto' for offloading",
        "load_in_8bit=True or load_in_4bit=True",
        "Reduce batch size, use gradient checkpointing"
      ]
    },
    {
      "id": "transformers_trainer",
      "pattern": "Trainer|training|fine-tune",
      "severity": "info",
      "causes": [
        "Fine-tuning models",
        "Training loop",
        "Configuration"
      ],
      "solutions": [
        "trainer = Trainer(model, args, train_dataset)",
        "TrainingArguments for config",
        "trainer.train()"
      ]
    }
  ]
}
