{
  "tool": "ollama",
  "category": "ai_local",
  "common_errors": [
    {
      "id": "ollama_install",
      "pattern": "ollama not found|install ollama|command not found",
      "severity": "error",
      "causes": [
        "Ollama not installed",
        "Not in PATH",
        "Service not running"
      ],
      "solutions": [
        "macOS: brew install ollama",
        "Linux: curl -fsSL https://ollama.com/install.sh | sh",
        "Windows: download from ollama.com"
      ]
    },
    {
      "id": "ollama_serve",
      "pattern": "connection refused|server not running|port 11434",
      "severity": "error",
      "causes": [
        "Ollama server not started",
        "Wrong port",
        "Firewall blocking"
      ],
      "solutions": [
        "Start: ollama serve",
        "Or: ollama serve &",
        "Default port: 11434"
      ]
    },
    {
      "id": "ollama_pull",
      "pattern": "model not found|pull model|download",
      "severity": "error",
      "causes": [
        "Model not downloaded",
        "Typo in model name",
        "Network issues"
      ],
      "solutions": [
        "Pull: ollama pull llama3",
        "List available: ollama list",
        "Models: llama3, mistral, codellama, etc"
      ]
    },
    {
      "id": "ollama_memory",
      "pattern": "out of memory|VRAM|GPU memory",
      "severity": "error",
      "causes": [
        "Model too large",
        "Insufficient GPU memory",
        "Multiple models loaded"
      ],
      "solutions": [
        "Use smaller model: llama3:8b vs llama3:70b",
        "Stop other models: ollama stop",
        "Use quantized versions: model:q4_0"
      ]
    },
    {
      "id": "ollama_api",
      "pattern": "API|REST|curl|programmatic",
      "severity": "info",
      "causes": [
        "Programmatic access",
        "Integration",
        "Custom apps"
      ],
      "solutions": [
        "Generate: POST http://localhost:11434/api/generate",
        "Chat: POST http://localhost:11434/api/chat",
        "Python: pip install ollama"
      ]
    },
    {
      "id": "ollama_python",
      "pattern": "python|ollama-python|pip install ollama",
      "severity": "info",
      "causes": [
        "Python integration",
        "Library usage",
        "Import issues"
      ],
      "solutions": [
        "Install: pip install ollama",
        "Usage: import ollama; ollama.chat(model='llama3', ...)",
        "Streaming: stream=True"
      ]
    },
    {
      "id": "ollama_modelfile",
      "pattern": "Modelfile|custom model|create",
      "severity": "info",
      "causes": [
        "Creating custom model",
        "System prompt",
        "Parameters"
      ],
      "solutions": [
        "Create Modelfile with FROM base_model",
        "SYSTEM 'custom system prompt'",
        "Build: ollama create mymodel -f Modelfile"
      ]
    },
    {
      "id": "ollama_context",
      "pattern": "context length|num_ctx|token limit",
      "severity": "warning",
      "causes": [
        "Long conversations",
        "Context overflow",
        "Memory usage"
      ],
      "solutions": [
        "Set context: num_ctx in options",
        "Default varies by model",
        "Larger context = more memory"
      ]
    }
  ]
}
