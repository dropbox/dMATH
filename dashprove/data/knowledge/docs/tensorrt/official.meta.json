{
  "tool_id": "tensorrt",
  "tool_name": "NVIDIA TensorRT",
  "source_url": "https://developer.nvidia.com/tensorrt",
  "doc_type": "official",
  "fetched_at": "2025-12-23T04:30:03.048885+00:00",
  "http_status": 200,
  "content_type_header": "text/html; charset=utf-8",
  "word_count": 1995,
  "sections": [
    "NVIDIA TensorRT",
    "How TensorRT Works",
    "Read the Introductory TensorRT Blog",
    "Watch On-Demand TensorRT Sessions From GTC",
    "Get the Complete Developer Guide",
    "Navigate AI infrastructure and Performance",
    "Key Features",
    "Large Language Model Inference",
    "Compile in the Cloud",
    "Optimize Neural Networks",
    "Major Framework Integrations",
    "Deploy, Run, and Scale With Dynamo-Triton",
    "Simplify AI deployment on RTX",
    "Accelerate Every Inference Platform",
    "Get Started With TensorRT",
    "Get Started With TensorRT Frameworks",
    "Download ONNX and Torch-TensorRT",
    "Experience Tripy: Pythonic Inference With TensorRT",
    "Deploy",
    "World-Leading Inference Performance",
    "8X Increase in GPT-J 6B Inference Performance",
    "4X Higher Llama2 Inference Performance",
    "Total Cost of Ownership",
    "Energy Use",
    "Starter Kits",
    "Beginner Guide to TensorRT",
    "Beginner Guide to TensorRT-LLM",
    "Beginner Guide to TensorRT Model Optimizer",
    "Beginner Guide to Torch-TensorRT",
    "Beginner Guide to TensorRT Pythonic Frontend: Tripy",
    "Beginner Guide to TensorRT for RTX",
    "Run High-Performance AI Applications with NVIDIA TensorRT for RTX",
    "TensorRT Learning Library",
    "TensorRT Ecosystem Ecosystem",
    "More Resources",
    "Explore the Community",
    "Get Training and Certification",
    "Read Top Stories and Blogs",
    "Ethical AI"
  ],
  "code_block_count": 0
}