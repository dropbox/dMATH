[package]
name = "gamma-onnx"
description = "ONNX model loading and conversion to Î³-CROWN IR"
version.workspace = true
edition.workspace = true
license.workspace = true
repository.workspace = true
rust-version.workspace = true

[features]
default = []
pytorch = ["dep:candle-core", "dep:zip"]
coreml = ["dep:coreml-proto", "dep:prost_014"]
gguf = ["dep:gguf"]

[dependencies]
gamma-core = { path = "../gamma-core" }
gamma-tensor = { path = "../gamma-tensor" }
gamma-propagate = { path = "../gamma-propagate" }
gamma-transformer = { path = "../gamma-transformer" }
gamma-gpu = { path = "../gamma-gpu" }
ndarray.workspace = true
prost.workspace = true
safetensors.workspace = true
tracing.workspace = true
thiserror.workspace = true
serde.workspace = true
serde_json.workspace = true
ort.workspace = true
ndarray-npy = "0.9"
candle-core = { workspace = true, optional = true }
zip = { workspace = true, optional = true }
coreml-proto = { workspace = true, optional = true }
# prost 0.14 for coreml-proto compatibility (coreml-proto requires prost 0.14, we use 0.13)
prost_014 = { package = "prost", version = "0.14", optional = true }
# GGUF format support (for llama.cpp models)
gguf = { workspace = true, optional = true }
# Memory-mapped file I/O (for efficient large model loading)
memmap2.workspace = true
flate2 = "1.0"

[dev-dependencies]
approx.workspace = true
libm.workspace = true
tempfile = "3.20"
tracing-subscriber.workspace = true
